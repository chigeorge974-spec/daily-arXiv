<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 65]
- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: AIdentifyAGE ontology provides standardized semantic framework for forensic dental age assessment, addressing methodological heterogeneity and enabling traceable linkage between observations, methods, and outcomes in medico-legal contexts.


<details>
  <summary>Details</summary>
Motivation: Current dental age assessment practices face challenges including methodological heterogeneity, fragmented data representation, limited interoperability between systems, and lack of transparency/reproducibility - problems amplified by increasing adoption of AI methods in forensic and judicial contexts.

Method: Developed with domain experts, the AIdentifyAGE ontology builds on established upper and biomedical/dental/machine learning ontologies to create a standardized semantic framework that models complete medico-legal workflows, integrating judicial context, individual data, forensic examinations, dental development methods, imaging, reference studies, and AI estimation methods.

Result: The ontology provides a semantically coherent framework enabling traceable linkage between observations, methods, reference data, and reported outcomes, ensuring interoperability, extensibility, and compliance with FAIR principles.

Conclusion: AIdentifyAGE establishes a robust foundation for ontology-driven decision support systems, enhancing consistency, transparency, and explainability in forensic dental age assessment for medico-legal and judicial applications.

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [2] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: Single-state reuse in adaptive systems inevitably leads to contextuality in classical probabilistic representations, requiring irreducible information-theoretic costs that nonclassical frameworks avoid by relaxing joint probability assumptions.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental representational consequences of single-state reuse in adaptive systems, which is ubiquitous in natural and artificial intelligence but poorly understood in terms of contextuality.

Method: Model contexts as interventions acting on a shared internal state, prove that classical models reproducing contextual outcome statistics must incur irreducible information-theoretic costs, provide a minimal constructive example, and analyze how nonclassical frameworks avoid this obstruction.

Result: Contextuality is not unique to quantum mechanics but an inevitable consequence of single-state reuse in classical probabilistic representations; classical models must incur irreducible information-theoretic costs where dependence on context cannot be mediated solely through internal state.

Conclusion: Contextuality is a general representational constraint on adaptive intelligence independent of physical implementation, arising from single-state reuse in classical frameworks but avoidable in nonclassical probabilistic frameworks that relax joint probability assumptions.

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [3] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: MobCache: A mobility-aware cache framework using reconstructible caches and latent-space reasoning to enable efficient large-scale human mobility simulations while maintaining fidelity to LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: Large-scale human mobility simulation is important for urban planning, epidemiology, and transportation analysis, but current LLM-based approaches have high computational costs that limit scalability.

Method: MobCache consists of two components: (1) a reasoning component that encodes reasoning steps as latent-space embeddings with a latent-space evaluator for reuse and recombination, and (2) a decoding component with a lightweight decoder trained using mobility law-constrained distillation to translate latent-space reasoning chains into natural language.

Result: Experiments show MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

Conclusion: MobCache enables efficient large-scale human mobility simulations by leveraging reconstructible caches and latent-space reasoning, addressing the scalability limitations of current LLM-based approaches while preserving simulation fidelity.

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [4] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: Analysis of 60 LLM benchmarks reveals nearly half show saturation, with expert-curated benchmarks resisting saturation better than crowdsourced ones, and hidden test data providing no protective effect.


<details>
  <summary>Details</summary>
Motivation: AI benchmarks often become saturated, losing their ability to differentiate between top-performing models, which diminishes their long-term value for measuring progress and guiding deployment decisions.

Method: Analyzed 60 LLM benchmarks from major model developers' technical reports, characterized them along 14 properties spanning task design, data construction, and evaluation format, and tested five hypotheses about how each property contributes to saturation rates.

Result: Nearly half of benchmarks exhibit saturation, with saturation rates increasing as benchmarks age. Expert-curated benchmarks resist saturation better than crowdsourced ones, while hiding test data (public vs. private) shows no protective effect against saturation.

Conclusion: The findings identify which benchmark design choices extend longevity and inform strategies for creating more durable evaluation frameworks that maintain their ability to differentiate between state-of-the-art models over time.

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [5] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: Simple baselines match or exceed sophisticated code evolution methods across mathematical bounds, agentic scaffolds, and ML competitions, revealing shortcomings in current code evolution development and evaluation practices.


<details>
  <summary>Details</summary>
Motivation: Many code evolution pipelines show impressive performance but lack comparison to simpler baselines, raising questions about whether sophisticated methods actually outperform basic approaches.

Method: Tested two simple baselines across three domains: mathematical bounds optimization, agentic scaffold design, and machine learning competitions. Analyzed performance differences and identified evaluation shortcomings.

Result: Simple baselines matched or exceeded sophisticated code evolution methods in all three domains. For mathematical bounds, search space design and domain knowledge were more important than the evolution pipeline. For agentic scaffolds, high variance and small datasets led to suboptimal selection, with hand-designed majority vote performing best.

Conclusion: Code evolution research needs better baselines and evaluation methods. The primary challenge is often designing good search spaces rather than the search algorithm itself. Proposed improved evaluation methods and best practices for more rigorous future work.

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [6] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: The paper improves the upper bound for slicing all edges of an n-dimensional hypercube with hyperplanes, showing S(n) ≤ ⌈4n/5⌉ (with a +1 adjustment for odd multiples of 5), improving on the previous bound of ⌈5n/6⌉ from 1971.


<details>
  <summary>Details</summary>
Motivation: The problem concerns the minimum number of hyperplanes needed to slice all edges of an n-dimensional hypercube. This is a combinatorial geometry problem with connections to computational geometry and discrete mathematics. The previous best upper bound was from 1971, suggesting room for improvement.

Method: The authors construct 8 hyperplanes that slice Q₁₀, then use this construction to derive the general bound. They employ CPro1, an automatic tool that combines reasoning LLMs with automated hyperparameter tuning to create search algorithms for discovering mathematical constructions.

Result: Proved that S(n) ≤ ⌈4n/5⌉, except when n is an odd multiple of 5, where S(n) ≤ 4n/5 + 1. This improves the previous bound of S(n) ≤ ⌈5n/6⌉. Also obtained new lower bounds on the maximum number of edges that can be sliced using k < n hyperplanes.

Conclusion: The paper establishes improved upper bounds for the hypercube edge slicing problem using a combination of constructive methods and AI-assisted search techniques. The use of CPro1 demonstrates the potential of automated reasoning tools in mathematical discovery.

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [7] [NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography](https://arxiv.org/abs/2602.16812)
*Zhongcan Xiao,Leyi Zhang,Guannan Zhang,Xiaoping Wang*

Main category: cs.AI

TL;DR: NeuDiff Agent is an AI workflow that automates neutron diffraction data analysis from raw data to validated crystal structure, reducing analysis time by 4.6-5.0x while maintaining validation standards.


<details>
  <summary>Details</summary>
Motivation: Large-scale neutron facilities face analysis latency as the bottleneck for scientific throughput, especially for complex samples requiring iterative reduction, integration, refinement, and validation steps.

Method: A governed, tool-using AI workflow that executes established crystallography pipelines under explicit governance: restricts actions to allowlisted tools, enforces fail-closed verification gates at workflow boundaries, and captures complete provenance for inspection and auditing.

Result: Reduces wall time from 435 minutes (manual) to 86.5-94.4 minutes (4.6-5.0x faster) while producing validated CIF files with no checkCIF level A or B alerts, using two different LLM backends.

Conclusion: Establishes a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements, demonstrating significant efficiency gains without compromising quality.

Abstract: Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.

</details>


### [8] [Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI](https://arxiv.org/abs/2602.16814)
*Eiman Kanjo,Mustafa Aslanov*

Main category: cs.AI

TL;DR: Node Learning is a decentralized AI paradigm where edge nodes autonomously learn from local data and selectively share knowledge with peers, enabling intelligence propagation through diffusion rather than centralized aggregation.


<details>
  <summary>Details</summary>
Motivation: Centralized AI faces scalability issues at the edge due to data transmission costs, latency, energy consumption, and dependence on large data centers, which are unsuitable for heterogeneous, mobile, and resource-constrained environments.

Method: Nodes continuously learn from local data, maintain individual model states, and opportunistically exchange learned knowledge through selective peer interactions when collaboration is beneficial, with intelligence propagating through overlap and diffusion rather than global synchronization.

Result: The paper develops conceptual foundations for Node Learning, contrasting it with existing decentralized approaches and examining implications for communication, hardware, trust, and governance in edge AI systems.

Conclusion: Node Learning provides a unified abstraction for autonomous and cooperative behavior that accommodates heterogeneity in data, hardware, objectives, and connectivity, placing existing paradigms within a broader decentralized perspective without discarding them.

Abstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective

</details>


### [9] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: The paper proposes an order-oriented scoring framework for hesitant fuzzy sets, proves classical orders don't form lattices, introduces dominance functions for ranking with acceptability thresholds, and applies them to group decision-making.


<details>
  <summary>Details</summary>
Motivation: Traditional scoring approaches for hesitant fuzzy sets lack formal order-theoretic foundations, leading to less flexible and coherent scoring mechanisms that don't properly account for underlying ordering structures.

Method: Develops a unified order-oriented scoring framework, analyzes classical orders on hesitant fuzzy elements, proves they don't induce lattice structures, introduces dominance functions (discrete and relative) for ranking with control sets and acceptability thresholds, and applies them to construct fuzzy preference relations.

Result: Shows classical orders on hesitant fuzzy elements don't form lattices (contrary to prior claims), proves symmetric-order scores satisfy normative criteria (strong monotonicity, Gärdenfors condition), and demonstrates dominance functions can effectively rank hesitant fuzzy elements for group decision-making.

Conclusion: The order-oriented framework provides formal foundations for hesitant fuzzy scoring, dominance functions offer practical ranking tools with acceptability thresholds, and the approach supports flexible group decision-making applications.

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [10] [Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents](https://arxiv.org/abs/2602.16855)
*Haiyang Xu,Xi Zhang,Haowei Liu,Junyang Wang,Zhaozai Zhu,Shengjie Zhou,Xuhao Hu,Feiyu Gao,Junjie Cao,Zihua Wang,Zhiyuan Chen,Jitong Liao,Qi Zheng,Jiahui Zeng,Ze Xu,Shuai Bai,Junyang Lin,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: GUI-Owl-1.5 is a state-of-the-art native GUI agent model with multiple size variants (2B-235B) supporting desktop, mobile, browser platforms for cloud-edge collaboration, achieving SOTA results on 20+ GUI benchmarks through hybrid data collection, unified capability enhancement, and multi-platform RL scaling.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive GUI agent model that can operate across multiple platforms (desktop, mobile, browser) with cloud-edge collaboration capabilities, addressing the need for efficient GUI automation, grounding, tool-calling, and knowledge tasks across diverse environments.

Method: 1) Hybrid Data Flywheel: Combines simulated and cloud-based sandbox environments for UI understanding and trajectory generation. 2) Unified Enhancement: Uses thought-synthesis pipeline to enhance reasoning capabilities with focus on tool/MCP use, memory, and multi-agent adaptation. 3) Multi-platform Environment RL Scaling: Introduces MRPO algorithm to handle multi-platform conflicts and improve training efficiency for long-horizon tasks.

Result: Achieves state-of-the-art results on 20+ GUI benchmarks: 56.5 on OSWorld, 71.6 on AndroidWorld, 48.4 on WebArena for automation; 80.3 on ScreenSpotPro for grounding; 47.6 on OSWorld-MCP, 46.8 on MobileWorld for tool-calling; 75.5 on GUI-Knowledge Bench for memory/knowledge tasks.

Conclusion: GUI-Owl-1.5 represents a significant advancement in GUI agent technology with multi-platform support, cloud-edge collaboration, and comprehensive benchmark performance. The model is open-sourced with available online demo, providing a versatile solution for real-time GUI interaction across diverse environments.

Abstract: The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.

</details>


### [11] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: OpenSage is the first agent development kit that enables LLMs to automatically create agents with self-generated topology and toolsets, featuring hierarchical memory and specialized software engineering toolkits, outperforming existing ADKs across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current agent development kits either lack sufficient functional support or require manual human design of agent topology, tools, and memory components, which limits agent generalizability and overall performance.

Method: OpenSage enables LLMs to automatically create agents with self-generated topology and toolsets, provides comprehensive structured memory support with hierarchical graph-based memory system, and includes specialized toolkits for software engineering tasks.

Result: Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate OpenSage's advantages over existing ADKs, with rigorous ablation studies confirming the effectiveness of each design component.

Conclusion: OpenSage paves the way for next-generation agent development by shifting from human-centered to AI-centered paradigms, enabling automatic agent creation with self-generated components.

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [12] [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
*Tanqiu Jiang,Yuhui Wang,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: AgentLAB is the first benchmark for evaluating LLM agent vulnerabilities to adaptive, long-horizon attacks across 28 environments and 644 test cases, revealing current agents remain highly susceptible despite existing single-turn defenses.


<details>
  <summary>Details</summary>
Motivation: As LLM agents are increasingly deployed in complex, long-horizon environments, they become vulnerable to multi-turn attacks that exploit user-agent-environment interactions, which cannot be effectively measured by existing single-turn security evaluations.

Method: Developed AgentLAB benchmark supporting five novel attack types (intent hijacking, tool chaining, task injection, objective drifting, memory poisoning) across 28 realistic agentic environments with 644 security test cases to systematically evaluate LLM agent vulnerabilities.

Result: Evaluation of representative LLM agents shows they remain highly susceptible to long-horizon attacks, and defenses designed for single-turn interactions fail to reliably mitigate these multi-turn threats.

Conclusion: AgentLAB serves as a valuable benchmark for tracking progress on securing LLM agents in practical settings, highlighting the need for new defense mechanisms specifically designed for long-horizon security threats.

Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.

</details>


### [13] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: LLM-Wikirace is a benchmark testing LLMs' ability to navigate Wikipedia hyperlinks from source to target pages, evaluating planning, reasoning, and world knowledge. While frontier models achieve superhuman performance on easy tasks, they struggle significantly on hard difficulty (best model succeeds only 23%), revealing limitations in long-horizon reasoning and replanning capabilities.


<details>
  <summary>Details</summary>
Motivation: To create a benchmark that evaluates planning, reasoning, and world knowledge in LLMs through Wikipedia navigation tasks, identifying limitations in current reasoning systems and providing an open arena for testing planning-capable models.

Method: Developed LLM-Wikirace benchmark where models must navigate Wikipedia hyperlinks step-by-step from source to target pages, requiring look-ahead planning and reasoning about concept connections. Evaluated broad set of open- and closed-source models including Gemini-3, GPT-5, and Claude Opus 4.5 across easy and hard difficulty levels.

Result: Frontier models achieve superhuman performance on easy tasks but performance drops sharply on hard difficulty: best-performing model (Gemini-3) succeeds in only 23% of hard games. World knowledge is necessary but insufficient beyond a threshold, where planning and long-horizon reasoning become dominant. Models struggle to replan after failure, frequently entering loops rather than recovering.

Conclusion: LLM-Wikirace reveals clear limitations in current reasoning systems, showing that while frontier models excel on easy tasks, they face substantial challenges in hard scenarios requiring complex planning and long-horizon reasoning. The benchmark offers an open arena where planning-capable LLMs still have much to prove.

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [14] [Narrow fine-tuning erodes safety alignment in vision-language agents](https://arxiv.org/abs/2602.16931)
*Idhant Gulati,Shivam Raval*

Main category: cs.AI

TL;DR: Fine-tuning aligned vision-language models on harmful datasets causes severe emergent misalignment that generalizes across tasks and modalities, with multimodal evaluation revealing higher degradation than text-only benchmarks.


<details>
  <summary>Details</summary>
Motivation: Lifelong multimodal agents need continuous adaptation through post-training, but this creates tension between acquiring capabilities and preserving safety alignment. The paper investigates how fine-tuning aligned models on harmful data induces misalignment that generalizes broadly.

Method: Experiments on Gemma3-4B with LoRA fine-tuning on harmful datasets, measuring misalignment scaling with LoRA rank. Used multimodal vs text-only evaluation, analyzed geometric properties of harmful behaviors via PCA, and tested mitigation strategies (benign narrow fine-tuning and activation-based steering).

Result: Misalignment scales monotonically with LoRA rank, with multimodal evaluation showing substantially higher misalignment (70.71±1.22 at r=128) than text-only evaluation (41.19±2.51). Even 10% harmful data induces substantial degradation. Harmful behaviors occupy low-dimensional subspace (10 principal components capture majority). Mitigation strategies reduce but don't completely remove harmful behaviors.

Conclusion: Current post-training paradigms may not sufficiently preserve alignment in deployment settings, highlighting need for robust continual learning frameworks. Unimodal safety benchmarks underestimate alignment degradation in vision-language models.

Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.

</details>


### [15] [DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs](https://arxiv.org/abs/2602.16935)
*Justin Albrethsen,Yash Datta,Kunal Kumar,Sharath Rajasekar*

Main category: cs.AI

TL;DR: DeepContext is a stateful RNN-based framework for multi-turn jailbreak detection that tracks temporal intent evolution, outperforming stateless models with 0.84 F1 score and <20ms inference overhead.


<details>
  <summary>Details</summary>
Motivation: Current LLM safety guardrails are stateless and treat multi-turn dialogues as disconnected events, creating a "Safety Gap" where adversarial tactics like Crescendo and ActorAttack can slowly bleed malicious intent across turn boundaries to bypass filters.

Method: DeepContext uses a Recurrent Neural Network (RNN) architecture that ingests sequences of fine-tuned turn-level embeddings, propagating a hidden state across conversations to capture incremental risk accumulation that stateless models overlook.

Result: DeepContext achieves state-of-the-art F1 score of 0.84 in multi-turn jailbreak detection, significantly outperforming hyperscaler cloud-provider guardrails and leading open-weight models like Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67), with sub-20ms inference overhead on T4 GPU.

Conclusion: Modeling the sequential evolution of intent is more effective and computationally efficient than deploying massive stateless models, making stateful monitoring frameworks like DeepContext viable for real-time safety applications.

Abstract: While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a "Safety Gap" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext discards the isolated evaluation model in favor of a Recurrent Neural Network (RNN) architecture that ingests a sequence of fine-tuned turn-level embeddings. By propagating a hidden state across the conversation, DeepContext captures the incremental accumulation of risk that stateless models overlook. Our evaluation demonstrates that DeepContext significantly outperforms existing baselines in multi-turn jailbreak detection, achieving a state-of-the-art F1 score of 0.84, which represents a substantial improvement over both hyperscaler cloud-provider guardrails and leading open-weight models such as Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67). Furthermore, DeepContext maintains a sub-20ms inference overhead on a T4 GPU, ensuring viability for real-time applications. These results suggest that modeling the sequential evolution of intent is a more effective and computationally efficient alternative to deploying massive, stateless models.

</details>


### [16] [SourceBench: Can AI Answers Reference Quality Web Sources?](https://arxiv.org/abs/2602.16942)
*Hexi Jin,Stephen Liu,Yuheng Li,Simran Malik,Yiying Zhang*

Main category: cs.AI

TL;DR: SourceBench is a new benchmark for evaluating the quality of web sources cited by LLMs, covering 100 real-world queries across multiple intents with an 8-metric framework for content quality and page-level signals.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of LLMs focus primarily on answer correctness while neglecting the quality of cited web sources, creating a gap in assessing the reliability and usefulness of evidence provided by LLMs in real-world applications.

Method: Developed SourceBench with 100 real-world queries spanning informational, factual, argumentative, social, and shopping intents. Created an 8-metric evaluation framework covering content quality (relevance, accuracy, objectivity) and page-level signals (freshness, authority, clarity). Built a human-labeled dataset and calibrated LLM-based evaluator matching expert judgments. Evaluated 8 LLMs, Google Search, and 3 AI search tools over 3996 cited sources.

Result: The benchmark provides systematic evaluation of source quality across multiple dimensions. The calibrated LLM-based evaluator closely matches expert human judgments. The evaluation of 12 systems (8 LLMs + Google Search + 3 AI tools) reveals four key insights about source citation quality in GenAI and web search.

Conclusion: SourceBench addresses the critical gap in evaluating web source quality for LLM citations, providing a comprehensive framework and benchmark that reveals important insights to guide future research in generative AI and web search integration.

Abstract: Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational, factual, argumentative, social, and shopping intents. SourceBench uses an eight-metric framework covering content quality (content relevance, factual accuracy, objectivity) and page-level signals (e.g., freshness, authority/accountability, clarity), and includes a human-labeled dataset with a calibrated LLM-based evaluator that matches expert judgments closely. We evaluate eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench and conduct further experiments to understand the evaluation results. Overall, our work reveals four key new insights that can guide future research in the direction of GenAI and web search.

</details>


### [17] [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
*Arnold Cartagena,Ariane Teixeira*

Main category: cs.AI

TL;DR: Text-only safety evaluations fail to predict harmful actions by LLM agents; models often refuse harmful requests in text while simultaneously executing forbidden actions through tool calls, revealing a critical safety gap.


<details>
  <summary>Details</summary>
Motivation: Current safety evaluations focus on text-level refusal behavior, but LLM agents increasingly interact with external systems through tool calls with real-world consequences. There's a critical need to understand whether alignment that suppresses harmful text also suppresses harmful actions.

Method: Introduced GAP benchmark: systematic evaluation framework measuring divergence between text-level safety and tool-call-level safety. Tested six frontier models across six regulated domains, seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints.

Result: Text safety does not transfer to tool-call safety. Models frequently refuse harmful requests in text while simultaneously executing forbidden actions through tool calls. 219 such divergence cases persist across all six models even under safety-reinforced prompts. System prompt wording substantially influences tool-call behavior (TC-safe rates vary 21-57 percentage points). Runtime governance contracts reduce information leakage but don't deter forbidden tool-call attempts.

Conclusion: Text-only safety evaluations are insufficient for assessing agent behavior. Tool-call safety requires dedicated measurement and mitigation strategies beyond traditional text-based alignment approaches.

Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.

</details>


### [18] [LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation](https://arxiv.org/abs/2602.16953)
*Hejia Zhang,Zhongming Yu,Chia-Tung Ho,Haoxing Ren,Brucek Khailany,Jishen Zhao*

Main category: cs.AI

TL;DR: LLM4Cov: Offline agent-learning framework for hardware verification that uses execution-validated data curation, policy-aware synthesis, and worst-state prioritization to enable scalable learning without expensive online feedback.


<details>
  <summary>Details</summary>
Motivation: Execution-aware LLM agents need expensive, slow tool feedback for learning, making online RL impractical for hardware verification which relies on industrial simulators and non-differentiable execution signals.

Method: Models verification as memoryless state transitions guided by deterministic evaluators; uses execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling for offline learning.

Result: Compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and competing with models an order of magnitude larger.

Conclusion: LLM4Cov enables effective offline agent learning for hardware verification by addressing execution feedback constraints through structured data curation and synthesis techniques.

Abstract: Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.

</details>


### [19] [Automating Agent Hijacking via Structural Template Injection](https://arxiv.org/abs/2602.16958)
*Xinhao Deng,Jiaqing Wu,Miao Chen,Yue Xiao,Ke Xu,Qi Li*

Main category: cs.AI

TL;DR: Phantom is an automated agent hijacking framework using Structured Template Injection to exploit LLM agent architecture vulnerabilities, achieving high attack success rates and transferability across commercial models.


<details>
  <summary>Details</summary>
Motivation: Current agent hijacking attacks rely on manual prompt manipulation with low success rates and poor transferability to closed-source models. There's a need for automated attacks that target fundamental architectural mechanisms of LLM agents rather than just semantic manipulation.

Method: Phantom uses Structured Template Injection targeting LLM agent chat template tokens. It employs multi-level template augmentation for structural diversity, trains a Template Autoencoder (TAE) to embed templates in continuous latent space, and uses Bayesian optimization to find optimal adversarial vectors decoded into effective structured templates.

Result: The framework significantly outperforms existing baselines in Attack Success Rate (ASR) and query efficiency across Qwen, GPT, and Gemini models. It identified over 70 vulnerabilities in real-world commercial products confirmed by vendors.

Conclusion: Structured template-based hijacking poses severe practical threats to LLM agents, demonstrating the need for architectural-level security considerations in next-generation agentic systems.

Abstract: Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.

</details>


### [20] [HQFS: Hybrid Quantum Classical Financial Security with VQC Forecasting, QUBO Annealing, and Audit-Ready Post-Quantum Signing](https://arxiv.org/abs/2602.16976)
*Srikumar Nayak*

Main category: cs.AI

TL;DR: HQFS is a hybrid quantum-classical pipeline that integrates forecasting, discrete risk optimization, and auditability for financial risk systems, improving prediction accuracy, portfolio performance, and solve times while providing verifiable allocation records.


<details>
  <summary>Details</summary>
Motivation: Traditional two-step financial risk systems (prediction followed by optimization) break under real-world constraints like market shifts, discrete constraints (lot sizes, caps), slow optimization for large asset sets, and regulatory requirements for clear audit trails linking decisions to model states.

Method: HQFS uses a three-stage hybrid pipeline: 1) Learns next-step return and volatility proxy using variational quantum circuit (VQC) with small classical head; 2) Converts risk-return objective and constraints into QUBO, solving with quantum annealing when available (classical QUBO solver fallback); 3) Signs each rebalance output with post-quantum signature for verifiable allocation records.

Result: On market dataset: 7.8% reduction in return prediction error, 6.1% reduction in volatility prediction error vs classical baseline. Decision layer: 9.4% improvement in out-of-sample Sharpe ratio, 11.7% lower maximum drawdown. QUBO solve stage: 28% reduction in average solve time vs mixed-integer baseline, with fully traceable signed allocation records.

Conclusion: HQFS demonstrates a practical hybrid quantum-classical approach that addresses key limitations of traditional financial risk systems by integrating forecasting, discrete optimization, and auditability, delivering improved performance, efficiency, and regulatory compliance.

Abstract: Here's the corrected paragraph with all punctuation and formatting issues fixed:
  Financial risk systems usually follow a two-step routine: a model predicts return or risk, and then an optimizer makes a decision such as a portfolio rebalance. In practice, this split can break under real constraints. The prediction model may look good, but the final decision can be unstable when the market shifts, when discrete constraints are added (lot sizes, caps), or when the optimization becomes slow for larger asset sets. Also, regulated settings need a clear audit trail that links each decision to the exact model state and inputs. We present HQFS, a practical hybrid pipeline that connects forecasting, discrete risk optimization, and auditability in one flow. First, HQFS learns next-step return and a volatility proxy using a variational quantum circuit (VQC) with a small classical head. Second, HQFS converts the risk-return objective and constraints into a QUBO and solves it with quantum annealing when available, while keeping a compatible classical QUBO solver as a fallback for deployment. Third, HQFS signs each rebalance output using a post-quantum signature so the allocation can be verified later without trusting the runtime environment. On our market dataset study, HQFS reduces return prediction error by 7.8% and volatility prediction error by 6.1% versus a tuned classical baseline. For the decision layer, HQFS improves out-of-sample Sharpe by 9.4% and lowers maximum drawdown by 11.7%. The QUBO solve stage also cuts average solve time by 28% compared to a mixed-integer baseline under the same constraints, while producing fully traceable, signed allocation records.

</details>


### [21] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: Black-box safety evaluation fails for models with latent context-conditioned policies due to fundamental statistical limits, requiring additional safeguards beyond testing.


<details>
  <summary>Details</summary>
Motivation: Challenge the assumption that black-box safety evaluation reliably predicts deployment performance for AI systems with unobserved internal variables that trigger unsafe behaviors.

Method: Formal analysis using statistical minimax theory: (1) passive evaluation with Le Cam's method, (2) adaptive evaluation with hash-based triggers and Yao's minimax principle, (3) computational separation via trapdoor one-way functions, and (4) white-box probing with bias correction.

Result: Fundamental limits show no black-box evaluator can reliably estimate deployment risk: passive evaluation error ≥ 0.208δL, adaptive evaluation error ≥ δL/16, computational separation prevents polynomial-time detection, while white-box probing requires O(1/(γ²ε_R²)) samples.

Conclusion: Black-box testing is statistically underdetermined for latent context-conditioned policies; mathematical criteria establish when architectural constraints, training-time guarantees, interpretability, and deployment monitoring are necessary for worst-case safety assurance.

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [22] [Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation](https://arxiv.org/abs/2602.16990)
*Yan Wang,Yi Han,Lingfei Qian,Yueru He,Xueqing Peng,Dongji Feng,Zhuohan Xie,Vincent Jim Zhang,Rosie Guo,Fengran Mo,Jimin Huang,Yankai Chen,Xue Liu,Jian-Yun Nie*

Main category: cs.AI

TL;DR: Conv-FinRe is a conversational benchmark for stock recommendation that evaluates LLMs on decision quality rather than just behavior imitation, using multi-view references to distinguish rational analysis from user noise.


<details>
  <summary>Details</summary>
Motivation: Existing recommendation benchmarks conflate behavioral imitation with decision quality by treating user choices as ground truth. In financial advisory, observed user actions can be noisy or short-sighted under market volatility and may conflict with long-term goals.

Method: Built from real market data and human decision trajectories, Conv-FinRe provides: (1) onboarding interviews, (2) step-wise market context, and (3) advisory dialogues. Models generate rankings over fixed investment horizons. The benchmark uses multi-view references distinguishing descriptive behavior from normative utility based on investor-specific risk preferences.

Result: Evaluation of state-of-the-art LLMs reveals a persistent tension: models performing well on utility-based ranking often fail to match user choices, while behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face with code on GitHub.

Conclusion: Conv-FinRe enables diagnosis of whether LLMs follow rational analysis, mimic user noise, or are driven by market momentum, providing a more nuanced evaluation framework for financial advisory systems beyond simple behavior matching.

Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.

</details>


### [23] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: Sonar-TS is a neuro-symbolic framework for natural language querying of time series databases that uses a Search-Then-Verify pipeline with SQL-based candidate retrieval and Python-based verification, evaluated on a new benchmark NLQTSBench.


<details>
  <summary>Details</summary>
Motivation: Existing Text-to-SQL methods cannot handle continuous morphological intents like shapes or anomalies in time series, while time series models struggle with ultra-long histories, creating a gap for effective natural language querying of time series databases.

Method: Sonar-TS uses a neuro-symbolic Search-Then-Verify pipeline: 1) Feature indexing to ping candidate windows via SQL queries, 2) Generated Python programs to lock on and verify candidates against raw signals, analogous to active sonar systems.

Result: Experiments on NLQTSBench (the first large-scale benchmark for NLQ over TSDB-scale histories) demonstrate that Sonar-TS effectively handles complex temporal queries where traditional methods fail, highlighting unique domain challenges.

Conclusion: This work presents the first systematic study of NLQ4TSDB, offering a general neuro-symbolic framework and evaluation standard to facilitate future research in natural language querying for time series databases.

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [24] [IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents](https://arxiv.org/abs/2602.17049)
*Seoyoung Lee,Seobin Yoon,Seongbeen Lee,Yoojung Chun,Dayoung Park,Doyeon Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: IntentCUA is a multi-agent framework for computer-use agents that stabilizes long-horizon execution through intent-aligned plan memory, achieving 74.83% task success rate with improved efficiency over existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing computer-use agents suffer from error accumulation and inefficiency due to drifting from user intent and repeatedly solving routine subproblems in long-horizon tasks with noisy perception and multi-window contexts.

Method: Multi-agent framework with Planner, Plan-Optimizer, and Critic coordinating over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. Intent prototypes retrieve subgroup-aligned skills and inject them into partial plans.

Result: Achieved 74.83% task success rate with Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Multi-view intent abstraction and shared plan memory jointly improve execution stability, with cooperative multi-agent loop providing largest gains on long-horizon tasks.

Conclusion: System-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments, enabling stable long-horizon execution through intent-aligned plan memory.

Abstract: Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.

</details>


### [25] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: Cinder is a two-stage matchmaking system for multiplayer games that uses Ruzicka similarity for initial filtering and Kantorovich distance on skill buckets for precise fairness evaluation.


<details>
  <summary>Details</summary>
Motivation: Traditional matchmaking based on average team skill metrics often creates unbalanced games, especially with heterogeneous skill levels in pre-made teams (lobbies), impacting player retention and satisfaction.

Method: Two-stage system: 1) Rapid preliminary filter using Ruzicka similarity index on "non-outlier" skill ranges; 2) Precise fairness evaluation mapping player ranks to non-linear skill buckets (inverted normal distribution) and calculating Kantorovich distance on sorted bucket indices to produce a "Sanction Score."

Result: System viability demonstrated by analyzing distribution of Sanction Scores from 140 million simulated lobby pairings, providing robust foundation for fair matchmaking thresholds.

Conclusion: Cinder provides a fast and fair matchmaking solution that addresses limitations of average-based matching, particularly for lobbies with heterogeneous skill distributions.

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [26] [M2F: Automated Formalization of Mathematical Literature at Scale](https://arxiv.org/abs/2602.17016)
*Zichen Wang,Wanli Ma,Zhenyu Ming,Gong Zhang,Kun Yuan,Zaiwen Wen*

Main category: cs.AI

TL;DR: M2F is an agentic framework for end-to-end, project-scale autoformalization in Lean that converts textbook mathematics into fully formalized Lean code with high proof success rates.


<details>
  <summary>Details</summary>
Motivation: Current automated formalization of mathematics is limited to isolated theorems and short snippets, lacking the ability to scale to entire textbooks and research papers which require managing cross-file dependencies, resolving imports, and ensuring end-to-end compilation of entire projects.

Method: M2F operates in two stages: 1) Statement compilation stage splits documents into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles (allowing placeholders in proofs); 2) Proof repair stage closes proof holes under fixed signatures using goal-conditioned local edits. The framework keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement.

Result: In approximately three weeks, M2F converted long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages of textbooks on real analysis and convex analysis, fully formalized as Lean declarations with proofs. Achieved 96% proof success on FATE-H benchmark (vs. 80% for strong baseline).

Conclusion: Practical, large-scale automated formalization of mathematical literature is within reach, demonstrating textbook-scale formalization at a pace that would typically require months or years of expert effort.

Abstract: Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\%$ proof success (vs.\ $80\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.

</details>


### [27] [Sales Research Agent and Sales Research Bench](https://arxiv.org/abs/2602.17017)
*Deepanjan Bhol*

Main category: cs.AI

TL;DR: Sales Research Agent in Microsoft Dynamics 365 Sales outperforms leading AI models on enterprise CRM data analysis by 13-24 points on a purpose-built benchmark measuring eight quality dimensions.


<details>
  <summary>Details</summary>
Motivation: Enterprises need AI systems that can answer sales-leader questions over live, customized CRM data, but most available models lack transparent, repeatable evidence of quality. There's a need for observable quality metrics in AI-powered sales analytics.

Method: Developed Sales Research Agent in Microsoft Dynamics 365 Sales that connects to live CRM and related data, reasons over complex schemas, and produces decision-ready insights through text and chart outputs. Created Sales Research Bench benchmark that scores systems on eight customer-weighted dimensions: text and chart groundedness, relevance, explainability, schema accuracy, and chart quality.

Result: In a 200-question run on a customized enterprise schema on October 19, 2025, the Sales Research Agent outperformed Claude Sonnet 4.5 by 13 points and ChatGPT-5 by 24.1 points on the 100-point composite score.

Conclusion: The Sales Research Agent provides customers with a repeatable way to compare AI solutions for sales analytics, demonstrating superior performance over leading general-purpose AI models when working with live CRM data and complex enterprise schemas.

Abstract: Enterprises increasingly need AI systems that can answer sales-leader questions over live, customized CRM data, but most available models do not expose transparent, repeatable evidence of quality. This paper describes the Sales Research Agent in Microsoft Dynamics 365 Sales, an AI-first application that connects to live CRM and related data, reasons over complex schemas, and produces decision-ready insights through text and chart outputs. To make quality observable, we introduce the Sales Research Bench, a purpose-built benchmark that scores systems on eight customer-weighted dimensions, including text and chart groundedness, relevance, explainability, schema accuracy, and chart quality. In a 200-question run on a customized enterprise schema on October 19, 2025, the Sales Research Agent outperformed Claude Sonnet 4.5 by 13 points and ChatGPT-5 by 24.1 points on the 100-point composite score, giving customers a repeatable way to compare AI solutions.

</details>


### [28] [Phase-Aware Mixture of Experts for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.17038)
*Shengtian Yang,Yu Li,Shuo He,Yewen Li,Qingpeng Cai,Peng Jiang,Lei Feng*

Main category: cs.AI

TL;DR: PA-MoE introduces phase-aware routing in MoE architecture for RL agents to prevent simple tasks from dominating parameters and enable expert specialization in different task phases.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods with single policy networks suffer from simplicity bias where simple tasks dominate gradient updates, leaving insufficient capacity for complex tasks. Traditional MoE's token-level routing fragments phase-consistent patterns, undermining expert specialization.

Method: Proposes Phase-Aware Mixture of Experts (PA-MoE) with a lightweight phase router that learns latent phase boundaries directly from RL objective without pre-defined categories, allocating temporally consistent assignments to preserve phase-specific expertise.

Result: Experimental results demonstrate the effectiveness of the proposed PA-MoE approach.

Conclusion: PA-MoE addresses limitations of both single-policy networks and traditional MoE by enabling phase-aware expert specialization through learned phase boundaries, improving RL agent performance on complex tasks.

Abstract: Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \emph{single} policy network, causing \emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.

</details>


### [29] [Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs](https://arxiv.org/abs/2602.17046)
*Uria Franko*

Main category: cs.AI

TL;DR: ITR (Instruction-Tool Retrieval) reduces LLM agent costs by 70% and improves tool routing by 32% through dynamic retrieval of only necessary prompt fragments and tools per step.


<details>
  <summary>Details</summary>
Motivation: LLM agents incur high costs and performance issues by repeatedly processing long system instructions and large tool catalogs at each step, leading to increased latency, derailment probability, and tool-selection errors.

Method: ITR is a RAG variant that retrieves minimal system-prompt fragments and only necessary tools per step, composing a dynamic runtime system prompt with narrowed toolset and confidence-gated fallbacks.

Result: ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, cuts end-to-end episode cost by 70%, and enables agents to run 2-20x more loops within context limits.

Conclusion: ITR provides substantial efficiency gains for LLM agents, with savings compounding over multiple steps, making it particularly valuable for long-running autonomous agents.

Abstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.

</details>


### [30] [RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models](https://arxiv.org/abs/2602.17053)
*Yunseok Han,Yejoon Lee,Jaeyoung Do*

Main category: cs.AI

TL;DR: RFEval benchmark reveals 49.7% unfaithful reasoning in Large Reasoning Models, showing accuracy doesn't guarantee faithful reasoning processes.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models often produce plausible-sounding rationales that don't reflect their true decision processes, undermining reliability and trust in AI systems.

Method: Introduced formal framework with two testable conditions (stance consistency and causal influence), created RFEval benchmark with 7,186 instances across 7 tasks using controlled output-level counterfactual interventions, evaluated 12 open-source LRMs.

Result: Found unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency; failures concentrated in math and code domains; post-training RL-style objectives reduce faithfulness even when accuracy maintained; accuracy-faithfulness correlation weak and statistically insignificant.

Conclusion: Trustworthy AI requires optimizing for both correct outcomes and structural integrity of reasoning processes; established rigorous methodology for auditing LRM reliability.

Abstract: Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy-faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process. Our code and dataset can be found at project page: $\href{https://aidaslab.github.io/RFEval/}{https://aidaslab.github.io/RFEval/}$

</details>


### [31] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: S2Q learns multiple sub-value functions to retain alternative high-value actions, enabling better adaptation to shifting value functions in cooperative MARL.


<details>
  <summary>Details</summary>
Motivation: Existing value decomposition methods rely on single optimal actions and struggle to adapt when value functions shift during training, often converging to suboptimal policies.

Method: Successive Sub-value Q-learning (S2Q) learns multiple sub-value functions to retain alternative high-value actions, incorporating them into a Softmax-based behavior policy for persistent exploration.

Result: S2Q consistently outperforms various MARL algorithms on challenging benchmarks, demonstrating improved adaptability and overall performance.

Conclusion: S2Q addresses the limitation of existing value decomposition methods by maintaining multiple sub-value functions, enabling better adaptation to changing optima in cooperative MARL.

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [32] [Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization](https://arxiv.org/abs/2602.17066)
*Sumedh Rasal*

Main category: cs.AI

TL;DR: PBS accelerates language model training by using a lightweight predictor to prioritize high-loss samples during batch construction based on token-level features, achieving 6-13% faster convergence with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Current curriculum learning approaches require predefined difficulty metrics or expensive per-sample loss tracking, creating a need for a more efficient method to accelerate language model convergence through intelligent batch scheduling.

Method: Predictive Batch Scheduling (PBS) uses a lightweight linear predictor trained online to estimate sample difficulty from four static token-level features: token frequency, sequence length, vocabulary diversity, and rare token ratio, enabling dynamic prioritization of high-loss samples during batch construction.

Result: PBS achieves 6-13% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps on a 130M parameter transformer, demonstrating that token frequency statistics effectively encode sample difficulty information.

Conclusion: Token frequency statistics contain meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead through PBS, which offers a practical alternative to traditional curriculum learning and hard example mining methods.

Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.

</details>


### [33] [How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses](https://arxiv.org/abs/2602.17084)
*Kan Watanabe,Rikuto Tsuchida,Takahiro Monno,Bin Huang,Kazuma Yamasaki,Youmei Fan,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.AI

TL;DR: Empirical analysis of five AI coding agents' pull requests reveals distinct PR description characteristics that influence human reviewer engagement, response timing, sentiment, and merge outcomes in collaborative software development.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of large language models has led to AI coding agents autonomously creating pull requests on GitHub, but how these agents differ in their pull request description characteristics and how human reviewers respond to them remains underexplored.

Method: Conducted an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. Analyzed agent differences in pull request description characteristics (including structural features) and examined human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes.

Result: AI coding agents exhibit distinct PR description styles associated with differences in reviewer engagement, response time, and merge outcomes. Notable variation across agents observed in both reviewer interaction metrics and merge rates.

Conclusion: Findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development, emphasizing how AI agent communication styles influence human reviewer behavior and project outcomes.

Abstract: The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates. These findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development.

</details>


### [34] [Agentic Wireless Communication for 6G: Intent-Aware and Continuously Evolving Physical-Layer Intelligence](https://arxiv.org/abs/2602.17096)
*Zhaoyang Li,Xingzhi Jin,Junyu Pan,Qianqian Yang,Zhiguo Shi*

Main category: cs.AI

TL;DR: The paper explores using Large Language Model-based agentic AI for intent-driven autonomous intelligence in 6G physical layer communications, addressing multi-dimensional user objectives and dynamic environmental changes.


<details>
  <summary>Details</summary>
Motivation: 6G wireless systems face growing functional complexity and diverse service demands that require moving beyond rule-based control to intent-driven autonomous intelligence. User requirements are now multi-dimensional (latency sensitivity, energy preference, computational constraints, service-level requirements) and dynamic, necessitating accurate understanding of both communication environment and user intent for sustainable 6G evolution.

Method: The paper investigates agentic AI for 6G physical layer through a closed-loop pipeline of intent perception, autonomous decision making, and network execution. It reviews physical-layer tasks' limitations in supporting intent awareness, identifies advantageous application scenarios for agentic AI, discusses challenges and enabling technologies in multimodal perception, cross-layer decision making, and sustainable optimization. The approach leverages LLMs' contextual understanding and cross-modal reasoning to translate natural-language intents into executable control decisions.

Result: The paper presents AgenCom, a case study of an intent-driven link decision agent that adaptively constructs communication links under diverse user preferences and channel conditions. This demonstrates how LLM-based agents can integrate heterogeneous information and translate natural-language intents into executable control and configuration decisions for 6G systems.

Conclusion: LLM-based agentic AI provides a promising foundation for intent-aware network agents in 6G systems, enabling autonomous intelligence that can handle multi-dimensional objectives and dynamic environmental changes. The proposed closed-loop pipeline and enabling technologies address key challenges in achieving sustainable, intent-driven 6G communications.

Abstract: As 6G wireless systems evolve, growing functional complexity and diverse service demands are driving a shift from rule-based control to intent-driven autonomous intelligence. User requirements are no longer captured by a single metric (e.g., throughput or reliability), but by multi-dimensional objectives such as latency sensitivity, energy preference, computational constraints, and service-level requirements. These objectives may also change over time due to environmental dynamics and user-network interactions. Therefore, accurate understanding of both the communication environment and user intent is critical for autonomous and sustainably evolving 6G communications.
  Large language models (LLMs), with strong contextual understanding and cross-modal reasoning, provide a promising foundation for intent-aware network agents. Compared with rule-driven or centrally optimized designs, LLM-based agents can integrate heterogeneous information and translate natural-language intents into executable control and configuration decisions.
  Focusing on a closed-loop pipeline of intent perception, autonomous decision making, and network execution, this paper investigates agentic AI for the 6G physical layer and its realization pathways. We review representative physical-layer tasks and their limitations in supporting intent awareness and autonomy, identify application scenarios where agentic AI is advantageous, and discuss key challenges and enabling technologies in multimodal perception, cross-layer decision making, and sustainable optimization. Finally, we present a case study of an intent-driven link decision agent, termed AgenCom, which adaptively constructs communication links under diverse user preferences and channel conditions.

</details>


### [35] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: Proposes a universal human-AI collaboration framework (STRIDE + SR-Delta) to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies, addressing the problem of inconsistent ESG ratings across agencies.


<details>
  <summary>Details</summary>
Motivation: Sustainability/ESG ratings from different agencies for the same company vary widely, limiting their comparability, credibility, and relevance to decision-making, creating a need for harmonization and trustworthy evaluation methods.

Method: Two-part framework: 1) STRIDE provides principled criteria and scoring system for constructing firm-level benchmark datasets using LLMs; 2) SR-Delta is a discrepancy-analysis procedural framework that surfaces insights for potential adjustments.

Result: The framework enables scalable and comparable assessment of sustainability rating methodologies, providing a systematic approach to evaluate and improve ESG rating consistency and reliability.

Conclusion: Calls for broader AI community adoption of AI-powered approaches to strengthen sustainability rating methodologies that support urgent sustainability agendas through improved harmonization and trustworthiness.

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [36] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: O-Shap introduces a new segmentation approach for Owen value-based feature attribution that satisfies the T-property, improving semantic alignment and outperforming baseline SHAP variants in accuracy, coherence, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard Shapley value methods assume feature independence, which breaks down in vision tasks where pixels have spatial and semantic dependencies. While Owen value (hierarchical Shapley) addresses this, its effectiveness depends on feature group definitions, and current segmentation methods violate key consistency properties.

Method: Proposes a new segmentation approach that satisfies the T-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while maintaining consistency properties. The method is applied within the Owen value framework for feature attribution.

Result: Experiments on image and tabular datasets show O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, particularly when structure matters.

Conclusion: The proposed T-property-satisfying segmentation approach for Owen value-based attribution provides more accurate, interpretable, and efficient explanations by properly accounting for feature dependencies and semantic structure in vision tasks.

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [37] [Instructor-Aligned Knowledge Graphs for Personalized Learning](https://arxiv.org/abs/2602.17111)
*Abdulrahman AlRabah,Priyanka Kargupta,Jiawei Han,Abdussalam Alawini*

Main category: cs.AI

TL;DR: InstructKG automatically constructs instructor-aligned knowledge graphs from lecture materials to capture learning dependencies for personalized education.


<details>
  <summary>Details</summary>
Motivation: Mastering educational concepts requires understanding prerequisite and sub-concept relationships, which is critical for identifying knowledge gaps and enabling personalized learning interventions, especially in large-scale courses where individual diagnosis is infeasible.

Method: InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships) from lecture materials (slides, notes). It synergizes temporal and semantic signals from educational materials with large language models' generalizability.

Result: Experiments on real-world, diverse lecture materials across multiple courses with human-based evaluation demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

Conclusion: InstructKG provides an effective framework for automatically constructing knowledge graphs that capture intended learning progressions from instructional materials, addressing limitations of existing surface-level approaches.

Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like "Algorithms" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., "recursion" is taught before "mergesort"; "recursion" is mentioned in the definition of "merge sort") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

</details>


### [38] [Epistemology of Generative AI: The Geometry of Knowing](https://arxiv.org/abs/2602.17116)
*Ilya Levin*

Main category: cs.AI

TL;DR: The paper argues that generative AI requires a new epistemological framework based on high-dimensional geometry, proposing "navigational knowledge" as a third mode of knowledge production distinct from symbolic reasoning and statistical recombination.


<details>
  <summary>Details</summary>
Motivation: Generative AI operates through opaque mechanisms that challenge traditional understandings of knowledge production. Without understanding its epistemic character, responsible integration into science, education, and institutions cannot proceed on principled grounds. Current information theory frameworks (Turing-Shannon-von Neumann tradition) treat semantics as external to computation, which fails to capture how neural networks transform symbolic input into semantic geometric spaces.

Method: The paper develops an "Indexical Epistemology of High-Dimensional Spaces" by analyzing four structural properties of high-dimensional geometry: concentration of measure, near-orthogonality, exponential directional capacity, and manifold regularity. It draws on Peirce's semiotics and Papert's constructionism to reconceptualize generative models as navigators of learned manifolds.

Result: The analysis reveals that neural network architectures rupture the traditional symbolic regime by projecting input into high-dimensional semantic spaces where coordinates correspond to semantic parameters. This geometric space constitutes the active epistemic condition shaping generative production, requiring a paradigmatic break from traditional information theory.

Conclusion: The paper proposes "navigational knowledge" as a third mode of knowledge production, distinct from both symbolic reasoning and statistical recombination. This framework provides the missing epistemological account needed for responsible integration of generative AI into scientific and institutional contexts by understanding how these systems operate within learned semantic manifolds.

Abstract: Generative AI presents an unprecedented challenge to our understanding of knowledge and its production. Unlike previous technological transformations, where engineering understanding preceded or accompanied deployment, generative AI operates through mechanisms whose epistemic character remains obscure, and without such understanding, its responsible integration into science, education, and institutional life cannot proceed on a principled basis. This paper argues that the missing account must begin with a paradigmatic break that has not yet received adequate philosophical attention. In the Turing-Shannon-von Neumann tradition, information enters the machine as encoded binary vectors, and semantics remains external to the process. Neural network architectures rupture this regime: symbolic input is instantly projected into a high-dimensional space where coordinates correspond to semantic parameters, transforming binary code into a position in a geometric space of meanings. It is this space that constitutes the active epistemic condition shaping generative production. Drawing on four structural properties of high-dimensional geometry concentration of measure, near-orthogonality, exponential directional capacity, and manifold regularity the paper develops an Indexical Epistemology of High-Dimensional Spaces. Building on Peirce semiotics and Papert constructionism, it reconceptualizes generative models as navigators of learned manifolds and proposes navigational knowledge as a third mode of knowledge production, distinct from both symbolic reasoning and statistical recombination.

</details>


### [39] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: A parallel algorithm for decomposing hard CircuitSAT instances using specialized constraints to partition SAT instances into weakened formulas, with parameterized parallel execution guided by hardness estimations.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenge of solving hard CircuitSAT instances by developing an efficient parallel decomposition approach that can handle complex problems like Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

Method: Uses specialized constraints to partition original SAT instances into families of weakened formulas, implemented as a parameterized parallel algorithm where parameters can be adjusted to efficiently identify high-quality decompositions, with hardness estimations computed in parallel to guide the process.

Result: Demonstrates practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

Conclusion: The proposed parallel decomposition algorithm provides an effective approach for tackling hard CircuitSAT problems through intelligent partitioning and parallel execution guided by hardness estimations.

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [40] [Bonsai: A Framework for Convolutional Neural Network Acceleration Using Criterion-Based Pruning](https://arxiv.org/abs/2602.17145)
*Joseph Bingham,Sam Helmich*

Main category: cs.AI

TL;DR: Combine is a criterion-based pruning framework for CNNs that provides standardized implementation, comparison of pruning criteria, and achieves up to 79% filter pruning with maintained or improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing CNN pruning solutions lack common implementation standards, making them difficult to implement, compare, and deploy. There's a need for a unified framework to standardize pruning criteria evaluation and implementation.

Method: Introduces Combine, a criterion-based pruning framework that provides standardized language for comparing pruning criteria functions, supports iterative pruning, and includes novel criterion functions for weight removal decisions.

Result: On VGG-inspired models, Combine achieves up to 79% filter pruning while retaining or improving accuracy, reduces computations by up to 68%, and demonstrates that different criteria have varying effects on different models.

Conclusion: Combine provides an effective, fast framework for CNN pruning with standardized criteria comparison, enabling significant model compression while maintaining performance across different architectures.

Abstract: As the need for more accurate and powerful Convolutional Neural Networks (CNNs) increases, so too does the size, execution time, memory footprint, and power consumption. To overcome this, solutions such as pruning have been proposed with their own metrics and methodologies, or criteria, for how weights should be removed. These solutions do not share a common implementation and are difficult to implement and compare. In this work, we introduce Combine, a criterion- based pruning solution and demonstrate that it is fast and effective framework for iterative pruning, demonstrate that criterion have differing effects on different models, create a standard language for comparing criterion functions, and propose a few novel criterion functions. We show the capacity of these criterion functions and the framework on VGG inspired models, pruning up to 79\% of filters while retaining or improving accuracy, and reducing the computations needed by the network by up to 68\%.

</details>


### [41] [JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.17162)
*Ariel Larey,Elay Dahan,Amit Bleiweiss,Raizy Kellerman,Guy Leib,Omri Nayshool,Dan Ofer,Tal Zinger,Dan Dominissini,Gideon Rechavi,Nicole Bussola,Simon Lee,Shane O'Connell,Dung Hoang,Marissa Wirth,Alexander W. Charney,Nati Daniel,Yoli Shavit*

Main category: cs.AI

TL;DR: JEPA-DNA is a novel genomic foundation model pre-training framework that integrates Joint-Embedding Predictive Architecture with traditional generative objectives to capture both local genomic syntax and global functional context.


<details>
  <summary>Details</summary>
Motivation: Current Genomic Foundation Models (GFMs) relying on Masked Language Modeling or Next Token Prediction excel at capturing local genomic syntax and motif patterns but fail to capture broader functional context, resulting in representations lacking global biological perspective.

Method: JEPA-DNA integrates Joint-Embedding Predictive Architecture with traditional generative objectives, introducing latent grounding by coupling token-level recovery with predictive objectives in latent space via CLS token supervision. The model predicts high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides.

Result: JEPA-DNA consistently yields superior performance across diverse genomic benchmarks in both supervised and zero-shot tasks compared to generative-only baselines, providing more robust and biologically grounded representations.

Conclusion: JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet but also the underlying functional logic of sequences, extending both NTP and MLM paradigms and deployable as standalone or continual pre-training enhancement.

Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.

</details>


### [42] [Texo: Formula Recognition within 20M Parameters](https://arxiv.org/abs/2602.17189)
*Sicheng Mao*

Main category: cs.AI

TL;DR: Texo is a lightweight formula recognition model with only 20M parameters that matches SOTA performance while being 80-65% smaller, enabling real-time inference on consumer hardware and browser deployment.


<details>
  <summary>Details</summary>
Motivation: To create a formula recognition model that is both high-performance and lightweight enough for practical deployment on consumer hardware and in web browsers, addressing the computational constraints of real-world applications.

Method: Minimalist architecture design combined with attentive design principles, knowledge distillation, and transfer of vocabulary and tokenizer from larger models to achieve efficiency while maintaining accuracy.

Result: Texo achieves comparable performance to state-of-the-art models (UniMERNet-T and PPFormulaNet-S) while reducing model size by 80% and 65% respectively, enabling real-time inference on consumer-grade hardware.

Conclusion: The paper demonstrates that efficient formula recognition is achievable through careful design and distillation, making advanced mathematical OCR accessible for practical applications including web-based deployment.

Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.

</details>


### [43] [Continual learning and refinement of causal models through dynamic predicate invention](https://arxiv.org/abs/2602.17217)
*Enrique Crespo-Fernandez,Oliver Ray,Telmo de Menezes e Silva Filho,Peter Flach*

Main category: cs.AI

TL;DR: Online symbolic causal world model construction via Meta-Interpretive Learning and predicate invention for sample-efficient, scalable, and interpretable agent learning.


<details>
  <summary>Details</summary>
Motivation: Standard world modelling methods suffer from sample inefficiency, lack of transparency, and poor scalability in complex environments, necessitating a framework that can construct interpretable, reusable abstractions online.

Method: Integrates continuous model learning and repair into agent's decision loop using Meta-Interpretive Learning and predicate invention to create symbolic causal world models, enabling lifted inference that avoids combinatorial explosion of propositional methods.

Result: Achieves orders of magnitude higher sample efficiency than PPO neural-network baseline and scales to domains with complex relational dynamics where propositional methods fail.

Conclusion: The framework successfully constructs hierarchical, disentangled, high-quality concepts from observations, providing transparent, scalable, and sample-efficient world modelling for complex environments.

Abstract: Efficiently navigating complex environments requires agents to internalize the underlying logic of their world, yet standard world modelling methods often struggle with sample inefficiency, lack of transparency, and poor scalability. We propose a framework for constructing symbolic causal world models entirely online by integrating continuous model learning and repair into the agent's decision loop, by leveraging the power of Meta-Interpretive Learning and predicate invention to find semantically meaningful and reusable abstractions, allowing an agent to construct a hierarchy of disentangled, high-quality concepts from its observations. We demonstrate that our lifted inference approach scales to domains with complex relational dynamics, where propositional methods suffer from combinatorial explosion, while achieving sample-efficiency orders of magnitude higher than the established PPO neural-network-based baseline.

</details>


### [44] [From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences](https://arxiv.org/abs/2602.17221)
*Yi-Chih Huang*

Main category: cs.AI

TL;DR: Proposes an AI Agent-based collaborative workflow for humanities/social sciences research, validated using Taiwan's Claude.ai usage data, emphasizing human-AI division of labor and human irreplaceability in research judgment.


<details>
  <summary>Details</summary>
Motivation: Existing generative AI research focuses on software engineering and natural sciences, with limited methodological exploration for humanities and social sciences, creating a gap in AI-assisted research methodologies for these fields.

Method: Designs a seven-stage modular workflow based on three principles: task modularization, human-AI division of labor, and verifiability. Validates using Taiwan's Claude.ai usage data (N=7,729 conversations) from Anthropic Economic Index as empirical demonstration.

Result: Proposes a replicable AI collaboration framework and identifies three operational modes of human-AI collaboration: direct execution, iterative refinement, and human-led. Demonstrates workflow feasibility through empirical analysis of Taiwan AI usage data.

Conclusion: Human judgment remains irreplaceable in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Acknowledges limitations including single-platform data, cross-sectional design, and AI reliability risks.

Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a "methodological experiment," this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.
  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).
  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.

</details>


### [45] [Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight](https://arxiv.org/abs/2602.17222)
*Ben Yellin,Ehud Ezra,Mark Foreman,Shula Grinapol*

Main category: cs.AI

TL;DR: LBM is a behavioral foundation model that uses structured trait profiles instead of persona prompting to predict individual strategic choices with high fidelity, outperforming prompting-based approaches and benefiting from increasingly detailed psychological profiles.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with consistent, individual-specific behavior prediction in high-stakes environments, particularly when predictions depend on complex interactions between psychological traits and situational constraints. Prompting-based approaches are brittle, exhibit identity drift, and have limited ability to leverage detailed persona descriptions.

Method: Introduced Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices. Shifted from transient persona prompting to behavioral embedding by conditioning on structured, high-dimensional trait profiles derived from comprehensive psychometric batteries. Trained on proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices.

Result: LBM fine-tuning improves behavioral prediction relative to unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. While prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided.

Conclusion: LBM establishes a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support by effectively mapping rich psychological profiles to discrete actions across diverse strategic dilemmas.

Abstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.

</details>


### [46] [Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy](https://arxiv.org/abs/2602.17229)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.AI

TL;DR: LLMs encode cognitive complexity levels from Bloom's Taxonomy in linearly separable neural representations, with early resolution of cognitive difficulty during processing.


<details>
  <summary>Details</summary>
Motivation: The black-box nature of LLMs requires new evaluation frameworks beyond surface-level metrics to understand how they internally represent cognitive complexity.

Method: Analyzed high-dimensional activation vectors from different LLMs using Bloom's Taxonomy as a hierarchical lens, probing whether different cognitive levels (Remember to Create) are linearly separable within residual streams.

Result: Linear classifiers achieved ~95% mean accuracy across all Bloom levels, indicating cognitive level is encoded in linearly accessible subspaces of model representations.

Conclusion: Models resolve cognitive difficulty early in forward pass, with representations becoming increasingly separable across layers, providing evidence for structured encoding of cognitive complexity.

Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.

</details>


### [47] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: The paper introduces Shapley-DCLR to measure temporal knowledge leakage in LLMs and proposes TimeSPEC to filter contaminated claims for reliable backtesting.


<details>
  <summary>Details</summary>
Motivation: LLMs may inadvertently use post-cutoff knowledge from training when making retrospective predictions, undermining the validity of backtesting for future event prediction.

Method: Decomposes model rationales into atomic claims, categorizes them by temporal verifiability, uses Shapley values to measure claim contributions, and introduces TimeSPEC for claim verification and regeneration.

Result: Experiments on Supreme Court cases, NBA salaries, and stock returns show substantial leakage in standard prompting; TimeSPEC reduces Shapley-DCLR while preserving task performance.

Conclusion: Explicit, interpretable claim-level verification (TimeSPEC) outperforms prompt-based temporal constraints for reliable backtesting by filtering temporal contamination.

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [48] [Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web](https://arxiv.org/abs/2602.17245)
*Linxi Jiang,Rui Xi,Zhijie Liu,Shuo Chen,Zhiqiang Lin,Suman Nath*

Main category: cs.AI

TL;DR: Web Verbs proposes a semantic layer of typed, documented functions that expose website capabilities through a uniform interface, enabling LLM agents to compose reliable, efficient, and verifiable workflows by abstracting away low-level browser interactions.


<details>
  <summary>Details</summary>
Motivation: Current web agents operate on low-level primitives (clicks, keystrokes) that are brittle, inefficient, and difficult to verify. As the web evolves into an agentic environment where software agents act on behalf of users, there's a need for a semantic layer for web actions that complements content-oriented semantic layers like NLWeb.

Method: Proposes Web Verbs - a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface. These verbs can be implemented through APIs or robust client-side workflows, and include preconditions, postconditions, policy tags, and logging support. They serve as stable, composable units that agents can discover, select, and synthesize into concise programs.

Result: The approach demonstrates improved reliability through stable interfaces, efficiency by reducing dozens of steps into few function calls, and verifiability through typed contracts and checkable traces. A proof-of-concept implementation and case studies show concise and robust execution compared to existing agents.

Conclusion: Web Verbs provides a semantic abstraction layer that unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. The paper outlines a roadmap for standardization to make verbs deployable and trustworthy at web scale.

Abstract: The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \textbf{reliability} by providing stable interfaces, \textbf{efficiency} by reducing dozens of steps into a few function calls, and \textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale.

</details>


### [49] [ArXiv-to-Model: A Practical Study of Scientific LM Training](https://arxiv.org/abs/2602.17288)
*Anuj Gupta*

Main category: cs.AI

TL;DR: Training a 1.36B-parameter scientific language model from raw arXiv LaTeX sources using constrained compute (2xA100 GPUs), with detailed analysis of preprocessing, tokenization, and infrastructure bottlenecks.


<details>
  <summary>Details</summary>
Motivation: Despite strong reasoning capabilities in frontier LLMs, there's limited documentation on the practical process of training domain-specialized scientific language models from raw sources. The paper aims to provide transparent engineering guidance for researchers with moderate compute budgets.

Method: End-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training. Conducted 24 experimental runs analyzing training stability, scaling behavior, data yield losses, and infrastructure bottlenecks using 2xA100 GPUs.

Result: Preprocessing decisions significantly affect usable token volume; tokenization impacts symbolic stability; storage and I/O constraints can rival compute as limiting factors. Demonstrated stable training behavior in data-rich regime (52B pretraining tokens) with 1.36B-parameter model.

Conclusion: Provides engineering-grounded, transparent account of training small scientific language models from scratch, offering practical insights for researchers with constrained compute budgets seeking to build domain-specialized models.

Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.

</details>


### [50] [MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions](https://arxiv.org/abs/2602.17308)
*Hui Min Wong,Philip Heesen,Pascal Janetzky,Martin Bendszus,Stefan Feuerriegel*

Main category: cs.AI

TL;DR: MedClarify is an AI agent that generates follow-up questions to reduce diagnostic uncertainty, improving medical LLM performance by ~27 percentage points through iterative, information-theoretic reasoning.


<details>
  <summary>Details</summary>
Motivation: Current medical LLMs lack the ability to engage in systematic, iterative reasoning like clinicians who use follow-up questioning to resolve diagnostic uncertainty. Real-world clinical diagnosis requires considering differential diagnoses and excluding emergencies through active information-seeking, which existing LLMs don't adequately support.

Method: MedClarify computes candidate diagnoses (differential diagnosis), then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. It selects questions with the highest expected information gain using an information-theoretic reasoning approach, enabling targeted, uncertainty-aware reasoning.

Result: The approach reduces diagnostic errors by ~27 percentage points compared to standard single-shot LLM baselines. It effectively generates follow-up questioning that addresses the limitations of current LLMs, which often yield multiple similarly likely diagnoses when cases are incomplete.

Conclusion: MedClarify offers a path to improve medical LLMs through agentic information-seeking, promoting effective dialogues that reflect the iterative and uncertain nature of real-world clinical reasoning.

Abstract: Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.

</details>


### [51] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: A dataless regularization method using curvature matrix approximation to mitigate cross-task interference in task arithmetic without requiring external task data.


<details>
  <summary>Details</summary>
Motivation: Task arithmetic enables modular adaptation of foundation models but suffers from cross-task interference causing representation drift and performance degradation. Existing regularization approaches require external task data, which conflicts with modularity and privacy constraints.

Method: Frames regularization against representation drift as a curvature matrix approximation problem, leveraging Kronecker-Factored Approximate Curvature (K-FAC) to obtain a practical dataless regularizer with constant complexity in number of tasks.

Result: Achieves state-of-the-art results in task addition and negation, promotes robustness to task vector rescaling, and eliminates the need for held-out tuning.

Conclusion: Proposes an effective dataless regularization approach for task arithmetic that addresses representation drift without requiring external task data, maintaining modularity while improving performance and robustness.

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [52] [Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval](https://arxiv.org/abs/2602.17386)
*Adrià Molina,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.AI

TL;DR: A novel framework integrates formal verification with deep learning-based image retrieval to handle complex natural language queries with precise constraints, providing transparent and verifiable results.


<details>
  <summary>Details</summary>
Motivation: Current embedding-based models struggle with complex queries involving relationships, object compositions, and precise constraints (identities, counts, proportions), lacking reliability and verifiability in results.

Method: Integration of formal verification into deep learning-based image retrieval through graph-based verification methods and neural code generation, grounding retrieval in formal reasoning to verify each atomic truth in user queries.

Result: Framework supports open-vocabulary natural language queries while producing trustworthy, verifiable results that explicitly indicate which constraints are satisfied/unmet, improving transparency and boosting performance over embedding-based approaches.

Conclusion: By moving beyond ambiguous vector representations to formal verification, the approach offers more transparent, accountable retrieval that handles complex constraints while enhancing existing embedding-based methods.

Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.

</details>


### [53] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: MCVAE: A multimodal contrastive variational autoencoder for NSCLC survival prediction that handles missing modalities via learned gating, stochastic masking, and multi-task learning.


<details>
  <summary>Details</summary>
Motivation: Predicting NSCLC survival is challenging due to individual variability; multimodal integration (WSI, transcriptomics, methylation) offers complementary information but real-world datasets often have missing modalities, and current methods lack robustness to severe missingness.

Method: Multimodal Contrastive Variational AutoEncoder (MCVAE) with modality-specific variational encoders, fusion bottleneck with learned gating to normalize contributions from present modalities, multi-task objective combining survival and reconstruction losses, cross-modal contrastive loss for latent space alignment, and stochastic modality masking during training.

Result: Extensive evaluations on TCGA-LUAD (n=475) and TCGA-LUSC (n=446) show MCVAE's efficacy in predicting disease-specific survival and robustness to severe missingness compared to two state-of-the-art models; analysis reveals multimodal integration is not always beneficial.

Conclusion: MCVAE effectively addresses missing modality challenges in NSCLC survival prediction through robust multimodal integration with learned gating and contrastive learning, though careful evaluation of modality subsets is needed as integration doesn't always improve performance.

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [54] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: A framework integrating Privacy-by-Design principles from multiple regulations (GDPR, PIPEDA, COPPA) and child-centered design guidelines (UNCRC, AADC) to address privacy risks in AI applications for children, particularly LLM-based systems, with a case study of an educational tutor.


<details>
  <summary>Details</summary>
Motivation: Growing concerns about privacy risks for children using AI technologies, despite existing privacy regulations. Challenges in implementing protections in practice, especially for AI applications like LLMs used by children.

Method: Proposes a Privacy-by-Design framework that maps regulatory principles (GDPR, PIPEDA, COPPA) to LLM lifecycle stages (data collection, model training, operational monitoring, validation). Integrates child-centered design guidelines from UNCRC, AADC, and academic research. Includes operational controls from literature and demonstrates application through a case study of an LLM-based educational tutor for children under 13.

Result: Framework shows how data protection strategies (technical/organizational controls) and age-appropriate design decisions throughout the LLM lifecycle can support development of AI applications for children that provide privacy protections and comply with legal requirements.

Conclusion: The Privacy-by-Design framework enables proactive, risk-averse approach to AI technology design for children, helping developers reduce privacy risks while meeting legal standards through integrated regulatory principles and child-centered design guidelines applied across LLM lifecycle stages.

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [55] [WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation](https://arxiv.org/abs/2602.17442)
*Marco Avolio,Potito Aghilar,Sabino Roccotelli,Vito Walter Anelli,Chiara Mallamaci,Vincenzo Paparella,Marco Valentini,Alejandro Bellogín,Michelantonio Trizio,Joseph Trotta,Antonio Ferrara,Tommaso Di Noia*

Main category: cs.AI

TL;DR: WarpRec is a high-performance recommender system framework that bridges academia-industry gap with backend-agnostic architecture, enabling seamless transition from local to distributed execution while incorporating sustainability metrics and preparing for agentic AI integration.


<details>
  <summary>Details</summary>
Motivation: The fractured ecosystem in recommender systems research forces researchers to choose between easy in-memory experimentation and costly distributed industrial engines, creating a barrier to innovation and practical implementation.

Method: Developed a novel backend-agnostic architecture with 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering/splitting strategies that seamlessly transition from local execution to distributed training and optimization, integrated with CodeCarbon for real-time energy tracking.

Result: Created a framework that eliminates the trade-off between experimental ease and industrial scalability, demonstrating that scalability doesn't require sacrificing scientific integrity or sustainability, while anticipating the shift toward agentic AI in recommender systems.

Conclusion: WarpRec bridges the academia-industry gap and serves as an architectural backbone for next-generation sustainable, agent-ready recommender systems, with code publicly available for community use and development.

Abstract: Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/

</details>


### [56] [Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems](https://arxiv.org/abs/2602.17508)
*Pranay Jain,Maximilian Kasper,Göran Köber,Axel Plinge,Dominik Seuß*

Main category: cs.AI

TL;DR: A benchmarking framework for optimizing AI models on ARM Cortex processors (M0+, M4, M7) with focus on energy efficiency, accuracy, and resource utilization in embedded systems, revealing processor-specific optimization strategies.


<details>
  <summary>Details</summary>
Motivation: The need for practical guidance on selecting optimal processor-AI model combinations for energy-efficient embedded AI applications, addressing the trade-offs between computational performance, accuracy, and power consumption in resource-constrained environments.

Method: Developed an automated test bench for systematic evaluation across key performance indicators (KPIs), analyzed correlation between FLOPs and inference time, and applied Pareto analysis to balance energy consumption vs. model accuracy trade-offs.

Result: Found near-linear correlation between FLOPs and inference time; identified M7 as optimal for short inference cycles, M4 for energy-efficient longer inference tasks, and M0+ suitable for simpler AI models; demonstrated how to balance energy-accuracy trade-offs.

Conclusion: Provides practical insights for developers to design energy-efficient AI systems on ARM Cortex processors, offering processor-specific recommendations and a systematic framework for optimizing AI model deployment in embedded applications.

Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.

</details>


### [57] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: KG-RAG integrates knowledge graphs with retrieval-augmented generation to enhance LLMs for telecom-specific tasks, improving accuracy and reducing hallucinations in complex telecom scenarios.


<details>
  <summary>Details</summary>
Motivation: General-domain LLMs struggle with telecom applications due to domain complexity, evolving standards, specialized terminology, leading to hallucinations and reduced utility in telecom operations.

Method: KG-RAG framework combines knowledge graphs (structured domain knowledge from telecom standards/technical docs) with retrieval-augmented generation (dynamic retrieval of relevant facts) to ground LLM outputs.

Result: KG-RAG outperforms both LLM-only and standard RAG baselines, achieving average accuracy improvements of 14.3% over RAG and 21.6% over LLM-only models across benchmark datasets.

Conclusion: KG-RAG effectively produces accurate, reliable, and explainable outputs in complex telecom scenarios by integrating structured domain knowledge with dynamic retrieval mechanisms.

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [58] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: The paper introduces two new metrics (reusability and verifiability) to evaluate Chain-of-Thought reasoning quality in multi-agent systems, finding they don't correlate with standard accuracy and that specialized reasoning models don't produce consistently better CoTs than general-purpose LLMs.


<details>
  <summary>Details</summary>
Motivation: Current CoT evaluation focuses only on target task accuracy, which fails to assess the quality or utility of the reasoning process itself in multi-agent IR pipelines where agents exchange CoT reasoning.

Method: Introduced reusability (how easily Executor can reuse Thinker's CoT) and verifiability (how frequently Executor matches Thinker's answer using CoT) metrics. Used Thinker-Executor framework to decouple CoT generation from execution. Evaluated four Thinker models against committee of ten Executor models across five benchmarks.

Result: Reusability and verifiability do not correlate with standard accuracy, exposing blind spot in current accuracy-based leaderboards. Surprisingly, CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

Conclusion: The paper demonstrates the need for new evaluation metrics beyond accuracy to properly assess reasoning quality in multi-agent systems, revealing limitations in current evaluation approaches and unexpected findings about specialized vs. general-purpose models.

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [59] [KLong: Training LLM Agent for Extremely Long-horizon Tasks](https://arxiv.org/abs/2602.17547)
*Yue Liu,Zhiyuan Hu,Flood Sung,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: KLong is an open-source LLM agent trained for extremely long-horizon tasks using trajectory-splitting SFT and progressive RL training, achieving state-of-the-art performance on research paper benchmarks.


<details>
  <summary>Details</summary>
Motivation: The need for LLM agents capable of solving extremely long-horizon tasks, particularly in research and complex problem-solving domains where current models struggle with extended reasoning chains and context management.

Method: Two-stage training: 1) Cold-start via trajectory-splitting SFT that preserves early context while progressively truncating later context with overlap between sub-trajectories; 2) Progressive RL training with multiple stages of progressively extended timeouts. Uses Research-Factory pipeline to generate high-quality training data from research papers with evaluation rubrics.

Result: KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, with performance improvements generalizing to other coding benchmarks like SWE-bench Verified and MLE-bench.

Conclusion: The proposed trajectory-splitting SFT and progressive RL training effectively enable LLM agents to handle extremely long-horizon tasks, demonstrating superior performance and generalization capabilities compared to larger models.

Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.

</details>


### [60] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: ODESteer: A unified ODE-based theoretical framework for activation steering in LLM alignment that interprets conventional activation addition as first-order ODE approximation and uses barrier functions for multi-step adaptive steering.


<details>
  <summary>Details</summary>
Motivation: Current activation steering methods lack unified theoretical framework and rely on one-step steering that fails to capture complex activation distribution patterns, limiting their effectiveness in LLM alignment.

Method: Proposes ODE-based theoretical framework where activation steering is formulated as solving ODEs; conventional activation addition is first-order ODE approximation; uses barrier functions (log-density ratio between positive/negative activations) to construct ODEs for multi-step adaptive steering.

Result: ODESteer achieves consistent improvements over SOTA methods: 5.7% improvement on TruthfulQA, 2.5% on UltraFeedback, 2.4% on RealToxicityPrompts across diverse LLM alignment benchmarks.

Conclusion: Establishes principled ODE-based theoretical foundation for activation steering, validated empirically by ODESteer's superior performance, providing unified framework for designing steering directions via barrier functions.

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


### [61] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: A hybrid federated learning approach combining SWIN Transformer and CNN models (DenseNet201, Inception V3, VGG19) for secure, distributed lung disease diagnosis (COVID-19 and Pneumonia) from X-ray images.


<details>
  <summary>Details</summary>
Motivation: To leverage AI advancements in healthcare while addressing data privacy concerns through federated learning, creating a secure distributed system for medical data processing that can help physicians with reliable disease diagnosis.

Method: Hybrid ensemble approach combining SWIN Transformer with CNN models (DenseNet201, Inception V3, VGG19) implemented using TensorFlow/Keras, integrated with federated learning for distributed, secure training across medical institutions.

Result: The proposed system enables accurate detection of COVID-19 and Pneumonia from X-ray reports while maintaining data privacy through federated learning, providing a reliable diagnostic tool for physicians.

Conclusion: Federated learning-based hybrid AI models can improve disease diagnosis accuracy and severity prediction while ensuring data security and authenticity, offering a practical solution for collaborative medical AI applications.

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [62] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: The paper proposes evaluating AI general intelligence through comprehensive game playing across all conceivable human games, introduces AI GameStore as a scalable platform for this purpose, and demonstrates current VLMs perform poorly (under 10% of human average) on 100 generated games.


<details>
  <summary>Details</summary>
Motivation: Current AI benchmarks are too narrow, static, and quickly saturated. There's a need for better evaluation methods that can assess human-like general intelligence across the broad spectrum of human capabilities.

Method: Proposes evaluating AI through comprehensive game playing across all conceivable human games. Introduces AI GameStore platform that uses LLMs with humans-in-the-loop to synthesize new representative human games by sourcing and adapting standardized variants from popular gaming platforms (Apple App Store, Steam). Generated 100 games as proof of concept.

Result: Evaluated seven frontier vision-language models (VLMs) on short gameplay episodes. Best models achieved less than 10% of human average score on majority of games. Models particularly struggled with games challenging world-model learning, memory, and planning.

Conclusion: The AI GameStore approach shows promise for measuring and driving progress toward human-like general intelligence. Current models perform poorly on comprehensive game evaluation, highlighting gaps in world-model learning, memory, and planning capabilities.

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


### [63] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: MolHIT introduces a hierarchical discrete diffusion model for molecular graph generation that achieves near-perfect chemical validity and state-of-the-art performance, surpassing 1D baselines on MOSES dataset.


<details>
  <summary>Details</summary>
Motivation: Existing graph diffusion models for molecular generation suffer from low chemical validity and struggle to meet desired properties compared to 1D modeling approaches, creating a performance gap that needs to be addressed.

Method: MolHIT uses a Hierarchical Discrete Diffusion Model that generalizes discrete diffusion to additional categories encoding chemical priors, combined with decoupled atom encoding that splits atom types according to their chemical roles.

Result: Achieves new state-of-the-art performance on MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics, and demonstrates strong performance in downstream tasks including multi-property guided generation and scaffold extension.

Conclusion: MolHIT represents a significant advancement in molecular graph generation, overcoming long-standing performance limitations and establishing graph diffusion as a competitive approach for AI-driven drug discovery and materials science.

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>


### [64] [AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing](https://arxiv.org/abs/2602.17607)
*Jianda Du,Youran Sun,Haizhao Yang*

Main category: cs.AI

TL;DR: AutoNumerics is a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for PDEs from natural language descriptions, generating transparent solvers grounded in classical numerical analysis rather than black-box neural approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional PDE solver design requires substantial mathematical expertise and manual tuning, while recent neural network approaches suffer from high computational costs and limited interpretability. There's a need for accessible, automated PDE solving that maintains transparency and interpretability.

Method: Multi-agent framework with coarse-to-fine execution strategy and residual-based self-verification mechanism. The system generates transparent solvers grounded in classical numerical analysis directly from natural language PDE descriptions.

Result: Experiments on 24 canonical and real-world PDE problems show competitive or superior accuracy compared to existing neural and LLM-based baselines. The framework correctly selects numerical schemes based on PDE structural properties.

Conclusion: AutoNumerics demonstrates viability as an accessible paradigm for automated PDE solving, offering transparent, interpretable solvers that bridge the gap between traditional numerical analysis and modern AI approaches.

Abstract: PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.

</details>


### [65] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026 is a CLEF evaluation lab focused on extracting person-place relations from historical texts, extending previous campaigns to semantic relation extraction with temporal reasoning requirements.


<details>
  <summary>Details</summary>
Motivation: To advance historical text processing by moving from named entity recognition to semantic relation extraction, specifically person-place associations, which supports downstream applications in digital humanities like knowledge-graph construction and historical biography reconstruction.

Method: A three-fold evaluation profile assessing: 1) accuracy of relation extraction, 2) computational efficiency, and 3) domain generalization across multiple languages and time periods. Systems must classify two relation types ($at$ and $isAt$) requiring temporal and geographical reasoning.

Result: The lab establishes a benchmark for person-place relation extraction from noisy historical texts, building on HIPE-2020 and HIPE-2022 campaigns, with evaluation across multilingual and multi-temporal datasets.

Conclusion: HIPE-2026 advances historical text processing by introducing semantic relation extraction as an evaluation task, linking relation extraction to large-scale historical data processing to support digital humanities applications.

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [66] [ICP-Based Pallet Tracking for Unloading on Inclined Surfaces by Autonomous Forklifts](https://arxiv.org/abs/2602.16744)
*Takuro Kato,Mitsuharu Morisawa*

Main category: cs.RO

TL;DR: Autonomous forklift control method for unloading pallets on inclined surfaces using ICP algorithm to track pallet-fork alignment and enable drag-free withdrawal.


<details>
  <summary>Details</summary>
Motivation: Current autonomous forklifts struggle with unloading pallets on inclined surfaces, often causing pallet dragging during fork withdrawal due to misalignment between the fork and the target surface.

Method: Uses Iterative Closest Point (ICP) algorithm on point clouds from the pallet's upper region to track relative position and attitude angle between pallet and fork in real-time, then aligns fork parallel to target surface before withdrawal.

Result: Method effectiveness verified through dynamic simulations and real forklift experiments replicating unloading operations onto inclined truck beds, demonstrating successful drag-free pallet unloading.

Conclusion: Proposed ICP-based control method enables autonomous forklifts to unload pallets on inclined surfaces without dragging by achieving proper fork-pallet alignment before withdrawal.

Abstract: This paper proposes a control method for autonomous forklifts to unload pallets on inclined surfaces, enabling the fork to be withdrawn without dragging the pallets. The proposed method applies the Iterative Closest Point (ICP) algorithm to point clouds measured from the upper region of the pallet and thereby tracks the relative position and attitude angle difference between the pallet and the fork during the unloading operation in real-time. According to the tracking result, the fork is aligned parallel to the target surface. After the fork is aligned, it is possible to complete the unloading process by withdrawing the fork along the tilt, preventing any dragging of the pallet. The effectiveness of the proposed method is verified through dynamic simulations and experiments using a real forklift that replicate unloading operations onto the inclined bed of a truck.

</details>


### [67] [Smooth trajectory generation and hybrid B-splines-Quaternions based tool path interpolation for a 3T1R parallel kinematic milling robot](https://arxiv.org/abs/2602.16758)
*Sina Akhbari,Mehran Mahboubkhah*

Main category: cs.RO

TL;DR: Smooth trajectory generation method for 4-DOF parallel kinematic milling robot using B-spline/quaternion interpolation with Bezier curve synchronization and minimum jerk optimization.


<details>
  <summary>Details</summary>
Motivation: Need for accurate, smooth trajectory generation in parallel kinematic milling robots to handle decoupled position/orientation data while ensuring spatial/temporal constraints and avoiding gimbal lock issues.

Method: Integrates B-spline and quaternion interpolation for position/orientation decoupling. Uses smooth piece-wise Bezier curves for synchronization via sequential quadratic programming. Employs unit quaternions for orientation interpolation and modifier polynomials for position. Two-stage optimization: task space then joint space minimum jerk time-optimal Bezier curves on low-cost microcontroller.

Result: Experimental results show enhanced accuracy, reduced velocity fluctuations, and computational efficiency compared to conventional interpolation methods.

Conclusion: Proposed method effectively generates smooth trajectories for parallel kinematic milling robots with improved performance metrics while being implementable on low-cost hardware.

Abstract: This paper presents a smooth trajectory generation method for a four-degree-of-freedom parallel kinematic milling robot. The proposed approach integrates B-spline and Quaternion interpolation techniques to manage decoupled position and orientation data points. The synchronization of orientation and arc-length-parameterized position data is achieved through the fitting of smooth piece-wise Bezier curves, which describe the non-linear relationship between path length and tool orientation, solved via sequential quadratic programming. By leveraging the convex hull properties of Bezier curves, the method ensures spatial and temporal separation constraints for multi-agent trajectory generation. Unit quaternions are employed for orientation interpolation, providing a robust and efficient representation that avoids gimbal lock and facilitates smooth, continuous rotation. Modifier polynomials are used for position interpolation. Temporal trajectories are optimized using minimum jerk, time-optimal piece-wise Bezier curves in two stages: task space followed by joint space, implemented on a low-cost microcontroller. Experimental results demonstrate that the proposed method offers enhanced accuracy, reduced velocity fluctuations, and computational efficiency compared to conventional interpolation methods.

</details>


### [68] [RRT$^η$: Sampling-based Motion Planning and Control from STL Specifications using Arithmetic-Geometric Mean Robustness](https://arxiv.org/abs/2602.16825)
*Ahmad Ahmad,Shuo Liu,Roberto Tron,Calin Belta*

Main category: cs.RO

TL;DR: RRT$^η$: A sampling-based motion planning framework that integrates Arithmetic-Geometric Mean (AGM) robustness measure for STL specifications, enabling smoother optimization landscapes and improved tree exploration compared to traditional min-max robustness approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional sampling-based planners with STL specifications rely on min-max robustness measures that create non-smooth optimization landscapes with sharp decision boundaries, focusing only on critical time points and subformulae, which hinders efficient tree exploration.

Method: Proposes RRT$^η$ framework integrating AGM robustness measure with three key contributions: (1) AGM robustness interval semantics for partial trajectory reasoning, (2) efficient incremental monitoring algorithm for interval computation, and (3) enhanced Direction of Increasing Satisfaction vectors using Fulfillment Priority Logic for principled objective composition.

Result: The framework synthesizes dynamically feasible control sequences satisfying STL specifications with high robustness while maintaining probabilistic completeness and asymptotic optimality of RRT$^\ast$. Validated on three robotic systems (double integrator, unicycle mobile robot, 7-DOF arm) showing superior performance over traditional STL robustness-based planners in multi-constraint scenarios with limited guidance.

Conclusion: RRT$^η$ successfully addresses limitations of traditional min-max robustness measures by providing smoother optimization landscapes through AGM robustness, enabling more efficient exploration of complex configuration spaces while satisfying STL specifications with high robustness.

Abstract: Sampling-based motion planning has emerged as a powerful approach for robotics, enabling exploration of complex, high-dimensional configuration spaces. When combined with Signal Temporal Logic (STL), a temporal logic widely used for formalizing interpretable robotic tasks, these methods can address complex spatiotemporal constraints. However, traditional approaches rely on min-max robustness measures that focus only on critical time points and subformulae, creating non-smooth optimization landscapes with sharp decision boundaries that hinder efficient tree exploration.
  We propose RRT$^η$, a sampling-based planning framework that integrates the Arithmetic-Geometric Mean (AGM) robustness measure to evaluate satisfaction across all time points and subformulae. Our key contributions include: (1) AGM robustness interval semantics for reasoning about partial trajectories during tree construction, (2) an efficient incremental monitoring algorithm computing these intervals, and (3) enhanced Direction of Increasing Satisfaction vectors leveraging Fulfillment Priority Logic (FPL) for principled objective composition. Our framework synthesizes dynamically feasible control sequences satisfying STL specifications with high robustness while maintaining the probabilistic completeness and asymptotic optimality of RRT$^\ast$. We validate our approach on three robotic systems. A double integrator point robot, a unicycle mobile robot, and a 7-DOF robot arm, demonstrating superior performance over traditional STL robustness-based planners in multi-constraint scenarios with limited guidance signals.

</details>


### [69] [Sound of Touch: Active Acoustic Tactile Sensing via String Vibrations](https://arxiv.org/abs/2602.16846)
*Xili Yi,Ying Xing,Zachary Manchester,Nima Fazeli*

Main category: cs.RO

TL;DR: Sound of Touch: Active acoustic tactile sensing using vibrating tensioned strings with electromagnetic excitation and contact microphones to detect contact location, normal force, and slip through spectral analysis.


<details>
  <summary>Details</summary>
Motivation: Distributed tactile sensing is difficult to scale over large areas due to wiring complexity, cost, fragility of dense sensor arrays, and limited coverage or missed fast dynamics in existing alternatives.

Method: Uses vibrating tensioned strings as sensing elements with continuous electromagnetic excitation and a small number of pickups (contact microphones) to observe spectral changes induced by contact. Includes physics-based string-vibration simulator to predict contact-induced vibration mode shifts and real-time inference pipeline mapping vibration measurements to contact state.

Result: Demonstrates millimeter-scale localization, reliable force estimation, and real-time slip detection. The system estimates contact location, normal force, and detects slip from short-duration audio signals.

Conclusion: Presents a lightweight, scalable string-based tactile sensing hardware concept for instrumenting extended robot surfaces, with physics-grounded simulation tools and real-time inference capabilities for contact state estimation.

Abstract: Distributed tactile sensing remains difficult to scale over large areas: dense sensor arrays increase wiring, cost, and fragility, while many alternatives provide limited coverage or miss fast interaction dynamics. We present Sound of Touch, an active acoustic tactile-sensing methodology that uses vibrating tensioned strings as sensing elements. The string is continuously excited electromagnetically, and a small number of pickups (contact microphones) observe spectral changes induced by contact. From short-duration audio signals, our system estimates contact location and normal force, and detects slip. To guide design and interpret the sensing mechanism, we derive a physics-based string-vibration simulator that predicts how contact position and force shift vibration modes. Experiments demonstrate millimeter-scale localization, reliable force estimation, and real-time slip detection. Our contributions are: (i) a lightweight, scalable string-based tactile sensing hardware concept for instrumenting extended robot surfaces; (ii) a physics-grounded simulation and analysis tool for contact-induced spectral shifts; and (iii) a real-time inference pipeline that maps vibration measurements to contact state.

</details>


### [70] ["Hello, I'm Delivering. Let Me Pass By": Navigating Public Pathways with Walk-along with Robots in Crowded City Streets](https://arxiv.org/abs/2602.16861)
*EunJeong Cheon,Do Yeon Shin*

Main category: cs.RO

TL;DR: Proposes Walk-Along with Robots (WawR) methodology for studying autonomous robots in public spaces, inspired by public realm ethnography, to address limitations of controlled experiments and structured observations.


<details>
  <summary>Details</summary>
Motivation: As autonomous robots increasingly operate in public spaces, existing HRI research methods (controlled experiments, Wizard of Oz, structured observations) are inadequate for studying real-world autonomous robots that operate beyond researcher control in dynamic, unpredictable environments.

Method: Proposes Walk-Along with Robots (WawR) methodology inspired by public realm ethnography from urban studies, geography, and sociology. The method involves accompanying robots in public spaces to observe their interactions and navigation in real-world contexts.

Result: The paper outlines key features of WawR methodology, steps applied in their study, unique insights it offers, and ways it can be evaluated, though specific empirical results are not detailed in the abstract.

Conclusion: WawR methodology provides a more deliberate approach for studying autonomous robots in public spaces and should stimulate further discussion on research methodologies for this emerging domain.

Abstract: As the presence of autonomous robots in public spaces increases-whether navigating campus walkways or neighborhood sidewalks-understanding how to carefully study these robots becomes critical. While HRI research has conducted field studies in public spaces, these are often limited to controlled experiments with prototype robots or structured observational methods, such as the Wizard of Oz technique. However, the autonomous mobile robots we encounter today, particularly delivery robots, operate beyond the control of researchers, navigating dynamic routes and unpredictable environments. To address this challenge, a more deliberate approach is required. Drawing inspiration from public realm ethnography in urban studies, geography, and sociology, this paper proposes the Walk-Along with Robots (WawR) methodology. We outline the key features of this method, the steps we applied in our study, the unique insights it offers, and the ways it can be evaluated. We hope this paper stimulates further discussion on research methodologies for studying autonomous robots in public spaces.

</details>


### [71] [SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation](https://arxiv.org/abs/2602.16863)
*Kushal Kedia,Tyler Ga Wei Lum,Jeannette Bohg,C. Karen Liu*

Main category: cs.RO

TL;DR: SimToolReal is a sim-to-real RL approach for general dexterous tool manipulation that trains a single policy on procedurally generated tool-like objects, enabling zero-shot generalization to real-world tools without task-specific training.


<details>
  <summary>Details</summary>
Motivation: Tool manipulation is challenging for robots due to requirements for grasping thin objects, in-hand rotations, and forceful interactions. Teleoperation data collection is difficult, and existing sim-to-real RL approaches require substantial engineering effort for object modeling and reward tuning per task.

Method: Procedurally generate diverse tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses, enabling generalization without object or task-specific training.

Result: Outperforms prior retargeting and fixed-grasp methods by 37%, matches performance of specialist RL policies trained on specific objects/tasks, and achieves strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.

Conclusion: SimToolReal demonstrates that training on procedurally generated tool-like objects enables a single RL policy to generalize to diverse real-world tool manipulation tasks without task-specific training, representing progress toward generalizable sim-to-real RL for dexterous manipulation.

Abstract: The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.

</details>


### [72] [Boreas Road Trip: A Multi-Sensor Autonomous Driving Dataset on Challenging Roads](https://arxiv.org/abs/2602.16870)
*Daniil Lisus,Katya M. Papais,Cedric Le Gentil,Elliot Preston-Krebs,Andrew Lambert,Keith Y. K. Leung,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: Boreas-RT extends the Boreas dataset with 643 km of diverse driving data across 9 routes, providing multi-modal sensor data with centimeter-level ground truth to benchmark odometry and localization algorithms in challenging real-world conditions.


<details>
  <summary>Details</summary>
Motivation: Existing autonomous driving datasets often lack diversity in road conditions and environments, causing state-of-the-art odometry and localization algorithms to overfit to simple driving scenarios and degrade in more challenging real-world conditions.

Method: Collected 60 sequences over 9 real-world routes (643 km total) using a multi-sensor platform: 5MP FLIR camera, 360° Navtech Doppler radar, 128-channel Velodyne lidar, Aeva FMCW Doppler lidar, IMU, and wheel encoder. Provided centimeter-level ground truth via post-processed GNSS-INS, precise calibrations, development kit, and live leaderboard.

Result: Benchmark results show many state-of-the-art odometry and localization algorithms overfit to simple environments and degrade significantly on the more challenging Boreas-RT routes, demonstrating the dataset's ability to reveal algorithm limitations.

Conclusion: Boreas-RT provides a unified, diverse dataset for evaluating multi-modal autonomous driving algorithms, particularly for odometry and metric localization, with tools for benchmarking and development available publicly.

Abstract: The Boreas Road Trip (Boreas-RT) dataset extends the multi-season Boreas dataset to new and diverse locations that pose challenges for modern autonomous driving algorithms. Boreas-RT comprises 60 sequences collected over 9 real-world routes, totalling 643 km of driving. Each route is traversed multiple times, enabling evaluation in identical environments under varying traffic and, in some cases, weather conditions. The data collection platform includes a 5MP FLIR Blackfly S camera, a 360 degree Navtech RAS6 Doppler-enabled spinning radar, a 128-channel 360 degree Velodyne Alpha Prime lidar, an Aeva Aeries II FMCW Doppler-enabled lidar, a Silicon Sensing DMU41 inertial measurement unit, and a Dynapar wheel encoder. Centimetre-level ground truth is provided via post-processed Applanix POS LV GNSS-INS data. The dataset includes precise extrinsic and intrinsic calibrations, a publicly available development kit, and a live leaderboard for odometry and metric localization. Benchmark results show that many state-of-the-art odometry and localization algorithms overfit to simple driving environments and degrade significantly on the more challenging Boreas-RT routes. Boreas-RT provides a unified dataset for evaluating multi-modal algorithms across diverse road conditions. The dataset, leaderboard, and development kit are available at www.boreas.utias.utoronto.ca.

</details>


### [73] [MALLVI: a multi agent framework for integrated generalized robotics manipulation](https://arxiv.org/abs/2602.16898)
*Iman Ahmadi,Mehrshad Taji,Arad Mahdinezhad Kashani,AmirHossein Jadidi,Saina Kashani,Babak Khalaj*

Main category: cs.RO

TL;DR: MALLVi is a multi-agent LLM/VLM framework for closed-loop robotic manipulation that coordinates specialized agents for perception, localization, reasoning, and planning with environmental feedback.


<details>
  <summary>Details</summary>
Motivation: Prior LLM-based task planning approaches are fragile in dynamic settings due to open-loop operation without robust environmental feedback, reliance on specialized models/fine-tuning, and lack of adaptive error recovery.

Method: Multi-agent framework with specialized agents: Decomposer (perception), Localizer (object localization), Thinker (reasoning), Reflector (high-level planning/error recovery), and optional Descriptor (visual memory). Uses closed-loop feedback where VLM evaluates execution results and decides next steps, with targeted error recovery by reactivating only relevant agents.

Result: Experiments in simulation and real-world settings show improved generalization and increased success rates in zero-shot manipulation tasks compared to prior approaches.

Conclusion: Iterative closed-loop multi-agent coordination with specialized agents and targeted error recovery enables more robust and generalizable robotic manipulation in dynamic environments without requiring model fine-tuning.

Abstract: Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present MALLVi, a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step.Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI.

</details>


### [74] [SparTa: Sparse Graphical Task Models from a Handful of Demonstrations](https://arxiv.org/abs/2602.16911)
*Adrian Röfer,Nick Heppert,Abhinav Valada*

Main category: cs.RO

TL;DR: A graph-based approach for learning long-horizon manipulation tasks from demonstrations by representing scene states as object relationships across task phases, with improved robustness through object matching and multi-demonstration learning.


<details>
  <summary>Details</summary>
Motivation: Learning long-horizon manipulation tasks efficiently from demonstrations is challenging. Existing methods often focus on learning actions directly, but the authors argue it's more effective to infer what the robot should achieve rather than how to achieve it, requiring better representation of evolving scene states and object interactions.

Method: Proposes a demonstration segmentation and pooling approach that extracts manipulation graphs representing object relationships across task phases. Uses graphical object relationships to represent evolving scene states, captures complete object interactions from control onset to manipulation completion, and employs object matching with pre-trained visual features for robustness when learning from multiple demonstrations.

Result: The method demonstrates accurate demonstration segmentation and shows utility in learning from multiple demonstrations to find minimal task models. The fitted models successfully deploy in both simulation and real robot environments, supporting reliable task execution across different settings.

Conclusion: The graph-based representation approach effectively captures long-horizon manipulation tasks by focusing on what to achieve rather than how, with robust multi-demonstration learning enabling reliable task execution across environments.

Abstract: Learning long-horizon manipulation tasks efficiently is a central challenge in robot learning from demonstration. Unlike recent endeavors that focus on directly learning the task in the action domain, we focus on inferring what the robot should achieve in the task, rather than how to do so. To this end, we represent evolving scene states using a series of graphical object relationships. We propose a demonstration segmentation and pooling approach that extracts a series of manipulation graphs and estimates distributions over object states across task phases. In contrast to prior graph-based methods that capture only partial interactions or short temporal windows, our approach captures complete object interactions spanning from the onset of control to the end of the manipulation. To improve robustness when learning from multiple demonstrations, we additionally perform object matching using pre-trained visual features. In extensive experiments, we evaluate our method's demonstration segmentation accuracy and the utility of learning from multiple demonstrations for finding a desired minimal task model. Finally, we deploy the fitted models both in simulation and on a real robot, demonstrating that the resulting task representations support reliable execution across environments.

</details>


### [75] [Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success](https://arxiv.org/abs/2602.17101)
*Varun Burde,Pavel Burget,Torsten Sattler*

Main category: cs.RO

TL;DR: The paper introduces a physics-based benchmark to evaluate 3D reconstruction quality and 6D pose estimation based on their functional impact on robotic grasping performance, revealing that reconstruction artifacts reduce grasp candidates but have minimal effect on success given accurate pose estimation.


<details>
  <summary>Details</summary>
Motivation: Standard geometric evaluations of 3D reconstruction methods don't reflect how reconstruction quality affects downstream robotic manipulation tasks like grasping. There's a gap between visual/geometric quality assessment and functional performance in real-world applications.

Method: Created a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models by generating grasps on reconstructed meshes and executing them on ground-truth models in simulation. This assesses combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction.

Result: Reconstruction artifacts significantly decrease the number of grasp pose candidates but have negligible effect on grasping performance when pose is accurately estimated. The relationship between grasp success and pose error is dominated by spatial error, and even simple translation error provides insight into grasping success for symmetric objects.

Conclusion: The work provides insights into how perception systems relate to object manipulation in robotics, establishing a functional evaluation framework that connects 3D reconstruction quality and pose estimation accuracy to actual grasping performance rather than just geometric metrics.

Abstract: 3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. We analyze the impact of model fidelity by generating grasps on various reconstructed 3D meshes and executing them on the ground-truth model, simulating how grasp poses generated with an imperfect model affect interaction with the real object. This assesses the combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction. Our results show that reconstruction artifacts significantly decrease the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. Our results also reveal that the relationship between grasp success and pose error is dominated by spatial error, and even a simple translation error provides insight into the success of the grasping pose of symmetric objects. This work provides insight into how perception systems relate to object manipulation using robots.

</details>


### [76] [Grasp Synthesis Matching From Rigid To Soft Robot Grippers Using Conditional Flow Matching](https://arxiv.org/abs/2602.17110)
*Tanisha Parulekar,Ge Shi,Josh Pinskier,David Howard,Jen Jen Chung*

Main category: cs.RO

TL;DR: A CFM-based framework maps rigid gripper grasp poses to soft Fin-ray gripper poses, achieving significantly higher success rates than baseline rigid poses for both seen and unseen objects.


<details>
  <summary>Details</summary>
Motivation: There's a representation gap between grasp synthesis for rigid and soft grippers. Existing methods like Anygrasp are designed for rigid parallel grippers and fail to capture the unique compliant behaviors of soft grippers, resulting in data-intensive and inaccurate models when adapted.

Method: Proposes a framework using Conditional Flow Matching (CFM) to map grasp poses from rigid gripper models to soft Fin-ray grippers. Includes a data collection pipeline for paired rigid-soft grasp poses. A U-Net autoencoder conditions the CFM model on object geometry from depth images, enabling continuous mapping from initial Anygrasp poses to stable Fin-ray gripper poses.

Result: CFM-generated poses achieve 34% success rate for seen objects and 46% for unseen objects, compared to baseline rigid poses (6% and 25% respectively). Significant improvements for cylindrical (50% and 100% success) and spherical objects (25% and 31% success). Successfully generalizes to unseen objects.

Conclusion: CFM is a data-efficient and effective method for transferring grasp strategies from rigid to soft grippers, offering a scalable methodology for other soft robotic systems.

Abstract: A representation gap exists between grasp synthesis for rigid and soft grippers. Anygrasp [1] and many other grasp synthesis methods are designed for rigid parallel grippers, and adapting them to soft grippers often fails to capture their unique compliant behaviors, resulting in data-intensive and inaccurate models. To bridge this gap, this paper proposes a novel framework to map grasp poses from a rigid gripper model to a soft Fin-ray gripper. We utilize Conditional Flow Matching (CFM), a generative model, to learn this complex transformation. Our methodology includes a data collection pipeline to generate paired rigid-soft grasp poses. A U-Net autoencoder conditions the CFM model on the object's geometry from a depth image, allowing it to learn a continuous mapping from an initial Anygrasp pose to a stable Fin-ray gripper pose. We validate our approach on a 7-DOF robot, demonstrating that our CFM-generated poses achieve a higher overall success rate for seen and unseen objects (34% and 46% respectively) compared to the baseline rigid poses (6% and 25% respectively) when executed by the soft gripper. The model shows significant improvements, particularly for cylindrical (50% and 100% success for seen and unseen objects) and spherical objects (25% and 31% success for seen and unseen objects), and successfully generalizes to unseen objects. This work presents CFM as a data-efficient and effective method for transferring grasp strategies, offering a scalable methodology for other soft robotic systems.

</details>


### [77] [Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy](https://arxiv.org/abs/2602.17128)
*Huishi Huang,Jack Klusmann,Haozhe Wang,Shuchen Ji,Fengkang Ying,Yiyuan Zhang,John Nassour,Gordon Cheng,Daniela Rus,Jun Liu,Marcelo H Ang,Cecilia Laschi*

Main category: cs.RO

TL;DR: AR-based teleoperation framework for hybrid rigid-soft robots using simulation-to-reality parameter identification for consistent virtual-physical behavior.


<details>
  <summary>Details</summary>
Motivation: Hybrid rigid-soft robots offer precision and compliance for unstructured environments, but coordination challenges exist due to modeling, perception, and cross-domain kinematics difficulties.

Method: AR-based physical human-robot interaction framework using AR headset to interact with simulated robot model in physics engine, superimposed on real system. Real-to-simulation parameter identification pipeline leverages soft robot's geometric properties for accurate static/dynamic behavior modeling.

Result: Framework enables direct teleoperation of hybrid rigid-soft robot for simple reaching and grasping tasks with consistent behavior between virtual and physical systems.

Conclusion: AR-based approach with parameter identification addresses coordination challenges in hybrid rigid-soft robots, enabling effective teleoperation for grasping tasks in unstructured environments.

Abstract: Hybrid rigid-soft robots combine the precision of rigid manipulators with the compliance and adaptability of soft arms, offering a promising approach for versatile grasping in unstructured environments. However, coordinating hybrid robots remains challenging, due to difficulties in modeling, perception, and cross-domain kinematics. In this work, we present a novel augmented reality (AR)-based physical human-robot interaction framework that enables direct teleoperation of a hybrid rigid-soft robot for simple reaching and grasping tasks. Using an AR headset, users can interact with a simulated model of the robotic system integrated into a general-purpose physics engine, which is superimposed on the real system, allowing simulated execution prior to real-world deployment. To ensure consistent behavior between the virtual and physical robots, we introduce a real-to-simulation parameter identification pipeline that leverages the inherent geometric properties of the soft robot, enabling accurate modeling of its static and dynamic behavior as well as the control system's response.

</details>


### [78] [Geometric Inverse Flight Dynamics on SO(3) and Application to Tethered Fixed-Wing Aircraft](https://arxiv.org/abs/2602.17166)
*Antonio Franchi,Chiara Gabellieri*

Main category: cs.RO

TL;DR: Coordinate-free inverse flight dynamics on SO(3) for fixed-wing aircraft, enabling closed-form trajectory-to-input mapping with aerodynamic moment recovery and application to tethered flight analysis.


<details>
  <summary>Details</summary>
Motivation: Bridge inverse simulation in aeronautics with geometric modeling in robotics by developing a coordinate-free formulation that avoids local attitude coordinates and provides rigorous building blocks for trajectory design and feasibility analysis.

Method: Coordinate-free formulation on SO(3) with translational force balance in world frame and rotational dynamics in body frame; geometric definition of aerodynamic directions; enforcement of coordinated flight (no sideslip) to derive closed-form trajectory-to-input map yielding attitude, angular velocity, thrust-angle-of-attack pair, and aerodynamic moment coefficients.

Result: Derived analytic expressions for required bank angle in tethered flight on spherical parallels, identified zero-bank locus where tether tension balances centrifugal effects, obtained closed-form minimal-thrust angle of attack under simple lift/drag law, and established pointwise quasi-steady inversion solutions that become steady-flight trim under time-invariant conditions.

Conclusion: The framework successfully bridges aeronautical inverse simulation with robotic geometric modeling, providing a rigorous mathematical foundation for trajectory design and feasibility verification in fixed-wing aircraft control.

Abstract: We present a robotics-oriented, coordinate-free formulation of inverse flight dynamics for fixed-wing aircraft on SO(3). Translational force balance is written in the world frame and rotational dynamics in the body frame; aerodynamic directions (drag, lift, side) are defined geometrically, avoiding local attitude coordinates. Enforcing coordinated flight (no sideslip), we derive a closed-form trajectory-to-input map yielding the attitude, angular velocity, and thrust-angle-of-attack pair, and we recover the aerodynamic moment coefficients component-wise. Applying such a map to tethered flight on spherical parallels, we obtain analytic expressions for the required bank angle and identify a specific zero-bank locus where the tether tension exactly balances centrifugal effects, highlighting the decoupling between aerodynamic coordination and the apparent gravity vector. Under a simple lift/drag law, the minimal-thrust angle of attack admits a closed form. These pointwise quasi-steady inversion solutions become steady-flight trim when the trajectory and rotational dynamics are time-invariant. The framework bridges inverse simulation in aeronautics with geometric modeling in robotics, providing a rigorous building block for trajectory design and feasibility checks.

</details>


### [79] [Nonlinear Predictive Control of the Continuum and Hybrid Dynamics of a Suspended Deformable Cable for Aerial Pick and Place](https://arxiv.org/abs/2602.17199)
*Antonio Rapuano,Yaolei Shen,Federico Califano,Chiara Gabellieri,Antonio Franchi*

Main category: cs.RO

TL;DR: A framework combining high-fidelity PDE modeling with reduced-order representation for real-time aerial manipulation of extensible cables, enabling dynamics-aware UAV control with suspended flexible cables.


<details>
  <summary>Details</summary>
Motivation: To enable real-time, dynamics-aware control of UAVs carrying suspended flexible cables, which requires handling complex cable dynamics while maintaining computational efficiency for practical implementation.

Method: Combines high-fidelity PDE modeling of cable dynamics with finite-difference discretization and proper orthogonal decomposition to extract a reduced-order model (ROM). Uses the ROM to formulate a nonlinear model predictive control scheme capable of stabilizing cable oscillations and handling hybrid transitions like payload attachment/detachment.

Result: Simulation results confirm the stability, efficiency, and robustness of the ROM, and demonstrate controller effectiveness in regulating cable dynamics under various operating conditions. Additional simulations show successful trajectory planning in constrained environments.

Conclusion: The framework successfully enables real-time, dynamics-aware control of UAVs carrying suspended flexible cables by balancing high-fidelity modeling with computational efficiency through reduced-order representation and nonlinear model predictive control.

Abstract: This paper presents a framework for aerial manipulation of an extensible cable that combines a high-fidelity model based on partial differential equations (PDEs) with a reduced-order representation suitable for real-time control. The PDEs are discretised using a finite-difference method, and proper orthogonal decomposition is employed to extract a reduced-order model (ROM) that retains the dominant deformation modes while significantly reducing computational complexity. Based on this ROM, a nonlinear model predictive control scheme is formulated, capable of stabilizing cable oscillations and handling hybrid transitions such as payload attachment and detachment. Simulation results confirm the stability, efficiency, and robustness of the ROM, as well as the effectiveness of the controller in regulating cable dynamics under a range of operating conditions. Additional simulations illustrate the application of the ROM for trajectory planning in constrained environments, demonstrating the versatility of the proposed approach. Overall, the framework enables real-time, dynamics-aware control of unmanned aerial vehicles (UAVs) carrying suspended flexible cables.

</details>


### [80] [Multi-session Localization and Mapping Exploiting Topological Information](https://arxiv.org/abs/2602.17226)
*Lorenzo Montano-Olivan,Julio A. Placed,Luis Montano,Maria T. Lazaro*

Main category: cs.RO

TL;DR: A multi-session framework for autonomous systems that uses map-based localization with topology-informed decision-making to selectively trigger mapping and loop closing in previously visited environments.


<details>
  <summary>Details</summary>
Motivation: Operating in previously visited environments is crucial for autonomous systems (driving, surveying, robotics), but repeated exposure to the same areas poses significant challenges for mapping and localization. Current approaches often greedily run full SLAM sessions and try to find correspondences between resulting maps, which is inefficient.

Method: A novel multi-session framework that builds on map-based localization rather than full SLAM sessions. It incorporates a topology-informed, uncertainty-aware decision-making mechanism that analyzes pose-graph structure to detect low-connectivity regions, selectively triggering mapping and loop closing modules. The resulting map and pose-graph are seamlessly integrated into the existing model.

Result: The method reduces accumulated error and enhances global consistency. It was validated on overlapping sequences from datasets and demonstrated effectiveness in a real-world mine-like environment.

Conclusion: The proposed framework provides an effective approach for multi-session operation in previously visited environments by intelligently managing mapping and localization resources through topology-aware decision making, improving efficiency and consistency compared to greedy full-SLAM approaches.

Abstract: Operating in previously visited environments is becoming increasingly crucial for autonomous systems, with direct applications in autonomous driving, surveying, and warehouse or household robotics. This repeated exposure to observing the same areas poses significant challenges for mapping and localization -- key components for enabling any higher-level task. In this work, we propose a novel multi-session framework that builds on map-based localization, in contrast to the common practice of greedily running full SLAM sessions and trying to find correspondences between the resulting maps. Our approach incorporates a topology-informed, uncertainty-aware decision-making mechanism that analyzes the pose-graph structure to detect low-connectivity regions, selectively triggering mapping and loop closing modules. The resulting map and pose-graph are seamlessly integrated into the existing model, reducing accumulated error and enhancing global consistency. We validate our method on overlapping sequences from datasets and demonstrate its effectiveness in a real-world mine-like environment.

</details>


### [81] [FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment](https://arxiv.org/abs/2602.17259)
*Han Zhao,Jingbo Wang,Wenxuan Song,Shuai Chen,Yang Liu,Yan Wang,Haoang Li,Donglin Wang*

Main category: cs.RO

TL;DR: FRAPPE introduces a two-stage fine-tuning method for VLA models that predicts future latent representations instead of pixel-level reconstructions, improving world modeling for robotics with better generalization and reduced error accumulation.


<details>
  <summary>Details</summary>
Motivation: Current world modeling approaches for VLA models in robotics have two main issues: 1) Training objectives force over-emphasis on pixel-level reconstruction, constraining semantic learning and generalization; 2) Reliance on predicted future observations during inference leads to error accumulation.

Method: FRAPPE uses a two-stage fine-tuning strategy: 1) Mid-training phase: model learns to predict latent representations of future observations; 2) Post-training phase: expands computational workload in parallel and aligns representations simultaneously with multiple different visual foundation models.

Result: FRAPPE outperforms state-of-the-art approaches on the RoboTwin benchmark and real-world tasks, showing strong generalization in long-horizon and unseen scenarios while improving fine-tuning efficiency and reducing dependence on action-annotated data.

Conclusion: FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies by addressing key limitations of current world modeling approaches through latent representation prediction and parallel progressive expansion.

Abstract: Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.

</details>


### [82] [Contact-Anchored Proprioceptive Odometry for Quadruped Robots](https://arxiv.org/abs/2602.17393)
*Minxing Sun,Yao Mao*

Main category: cs.RO

TL;DR: Proprioceptive state estimator using only IMU and motor measurements for legged robots, treating contacting legs as kinematic anchors with footfall constraints to suppress drift, achieving accurate pose estimation on various platforms.


<details>
  <summary>Details</summary>
Motivation: Reliable odometry for legged robots without cameras or LiDAR is challenging due to IMU drift and noisy joint velocity sensing. Existing methods often rely on external sensors, creating a need for purely proprioceptive solutions.

Method: Uses IMU and motor measurements to estimate body pose/velocity. Treats contacting legs as kinematic anchors: joint-torque-based foot wrench estimation selects reliable contacts, footfall positions provide world-frame constraints. Height clustering/time-decay correction prevents elevation drift. Inverse-kinematics cubature Kalman filter improves foot velocity observations. Multi-contact geometric consistency mitigates yaw drift.

Result: Evaluated on four quadruped platforms (three Astrall robots, one Unitree Go2 EDU). On Astrall robot A: ~200m horizontal loop error 0.1638m, ~15m vertical loop error 0.219m. On wheel-legged robot B: 0.2264m horizontal, 0.199m vertical. On robot C: ~700m horizontal error 7.68m, ~20m vertical error 0.540m. Unitree Go2 EDU: ~120m horizontal error 2.2138m, ~8m vertical error <0.1m.

Conclusion: The proposed proprioceptive state estimator effectively suppresses long-term drift using only IMU and motor measurements, achieving accurate pose estimation across various legged robot platforms without external sensors like cameras or LiDAR.

Abstract: Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a $\sim$200\,m horizontal loop and a $\sim$15\,m vertical loop return with 0.1638\,m and 0.219\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\,m and 0.199\,m. On wheel-legged robot~C, a $\sim$700\,m horizontal loop yields 7.68\,m error and a $\sim$20\,m vertical loop yields 0.540\,m error. Unitree Go2 EDU closes a $\sim$120\,m horizontal loop with 2.2138\,m error and a $\sim$8\,m vertical loop with less than 0.1\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git

</details>


### [83] [Distributed Virtual Model Control for Scalable Human-Robot Collaboration in Shared Workspace](https://arxiv.org/abs/2602.17415)
*Yi Zhang,Omar Faris,Chapa Sirithunge,Kai-Fung Chu,Fumiya Iida,Fulvio Forni*

Main category: cs.RO

TL;DR: Decentralized safety-aware control framework for human-robot collaboration using Virtual Model Control, eliminating deadlocks through force-based stall detection and negotiation.


<details>
  <summary>Details</summary>
Motivation: Need for safe, scalable human-robot collaboration systems that can handle dynamic multi-agent environments without explicit trajectory planning and prevent deadlocks.

Method: Virtual Model Control with humans and robots in shared virtual-component-shaped workspace using springs/dampers; decentralized force-based stall detection and negotiation for deadlock resolution; distributed implementation for scalability.

Result: Reduced deadlock probability from up to 61.2% to zero in block placement tasks; demonstrated safe collaboration with up to 2 robots/2 humans (real) and 4 robots (simulation); maintained ~20cm inter-agent separation; intuitive behavior shaping via parameter adjustment.

Conclusion: The framework provides effective, scalable, deadlock-free human-robot collaboration through decentralized virtual model control with safety awareness and negotiation-based deadlock resolution.

Abstract: We present a decentralized, agent agnostic, and safety-aware control framework for human-robot collaboration based on Virtual Model Control (VMC). In our approach, both humans and robots are embedded in the same virtual-component-shaped workspace, where motion is the result of the interaction with virtual springs and dampers rather than explicit trajectory planning. A decentralized, force-based stall detector identifies deadlocks, which are resolved through negotiation. This reduces the probability of robots getting stuck in the block placement task from up to 61.2% to zero in our experiments. The framework scales without structural changes thanks to the distributed implementation: in experiments we demonstrate safe collaboration with up to two robots and two humans, and in simulation up to four robots, maintaining inter-agent separation at around 20 cm. Results show that the method shapes robot behavior intuitively by adjusting control parameters and achieves deadlock-free operation across team sizes in all tested scenarios.

</details>


### [84] [3D-printed Soft Optical sensor with a Lens (SOLen) for light guidance in mechanosensing](https://arxiv.org/abs/2602.17421)
*Diana Cafiso,Petr Trunin,Carolina Gay,Lucia Beccai*

Main category: cs.RO

TL;DR: SOLen: A 3D printed soft optical sensing approach using printed lenses in Y-shaped waveguides for deformation sensing, with material modifications for improved compliance and optical properties.


<details>
  <summary>Details</summary>
Motivation: Additive manufacturing enables complex soft robot geometries but requires sensing solutions compatible with single-material, one-step fabrication. Optical soft sensors are promising but suffer from uncontrolled light propagation issues, and existing mitigation strategies typically require multimaterial interfaces.

Method: Developed SOLen approach with printed lens placed in front of an emitter within Y-shaped waveguide. Sensing relies on deformation-induced lens rotation and focal-spot translation redistributing optical power between branches. Modified acrylate polyurethane resin with lauryl acrylate for improved compliance/transmittance. Used single-layer optical characterization to derive wavelength-dependent refractive index/transmittance, minimizing DLP layer artifacts. Simulated lens profile using measured refractive index for target focal distance, printed with sub-millimeter fidelity.

Result: Successfully printed lens with sub-millimeter fidelity. Rotational tests demonstrated reproducible branch-selective signal switching over multiple cycles, showing the system can encode both motion direction and amplitude through differential output.

Conclusion: Established a transferable material-to-optics workflow for soft optical sensors with lenses, enabling new functionalities for next-generation soft robots through single-material, one-step fabrication compatible sensing.

Abstract: Additive manufacturing is enabling soft robots with increasingly complex geometries, creating a demand for sensing solutions that remain compatible with single-material, one-step fabrication. Optical soft sensors are attractive for monolithic printing, but their performance is often degraded by uncontrolled light propagation (ambient coupling, leakage, scattering), while common miti- gation strategies typically require multimaterial interfaces. Here, we present an approach for 3D printed soft optical sensing (SOLen), in which a printed lens is placed in front of an emitter within a Y-shaped waveguide. The sensing mechanism relies on deformation-induced lens rotation and focal-spot translation, redistributing optical power between the two branches to generate a differential output that encodes both motion direction and amplitude. An acrylate polyurethane resin was modified with lauryl acrylate to improve compliance and optical transmittance, and single-layer optical characterization was used to derive wavelength-dependent refractive index and transmittance while minimizing DLP layer-related artifacts. The measured refractive index was used in simulations to design a lens profile for a target focal distance, which was then printed with sub-millimeter fidelity. Rotational tests demonstrated reproducible branch-selective signal switching over multiple cycles. These results establish a transferable material-to-optics workflow for soft optical sensors with lens with new functionalities for next-generation soft robots

</details>


### [85] [A Cost-Effective and Climate-Resilient Air Pressure System for Rain Effect Reduction on Automated Vehicle Cameras](https://arxiv.org/abs/2602.17472)
*Mohamed Sabry,Joseba Gorospe,Cristina Olaverri-Monreal*

Main category: cs.RO

TL;DR: A cost-effective hardware solution for rainy conditions improves pedestrian detection accuracy from 8.3% to 41.6% for automated vehicles, supporting sustainability through compatibility with existing camera platforms.


<details>
  <summary>Details</summary>
Motivation: Limited research exists on physical hardware solutions for adverse weather perception in automated vehicles, despite their importance for critical applications like vehicle platooning. Existing approaches (hydrophilic/hydrophobic lenses, sprays) provide only partial mitigation, while industrial protection systems are costly and not scalable for automotive deployment.

Method: The paper presents a cost-effective hardware solution designed for rainy conditions that is compatible with multiple cameras simultaneously. The system works with existing camera-based sensing platforms without requiring additional high-cost sensors or hardware replacements.

Result: The proposed system increased pedestrian detection accuracy of a Deep Learning model from 8.3% to 41.6% in rainy conditions. It extends operational reliability of automated vehicles while reducing resource consumption and supporting modular upgrades.

Conclusion: The hardware solution enables more cost-efficient deployment of automated vehicle technologies in challenging weather conditions, supporting sustainability goals by reducing system failures that would otherwise lead to inefficiencies and increased emissions.

Abstract: Recent advances in automated vehicles have focused on improving perception performance under adverse weather conditions; however, research on physical hardware solutions remains limited, despite their importance for perception critical applications such as vehicle platooning. Existing approaches, such as hydrophilic or hydrophobic lenses and sprays, provide only partial mitigation, while industrial protection systems imply high cost and they do not enable scalability for automotive deployment.
  To address these limitations, this paper presents a cost-effective hardware solution for rainy conditions, designed to be compatible with multiple cameras simultaneously.
  Beyond its technical contribution, the proposed solution supports sustainability goals in transportation systems. By enabling compatibility with existing camera-based sensing platforms, the system extends the operational reliability of automated vehicles without requiring additional high-cost sensors or hardware replacements. This approach reduces resource consumption, supports modular upgrades, and promotes more cost-efficient deployment of automated vehicle technologies, particularly in challenging weather conditions where system failures would otherwise lead to inefficiencies and increased emissions. The proposed system was able to increase pedestrian detection accuracy of a Deep Learning model from 8.3% to 41.6%.

</details>


### [86] [Optically Sensorized Electro-Ribbon Actuator (OS-ERA)](https://arxiv.org/abs/2602.17474)
*Carolina Gay,Petr Trunin,Diana Cafiso,Yuejun Xu,Majid Taghavi,Lucia Beccai*

Main category: cs.RO

TL;DR: OS-ERA introduces optical waveguide sensors to Electro-Ribbon Actuators, enabling reliable proprioceptive sensing and accurate classification of bending states for improved control.


<details>
  <summary>Details</summary>
Motivation: Electro-Ribbon Actuators (ERAs) have ultrahigh displacement and fast movement but suffer from limited sensing precision with capacitive sensors, hindering accurate control. There's a need for reliable proprioceptive information without affecting actuation performance.

Method: Design and embed two soft optical waveguide sensors into ERA to analyze complex curvature in motion. Train a classifier to map sensing signals to distinguish eight bending states. Validate model on held-out trials and compare against training signal trajectories.

Result: Sensing output signals follow training manifold, predicted sequence mirrors real performance, and classification remains accurate despite train-test mismatches in actuation speed. OS-ERA demonstrates voltage- and speed-invariance, high fidelity bending state classification, fast response, and repeatability.

Conclusion: OS-ERA solves the longstanding sensing bottleneck of ERAs by providing reliable proprioceptive information through optical sensing, enabling steps toward closed-loop control while maintaining the actuator's high-performance characteristics.

Abstract: Electro-Ribbon Actuators (ERAs) are lightweight flexural actuators that exhibit ultrahigh displacement and fast movement. However, their embedded sensing relies on capacitive sensors with limited precision, which hinders accurate control. We introduce OS-ERA, an optically sensorized ERA that yields reliable proprioceptive information, and we focus on the design and integration of a sensing solution without affecting actuation. To analyse the complex curvature of an ERA in motion, we design and embed two soft optical waveguide sensors. A classifier is trained to map the sensing signals in order to distinguish eight bending states. We validate our model on six held-out trials and compare it against signals' trajectories learned from training runs. Across all tests, the sensing output signals follow the training manifold, and the predicted sequence mirrors real performance and confirms repeatability. Despite deliberate train-test mismatches in actuation speed, the signal trajectories preserve their shape, and classification remains consistently accurate, demonstrating practical voltage- and speed-invariance. As a result, OS-ERA classifies bending states with high fidelity; it is fast and repeatable, solving a longstanding bottleneck of the ERA, enabling steps toward closed-loop control.

</details>


### [87] [Proximal powered knee placement: a case study](https://arxiv.org/abs/2602.17502)
*Kyle R. Embry,Lorenzo Vianello,Jim Lipsey,Frank Ursetta,Michael Stephens,Zhi Wang,Ann M. Simon,Andrea J. Ikeda,Suzanne B. Finucane,Shawana Anarwala,Levi J. Hargrove*

Main category: cs.RO

TL;DR: Exploratory study finds above-knee placement of powered prosthetic knee powertrain improves walking speed and cadence compared to below-knee placement, suggesting careful mass distribution can preserve benefits of powered assistance while mitigating added weight effects.


<details>
  <summary>Details</summary>
Motivation: Powered prosthetic knees improve mobility but add mass that can negatively affect gait mechanics and metabolic cost. Optimizing mass distribution rather than just minimizing total mass may provide more effective solutions for above-knee amputees.

Method: Exploratory study evaluating feasibility of above-knee powertrain placement for powered prosthetic knee in small cohort. Compared above-knee vs below-knee configurations, measuring walking speed, cadence, gait symmetry, knee range of motion, and peak velocity. Additional testing on ramps and stairs to assess control strategy robustness across locomotion tasks.

Result: Above-knee configuration showed improved walking speed (+9.2% for one participant) and cadence (+3.6%) compared to below-knee placement, with mixed effects on gait symmetry. Kinematic measures showed similar knee range of motion and peak velocity across configurations. Control strategy performed robustly across multiple locomotion tasks including ramps and stairs.

Conclusion: Above-knee placement is functionally feasible for powered prosthetic knees. Careful mass distribution can preserve benefits of powered assistance while mitigating adverse effects of added weight. Further studies needed to confirm trends and guide design and clinical recommendations.

Abstract: Lower limb amputation affects millions worldwide, leading to impaired mobility, reduced walking speed, and limited participation in daily and social activities. Powered prosthetic knees can partially restore mobility by actively assisting knee joint torque, improving gait symmetry, sit-to-stand transitions, and walking speed. However, added mass from powered components may diminish these benefits, negatively affecting gait mechanics and increasing metabolic cost. Consequently, optimizing mass distribution, rather than simply minimizing total mass, may provide a more effective and practical solution. In this exploratory study, we evaluated the feasibility of above-knee powertrain placement for a powered prosthetic knee in a small cohort. Compared to below-knee placement, the above-knee configuration demonstrated improved walking speed (+9.2% for one participant) and cadence (+3.6%), with mixed effects on gait symmetry. Kinematic measures indicated similar knee range of motion and peak velocity across configurations. Additional testing on ramps and stairs confirmed the robustness of the control strategy across multiple locomotion tasks. These preliminary findings suggest that above-knee placement is functionally feasible and that careful mass distribution can preserve the benefits of powered assistance while mitigating adverse effects of added weight. Further studies are needed to confirm these trends and guide design and clinical recommendations.

</details>


### [88] [RA-Nav: A Risk-Aware Navigation System Based on Semantic Segmentation for Aerial Robots in Unpredictable Environments](https://arxiv.org/abs/2602.17515)
*Ziyi Zong,Xin Dong,Jinwu Xiang,Daochun Li,Zhan Tu*

Main category: cs.RO

TL;DR: RA-Nav is a risk-aware aerial robot navigation framework that uses semantic segmentation to classify obstacles and predict risks, enabling safe path planning when static obstacles suddenly move.


<details>
  <summary>Details</summary>
Motivation: Existing aerial navigation systems fail to adapt when static obstacles suddenly move, lacking environmental semantic awareness to estimate potential risks from such state transitions.

Method: Uses lightweight multi-scale semantic segmentation to identify obstacle categories in real time, classifies obstacles into stationary, temporarily static, and dynamic types, designs risk estimation functions for each type, constructs local risk maps, implements risk-informed path search algorithm, and applies trajectory optimization.

Result: RA-Nav achieves higher success rates than baselines in sudden obstacle state transition scenarios, with effectiveness validated in simulations using real-world data.

Conclusion: The proposed risk-aware navigation framework successfully addresses the challenge of suddenly moving obstacles by integrating semantic awareness and risk prediction, enabling safer and more adaptive aerial robot navigation.

Abstract: Existing aerial robot navigation systems typically plan paths around static and dynamic obstacles, but fail to adapt when a static obstacle suddenly moves. Integrating environmental semantic awareness enables estimation of potential risks posed by suddenly moving obstacles. In this paper, we propose RA- Nav, a risk-aware navigation framework based on semantic segmentation. A lightweight multi-scale semantic segmentation network identifies obstacle categories in real time. These obstacles are further classified into three types: stationary, temporarily static, and dynamic. For each type, corresponding risk estimation functions are designed to enable real-time risk prediction, based on which a complete local risk map is constructed. Based on this map, the risk-informed path search algorithm is designed to guarantee planning that balances path efficiency and safety. Trajectory optimization is then applied to generate trajectories that are safe, smooth, and dynamically feasible. Comparative simulations demonstrate that RA-Nav achieves higher success rates than baselines in sudden obstacle state transition scenarios. Its effectiveness is further validated in simulations using real- world data.

</details>


### [89] [IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control](https://arxiv.org/abs/2602.17537)
*Qilong Cheng,Matthew Mackay,Ali Bereyhi*

Main category: cs.RO

TL;DR: IRIS is a low-cost, 3D-printed 6-DOF robotic camera system that uses imitation learning to autonomously execute cinematic camera motions from human demonstrations.


<details>
  <summary>Details</summary>
Motivation: Industrial robotic camera systems are expensive and complex, limiting adoption. There's a need for affordable, accessible platforms that can perform dynamic cinematic motions without requiring explicit geometric programming.

Method: IRIS combines a lightweight, fully 3D-printed 6-DOF manipulator with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware, perceptually smooth camera trajectories directly from human demonstrations.

Result: The complete platform costs under $1,000 USD, supports 1.5 kg payload, achieves ~1 mm repeatability, demonstrates accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.

Conclusion: IRIS successfully addresses the cost and complexity barriers of robotic camera systems by combining affordable hardware with learning-based control, enabling accessible autonomous cinematic motion generation without explicit programming.

Abstract: Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.

</details>


### [90] [FR-GESTURE: An RGBD Dataset For Gesture-based Human-Robot Interaction In First Responder Operations](https://arxiv.org/abs/2602.17573)
*Konstantinos Foteinos,Georgios Angelidis,Aggelos Psiris,Vasileios Argyriou,Panagiotis Sarigiannidis,Georgios Th. Papadopoulos*

Main category: cs.RO

TL;DR: FR-GESTURE dataset for gesture-based UGV control by First Responders, featuring 12 commands, 3312 RGBD pairs from 2 viewpoints and 7 distances, with baseline experiments and public availability.


<details>
  <summary>Details</summary>
Motivation: Increasing disaster intensity and frequency make First Responders' work more difficult; AI and robotics solutions could facilitate operations by enabling gesture-based UGV control.

Method: Developed 12 gesture commands inspired by existing FR gestures and tactical hand signals, refined with FR feedback. Collected 3312 RGBD pairs from 2 viewpoints and 7 distances. Created evaluation protocols and performed baseline experiments.

Result: Created FR-GESTURE dataset - first dataset specifically for gesture-based UGV guidance by First Responders. Made data publicly available at https://doi.org/10.5281/zenodo.18131333 with baseline experiments for future improvement.

Conclusion: FR-GESTURE dataset addresses the need for gesture-based UGV control in disaster response, providing a foundation for future research in AI and robotics to support First Responders' operations.

Abstract: The ever increasing intensity and number of disasters make even more difficult the work of First Responders (FRs). Artificial intelligence and robotics solutions could facilitate their operations, compensating these difficulties. To this end, we propose a dataset for gesture-based UGV control by FRs, introducing a set of 12 commands, drawing inspiration from existing gestures used by FRs and tactical hand signals and refined after incorporating feedback from experienced FRs. Then we proceed with the data collection itself, resulting in 3312 RGBD pairs captured from 2 viewpoints and 7 distances. To the best of our knowledge, this is the first dataset especially intended for gesture-based UGV guidance by FRs. Finally we define evaluation protocols for our RGBD dataset, termed FR-GESTURE, and we perform baseline experiments, which are put forward for improvement. We have made data publicly available to promote future research on the domain: https://doi.org/10.5281/zenodo.18131333.

</details>


### [91] [Hybrid System Planning using a Mixed-Integer ADMM Heuristic and Hybrid Zonotopes](https://arxiv.org/abs/2602.17574)
*Joshua A. Robbins,Andrew F. Thompson,Jonah J. Glunt,Herschel C. Pangborn*

Main category: cs.RO

TL;DR: Hybrid zonotopes + ADMM heuristic for efficient embedded motion planning of hybrid systems, with applications to autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Embedded optimization-based planning for hybrid systems is computationally intensive due to mixed-integer programming, requiring more efficient methods for real-time applications like autonomous driving.

Method: Proposes a framework pairing hybrid zonotopes (advanced set representation) with a new ADMM mixed-integer programming heuristic. Presents general treatment of PWA system reachability analysis using hybrid zonotopes and extends to optimal planning problems.

Result: Sets produced have lower memory complexity and tighter convex relaxations than preexisting techniques. ADMM heuristic achieves improved convergence rates compared to state-of-the-art mixed-integer programming heuristics. Successfully applied to combined behavior and motion planning for autonomous driving on embedded hardware.

Conclusion: The hybrid zonotope framework with ADMM heuristic provides an efficient solution for embedded motion planning of hybrid systems, demonstrating practical viability for real-world applications like autonomous driving.

Abstract: Embedded optimization-based planning for hybrid systems is challenging due to the use of mixed-integer programming, which is computationally intensive and often sensitive to the specific numerical formulation. To address that challenge, this article proposes a framework for motion planning of hybrid systems that pairs hybrid zonotopes - an advanced set representation - with a new alternating direction method of multipliers (ADMM) mixed-integer programming heuristic. A general treatment of piecewise affine (PWA) system reachability analysis using hybrid zonotopes is presented and extended to formulate optimal planning problems. Sets produced using the proposed identities have lower memory complexity and tighter convex relaxations than equivalent sets produced from preexisting techniques. The proposed ADMM heuristic makes efficient use of the hybrid zonotope structure. For planning problems formulated as hybrid zonotopes, the proposed heuristic achieves improved convergence rates as compared to state-of-the-art mixed-integer programming heuristics. The proposed methods for hybrid system planning on embedded hardware are experimentally applied in a combined behavior and motion planning scenario for autonomous driving.

</details>


### [92] [Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space](https://arxiv.org/abs/2602.17586)
*Antonio Guillen-Perez*

Main category: cs.RO

TL;DR: Deep-Flow: Unsupervised safety-critical anomaly detection for autonomous vehicles using Optimal Transport Conditional Flow Matching on low-rank spectral manifolds to identify rare, high-risk scenarios overlooked by traditional rule-based methods.


<details>
  <summary>Details</summary>
Motivation: Safety validation for Level 4 autonomous vehicles is bottlenecked by inability to scale detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics, requiring data-driven approaches to identify safety-critical anomalies.

Method: Uses Optimal Transport Conditional Flow Matching to characterize continuous probability density of expert human driving behavior, constrained to low-rank spectral manifold via PCA bottleneck for kinematic smoothness. Incorporates Early Fusion Transformer encoder with lane-aware goal conditioning and direct skip-connection to flow head. Includes kinematic complexity weighting scheme prioritizing high-energy maneuvers during simulation-free training.

Result: Achieves AUC-ROC of 0.766 against heuristic golden set of safety-critical events on Waymo Open Motion Dataset. Reveals fundamental distinction between kinematic danger and semantic non-compliance, identifying predictability gap by surfacing out-of-distribution behaviors (lane-boundary violations, non-normative junction maneuvers) that traditional safety filters overlook.

Conclusion: Provides mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for safe deployment of autonomous fleets by addressing the critical challenge of detecting rare, high-risk scenarios in autonomous vehicle safety validation.

Abstract: Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets.

</details>


### [93] [Graph Neural Model Predictive Control for High-Dimensional Systems](https://arxiv.org/abs/2602.17601)
*Patrick Benito Eberhard,Luis Pabon,Daniele Gammelli,Hugo Buurmeijer,Amon Lahr,Mark Leone,Andrea Carron,Marco Pavone*

Main category: cs.RO

TL;DR: A framework combining GNN-based dynamics models with structure-exploiting MPC enables real-time control of high-dimensional systems like soft robots, achieving 100Hz closed-loop control for 1000-node systems with sub-centimeter tracking accuracy.


<details>
  <summary>Details</summary>
Motivation: High-dimensional systems like soft robots require models that capture complex dynamics while remaining computationally tractable for real-time control applications.

Method: Integrates Graph Neural Network (GNN)-based dynamics models with structure-exploiting Model Predictive Control (MPC). The system is represented as a graph with localized interactions, preserving sparsity. A tailored condensing algorithm eliminates state variables from the control problem, scaling linearly with system nodes and leveraging GPU parallelization.

Result: The method scales to systems with up to 1,000 nodes at 100 Hz in closed-loop control. Experimental validation on a physical soft robotic trunk demonstrates real-time reference tracking with sub-centimeter accuracy, outperforming baselines by 63.6%. Also shows capability for effective full-body obstacle avoidance.

Conclusion: The proposed framework successfully enables real-time control of high-dimensional systems by combining GNN-based modeling with efficient MPC implementation, achieving both computational efficiency and high control performance for complex systems like soft robots.

Abstract: The control of high-dimensional systems, such as soft robots, requires models that faithfully capture complex dynamics while remaining computationally tractable. This work presents a framework that integrates Graph Neural Network (GNN)-based dynamics models with structure-exploiting Model Predictive Control to enable real-time control of high-dimensional systems. By representing the system as a graph with localized interactions, the GNN preserves sparsity, while a tailored condensing algorithm eliminates state variables from the control problem, ensuring efficient computation. The complexity of our condensing algorithm scales linearly with the number of system nodes, and leverages Graphics Processing Unit (GPU) parallelization to achieve real-time performance. The proposed approach is validated in simulation and experimentally on a physical soft robotic trunk. Results show that our method scales to systems with up to 1,000 nodes at 100 Hz in closed-loop, and demonstrates real-time reference tracking on hardware with sub-centimeter accuracy, outperforming baselines by 63.6%. Finally, we show the capability of our method to achieve effective full-body obstacle avoidance.

</details>
