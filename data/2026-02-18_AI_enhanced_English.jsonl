{"id": "2602.15067", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15067", "abs": "https://arxiv.org/abs/2602.15067", "authors": ["Rut Pate", "Snehal Rajput", "Mehul S. Raval", "Rupal A. Kapdi", "Mohendra Roy"], "title": "Attention-gated U-Net model for semantic segmentation of brain tumors and feature extraction for survival prognosis", "comment": null, "summary": "Gliomas, among the most common primary brain tumors, vary widely in aggressiveness, prognosis, and histology, making treatment challenging due to complex and time-intensive surgical interventions. This study presents an Attention-Gated Recurrent Residual U-Net (R2U-Net) based Triplanar (2.5D) model for improved brain tumor segmentation. The proposed model enhances feature representation and segmentation accuracy by integrating residual, recurrent, and triplanar architectures while maintaining computational efficiency, potentially aiding in better treatment planning. The proposed method achieves a Dice Similarity Score (DSC) of 0.900 for Whole Tumor (WT) segmentation on the BraTS2021 validation set, demonstrating performance comparable to leading models. Additionally, the triplanar network extracts 64 features per planar model for survival days prediction, which are reduced to 28 using an Artificial Neural Network (ANN). This approach achieves an accuracy of 45.71%, a Mean Squared Error (MSE) of 108,318.128, and a Spearman Rank Correlation Coefficient (SRC) of 0.338 on the test dataset.", "AI": {"tldr": "Attention-Gated Recurrent Residual U-Net (R2U-Net) based Triplanar (2.5D) model for brain tumor segmentation achieves 0.900 DSC for Whole Tumor on BraTS2021, with triplanar features used for survival prediction via ANN achieving 45.71% accuracy.", "motivation": "Gliomas are common primary brain tumors with wide variability in aggressiveness, prognosis, and histology, making treatment challenging due to complex surgical interventions. There is a need for improved segmentation methods to aid in better treatment planning.", "method": "Proposes an Attention-Gated Recurrent Residual U-Net (R2U-Net) based Triplanar (2.5D) model that integrates residual, recurrent, and triplanar architectures to enhance feature representation and segmentation accuracy while maintaining computational efficiency. For survival prediction, the triplanar network extracts 64 features per planar model, which are reduced to 28 using an Artificial Neural Network (ANN).", "result": "Achieves Dice Similarity Score (DSC) of 0.900 for Whole Tumor (WT) segmentation on BraTS2021 validation set, demonstrating performance comparable to leading models. For survival prediction: accuracy of 45.71%, Mean Squared Error (MSE) of 108,318.128, and Spearman Rank Correlation Coefficient (SRC) of 0.338 on test dataset.", "conclusion": "The proposed Attention-Gated Recurrent Residual U-Net based Triplanar model shows promising results for brain tumor segmentation with computational efficiency, potentially aiding in better treatment planning. The triplanar feature extraction approach also demonstrates capability for survival prediction, though with moderate performance metrics."}}
{"id": "2602.15112", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15112", "abs": "https://arxiv.org/abs/2602.15112", "authors": ["Aniketh Garikaparthi", "Manasi Patwardhan", "Arman Cohan"], "title": "ResearchGym: Evaluating Language Model Agents on Real-World AI Research", "comment": null, "summary": "We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.", "AI": {"tldr": "ResearchGym is a benchmark and execution environment for evaluating AI agents on end-to-end research tasks using containerized environments from real ML papers, revealing a significant capability-reliability gap where agents occasionally achieve state-of-the-art performance but fail consistently.", "motivation": "To systematically evaluate AI agents' ability to conduct end-to-end research by creating realistic benchmarks from actual ML papers, addressing the need for controlled environments that test hypothesis generation, experimentation, and performance improvement beyond human baselines.", "method": "Repurposed five oral/spotlight papers from ICML, ICLR, and ACL by preserving datasets, evaluation harnesses, and baseline implementations while withholding the proposed methods. Created five containerized task environments with 39 sub-tasks total, requiring agents to propose hypotheses, run experiments, and surpass human baselines.", "result": "GPT-5 agent improved over provided baselines in only 1 of 15 evaluations (6.7%) by 11.5%, completing only 26.5% of sub-tasks on average. Identified recurring failure modes: impatience, poor resource management, overconfidence, difficulty coordinating parallel experiments, and context length limits. In one run, surpassed an ICML 2025 Spotlight task solution. Similar gaps observed with Claude Code (Opus-4.5) and Codex (GPT-5.2).", "conclusion": "ResearchGym provides infrastructure for systematic evaluation of autonomous research agents, revealing a sharp capability-reliability gap where frontier agents can occasionally reach state-of-the-art performance but do so unreliably, highlighting key challenges in long-horizon research tasks that need addressing for reliable autonomous research."}}
{"id": "2602.15143", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15143", "abs": "https://arxiv.org/abs/2602.15143", "authors": ["Xinhang Ma", "William Yeoh", "Ning Zhang", "Yevgeniy Vorobeychik"], "title": "Protecting Language Models Against Unauthorized Distillation through Trace Rewriting", "comment": null, "summary": "Knowledge distillation is a widely adopted technique for transferring capabilities from LLMs to smaller, more efficient student models. However, unauthorized use of knowledge distillation takes unfair advantage of the considerable effort and cost put into developing frontier models. We investigate methods for modifying teacher-generated reasoning traces to achieve two objectives that deter unauthorized distillation: (1) \\emph{anti-distillation}, or degrading the training usefulness of query responses, and (2) \\emph{API watermarking}, which embeds verifiable signatures in student models. We introduce several approaches for dynamically rewriting a teacher's reasoning outputs while preserving answer correctness and semantic coherence. Two of these leverage the rewriting capabilities of LLMs, while others use gradient-based techniques. Our experiments show that a simple instruction-based rewriting approach achieves a strong anti-distillation effect while maintaining or even improving teacher performance. Furthermore, we show that our rewriting approach also enables highly reliable watermark detection with essentially no false alarms.", "AI": {"tldr": "The paper introduces methods to modify LLM reasoning traces to deter unauthorized knowledge distillation through anti-distillation and API watermarking while preserving answer correctness.", "motivation": "Unauthorized knowledge distillation unfairly exploits the substantial investment in developing frontier LLMs, creating a need for techniques to protect model outputs while maintaining utility.", "method": "Dynamic rewriting of teacher reasoning outputs using LLM-based rewriting approaches and gradient-based techniques to achieve anti-distillation and embed verifiable watermarks.", "result": "Simple instruction-based rewriting achieves strong anti-distillation effects while maintaining or improving teacher performance, and enables highly reliable watermark detection with minimal false alarms.", "conclusion": "The proposed rewriting approaches effectively deter unauthorized distillation while preserving answer quality, offering practical protection mechanisms for LLM outputs."}}
{"id": "2602.15156", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15156", "abs": "https://arxiv.org/abs/2602.15156", "authors": ["Shreyas Rajesh", "Pavan Holur", "Mehmet Yigit Turali", "Chenda Duan", "Vwani Roychowdhury"], "title": "Panini: Continual Learning in Token Space via Structured Memory", "comment": "35 pages, code available at: https://github.com/roychowdhuryresearch/gsw-memory", "summary": "Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present Panini, which realizes this by representing documents as Generative Semantic Workspaces (GSW) -- an entity- and event-aware network of question-answer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given a query, Panini only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, Panini achieves the highest average performance, 5%-7% higher than other competitive baselines, while using 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time -- as achieved by the GSW framework -- yields both efficiency and reliability gains at read time. Code is available at https://github.com/roychowdhuryresearch/gsw-memory.", "code_url": "https://github.com/roychowdhuryresearch/gsw-memory", "code_stars": 4, "code_last_update": "2026-02-18", "AI": {"tldr": "Panini introduces a non-parametric continual learning framework using Generative Semantic Workspaces (GSW) - entity/event-aware QA networks that enable efficient reasoning over external knowledge without repeatedly processing verbatim documents.", "motivation": "Current RAG approaches inefficiently use test-time compute by repeatedly reasoning over the same documents, and chunk retrieval can inject irrelevant context leading to unsupported generation. There's a need for more efficient and reliable continual learning from new experiences.", "method": "Panini represents documents as Generative Semantic Workspaces (GSW) - networks of question-answer pairs capturing entities and events. The base model remains fixed while learning occurs by integrating experiences into an external semantic memory state that continually accumulates and consolidates. At inference, Panini traverses the GSW to retrieve the most likely inference chains rather than processing verbatim documents.", "result": "Across six QA benchmarks, Panini achieves 5%-7% higher average performance than competitive baselines, uses 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries.", "conclusion": "Efficient and accurate structuring of experiences at write time (via GSW framework) yields both efficiency and reliability gains at read time, demonstrating a human-like non-parametric continual learning approach that outperforms traditional RAG methods."}}
{"id": "2602.15060", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15060", "abs": "https://arxiv.org/abs/2602.15060", "authors": ["Tengjie Zhu", "Guanyu Cai", "Yang Zhaohui", "Guanzhu Ren", "Haohui Xie", "ZiRui Wang", "Junsong Wu", "Jingbo Wang", "Xiaokang Yang", "Yao Mu", "Yichao Yan", "Yichao Yan"], "title": "CLOT: Closed-Loop Global Motion Tracking for Whole-Body Humanoid Teleoperation", "comment": null, "summary": "Long-horizon whole-body humanoid teleoperation remains challenging due to accumulated global pose drift, particularly on full-sized humanoids. Although recent learning-based tracking methods enable agile and coordinated motions, they typically operate in the robot's local frame and neglect global pose feedback, leading to drift and instability during extended execution. In this work, we present CLOT, a real-time whole-body humanoid teleoperation system that achieves closed-loop global motion tracking via high-frequency localization feedback. CLOT synchronizes operator and robot poses in a closed loop, enabling drift-free human-to-humanoid mimicry over long timehorizons. However, directly imposing global tracking rewards in reinforcement learning, often results in aggressive and brittle corrections. To address this, we propose a data-driven randomization strategy that decouples observation trajectories from reward evaluation, enabling smooth and stable global corrections. We further regularize the policy with an adversarial motion prior to suppress unnatural behaviors. To support CLOT, we collect 20 hours of carefully curated human motion data for training the humanoid teleoperation policy. We design a transformer-based policy and train it for over 1300 GPU hours. The policy is deployed on a full-sized humanoid with 31 DoF (excluding hands). Both simulation and real-world experiments verify high-dynamic motion, high-precision tracking, and strong robustness in sim-to-real humanoid teleoperation. Motion data, demos and code can be found in our website.", "AI": {"tldr": "CLOT is a real-time whole-body humanoid teleoperation system that achieves closed-loop global motion tracking via high-frequency localization feedback, enabling drift-free human-to-humanoid mimicry over long time horizons.", "motivation": "Long-horizon whole-body humanoid teleoperation suffers from accumulated global pose drift, especially on full-sized humanoids. Existing learning-based tracking methods operate in local frames without global pose feedback, leading to drift and instability during extended execution.", "method": "CLOT synchronizes operator and robot poses in a closed loop. The approach uses a data-driven randomization strategy that decouples observation trajectories from reward evaluation for smooth global corrections, regularizes with an adversarial motion prior, employs a transformer-based policy trained on 20 hours of curated human motion data, and deploys on a 31-DoF full-sized humanoid.", "result": "The system achieves high-dynamic motion, high-precision tracking, and strong robustness in sim-to-real humanoid teleoperation. The policy was trained for over 1300 GPU hours and successfully deployed on a full-sized humanoid.", "conclusion": "CLOT demonstrates effective closed-loop global motion tracking for humanoid teleoperation, addressing the critical challenge of pose drift in long-horizon execution through innovative data-driven randomization and adversarial regularization techniques."}}
{"id": "2602.15158", "categories": ["cs.AI", "cs.IR", "math.LO"], "pdf": "https://arxiv.org/pdf/2602.15158", "abs": "https://arxiv.org/abs/2602.15158", "authors": ["Gabriel Rocha"], "title": "da Costa and Tarski meet Goguen and Carnap: a novel approach for ontological heterogeneity based on consequence systems", "comment": "22 pages, 5 figures, 1 table", "summary": "This paper presents a novel approach for ontological heterogeneity that draws heavily from Carnapian-Goguenism, as presented by Kutz, Mossakowski and L\u00fccke (2010). The approach is provisionally designated da Costian-Tarskianism, named after da Costa's Principle of Tolerance in Mathematics and after Alfred Tarski's work on the concept of a consequence operator. The approach is based on the machinery of consequence systems, as developed by Carnielli et al. (2008) and Citkin and Muravitsky (2022), and it introduces the idea of an extended consequence system, which is a consequence system extended with ontological axioms. The paper also defines the concept of an extended development graph, which is a graph structure that allows ontologies to be related via morphisms of extended consequence systems, and additionally via other operations such as fibring and splitting. Finally, we discuss the implications of this approach for the field of applied ontology and suggest directions for future research.", "AI": {"tldr": "A novel ontological heterogeneity approach called da Costian-Tarskianism, based on consequence systems extended with ontological axioms, using extended development graphs to relate ontologies via morphisms and operations like fibring and splitting.", "motivation": "To address ontological heterogeneity by developing a formal framework that can systematically relate different ontologies, building on Carnapian-Goguenism and leveraging consequence systems to handle diverse ontological commitments.", "method": "Introduces da Costian-Tarskianism based on consequence systems (Carnielli et al., 2008; Citkin & Muravitsky, 2022), extends them with ontological axioms to create extended consequence systems, and defines extended development graphs that relate ontologies via morphisms of these systems plus operations like fibring and splitting.", "result": "A formal framework for ontological heterogeneity that provides mathematical machinery to systematically relate ontologies through extended consequence systems and graph structures, offering operations for ontology integration and decomposition.", "conclusion": "The da Costian-Tarskian approach provides a robust formal foundation for handling ontological heterogeneity, with implications for applied ontology and promising directions for future research in ontology engineering and integration."}}
{"id": "2602.15061", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15061", "abs": "https://arxiv.org/abs/2602.15061", "authors": ["Zihan Zhang", "Haohui Que", "Junhan Chang", "Xin Zhang", "Hao Wei", "Tong Zhu"], "title": "Safe-SDL:Establishing Safety Boundaries and Control Mechanisms for AI-Driven Self-Driving Laboratories", "comment": null, "summary": "The emergence of Self-Driving Laboratories (SDLs) transforms scientific discovery methodology by integrating AI with robotic automation to create closed-loop experimental systems capable of autonomous hypothesis generation, experimentation, and analysis. While promising to compress research timelines from years to weeks, their deployment introduces unprecedented safety challenges differing from traditional laboratories or purely digital AI. This paper presents Safe-SDL, a comprehensive framework for establishing robust safety boundaries and control mechanisms in AI-driven autonomous laboratories. We identify and analyze the critical ``Syntax-to-Safety Gap'' -- the disconnect between AI-generated syntactically correct commands and their physical safety implications -- as the central challenge in SDL deployment. Our framework addresses this gap through three synergistic components: (1) formally defined Operational Design Domains (ODDs) that constrain system behavior within mathematically verified boundaries, (2) Control Barrier Functions (CBFs) that provide real-time safety guarantees through continuous state-space monitoring, and (3) a novel Transactional Safety Protocol (CRUTD) that ensures atomic consistency between digital planning and physical execution. We ground our theoretical contributions through analysis of existing implementations including UniLabOS and the Osprey architecture, demonstrating how these systems instantiate key safety principles. Evaluation against the LabSafety Bench reveals that current foundation models exhibit significant safety failures, demonstrating that architectural safety mechanisms are essential rather than optional. Our framework provides both theoretical foundations and practical implementation guidance for safe deployment of autonomous scientific systems, establishing the groundwork for responsible acceleration of AI-driven discovery.", "AI": {"tldr": "Safe-SDL framework addresses safety challenges in Self-Driving Labs by bridging the Syntax-to-Safety Gap through formal boundaries, real-time monitoring, and transactional protocols to ensure AI-driven autonomous laboratories operate safely.", "motivation": "Self-Driving Laboratories (SDLs) accelerate scientific discovery but introduce unprecedented safety challenges that differ from traditional labs or purely digital AI systems. The central problem is the \"Syntax-to-Safety Gap\" - the disconnect between AI-generated syntactically correct commands and their physical safety implications, requiring new safety frameworks beyond existing approaches.", "method": "The Safe-SDL framework employs three synergistic components: (1) formally defined Operational Design Domains (ODDs) that constrain system behavior within mathematically verified boundaries, (2) Control Barrier Functions (CBFs) providing real-time safety guarantees through continuous state-space monitoring, and (3) a novel Transactional Safety Protocol (CRUTD) ensuring atomic consistency between digital planning and physical execution. The approach is grounded through analysis of existing implementations like UniLabOS and Osprey architecture.", "result": "Evaluation using the LabSafety Bench reveals that current foundation models exhibit significant safety failures, demonstrating that architectural safety mechanisms are essential rather than optional. The framework shows how existing systems instantiate key safety principles and provides both theoretical foundations and practical implementation guidance.", "conclusion": "The Safe-SDL framework establishes comprehensive safety boundaries and control mechanisms for AI-driven autonomous laboratories, addressing the critical Syntax-to-Safety Gap. It provides the necessary theoretical foundations and practical guidance for safe deployment of autonomous scientific systems, enabling responsible acceleration of AI-driven discovery while ensuring robust safety guarantees."}}
{"id": "2602.15173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15173", "abs": "https://arxiv.org/abs/2602.15173", "authors": ["Luise Ge", "Yongyan Zhang", "Yevgeniy Vorobeychik"], "title": "Mind the (DH) Gap! A Contrast in Risky Choices Between Reasoning and Conversational LLMs", "comment": null, "summary": "The use of large language models either as decision support systems, or in agentic workflows, is rapidly transforming the digital ecosystem. However, the understanding of LLM decision-making under uncertainty remains limited. We initiate a comparative study of LLM risky choices along two dimensions: (1) prospect representation (explicit vs. experience based) and (2) decision rationale (explanation). Our study, which involves 20 frontier and open LLMs, is complemented by a matched human subjects experiment, which provides one reference point, while an expected payoff maximizing rational agent model provides another. We find that LLMs cluster into two categories: reasoning models (RMs) and conversational models (CMs). RMs tend towards rational behavior, are insensitive to the order of prospects, gain/loss framing, and explanations, and behave similarly whether prospects are explicit or presented via experience history. CMs are significantly less rational, slightly more human-like, sensitive to prospect ordering, framing, and explanation, and exhibit a large description-history gap. Paired comparisons of open LLMs suggest that a key factor differentiating RMs and CMs is training for mathematical reasoning.", "AI": {"tldr": "LLMs cluster into reasoning models (rational, insensitive to framing/ordering) and conversational models (less rational, more human-like, sensitive to framing/ordering), with mathematical reasoning training being a key differentiator.", "motivation": "Despite rapid adoption of LLMs in decision support and agentic workflows, understanding of LLM decision-making under uncertainty remains limited, necessitating comparative study of risky choices across different prospect representations and decision rationales.", "method": "Comparative study of 20 frontier and open LLMs along two dimensions: prospect representation (explicit vs. experience-based) and decision rationale (explanation), complemented by matched human subjects experiment and expected payoff maximizing rational agent model as reference points.", "result": "LLMs cluster into two categories: reasoning models (RMs) that behave rationally and are insensitive to framing/ordering effects, and conversational models (CMs) that are less rational, more human-like, and sensitive to prospect ordering, framing, and explanation. Mathematical reasoning training appears to be a key differentiating factor.", "conclusion": "LLM decision-making under uncertainty reveals systematic differences between reasoning-oriented and conversation-oriented models, with training objectives (particularly mathematical reasoning) playing a crucial role in determining rational behavior and sensitivity to cognitive biases."}}
{"id": "2602.15212", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15212", "abs": "https://arxiv.org/abs/2602.15212", "authors": ["Yuanyan Song", "Kezhi Wang", "Xinmian Xu"], "title": "Secure and Energy-Efficient Wireless Agentic AI Networks", "comment": "Submitted to journal", "summary": "In this paper, we introduce a secure wireless agentic AI network comprising one supervisor AI agent and multiple other AI agents to provision quality of service (QoS) for users' reasoning tasks while ensuring confidentiality of private knowledge and reasoning outcomes. Specifically, the supervisor AI agent can dynamically assign other AI agents to participate in cooperative reasoning, while the unselected AI agents act as friendly jammers to degrade the eavesdropper's interception performance. To extend the service duration of AI agents, an energy minimization problem is formulated that jointly optimizes AI agent selection, base station (BS) beamforming, and AI agent transmission power, subject to latency and reasoning accuracy constraints. To address the formulated problem, we propose two resource allocation schemes, ASC and LAW, which first decompose it into three sub-problems. Specifically, ASC optimizes each sub-problem iteratively using the proposed alternating direction method of multipliers (ADMM)-based algorithm, semi-definite relaxation (SDR), and successive convex approximation (SCA), while LAW tackles each sub-problem using the proposed large language model (LLM) optimizer within an agentic workflow. The experimental results show that the proposed solutions can reduce network energy consumption by up to 59.1% compared to other benchmark schemes. Furthermore, the proposed schemes are validated using a practical agentic AI system based on Qwen, demonstrating satisfactory reasoning accuracy across various public benchmarks.", "AI": {"tldr": "Secure wireless agentic AI network with supervisor agent coordinating multiple AI agents for QoS provisioning while protecting privacy through friendly jamming and energy-efficient resource allocation.", "motivation": "To provision quality of service for users' reasoning tasks while ensuring confidentiality of private knowledge and reasoning outcomes in wireless AI networks, addressing energy efficiency and security challenges.", "method": "Two resource allocation schemes: ASC (uses ADMM-based algorithm, SDR, and SCA for iterative optimization) and LAW (uses LLM optimizer within agentic workflow), both decomposing the joint optimization problem of AI agent selection, BS beamforming, and transmission power.", "result": "Proposed solutions reduce network energy consumption by up to 59.1% compared to benchmark schemes, with satisfactory reasoning accuracy validated on Qwen-based practical agentic AI system across various public benchmarks.", "conclusion": "The secure wireless agentic AI network framework effectively balances QoS provisioning, privacy protection, and energy efficiency through intelligent agent coordination and optimized resource allocation strategies."}}
{"id": "2602.15092", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15092", "abs": "https://arxiv.org/abs/2602.15092", "authors": ["Xuanyun Qiu", "Dorian Verdel", "Hector Cervantes-Culebro", "Alexis Devillard", "Etienne Burdet"], "title": "Augmenting Human Balance with Generic Supernumerary Robotic Limbs", "comment": null, "summary": "Supernumerary robotic limbs (SLs) have the potential to transform a wide range of human activities, yet their usability remains limited by key technical challenges, particularly in ensuring safety and achieving versatile control. Here, we address the critical problem of maintaining balance in the human-SLs system, a prerequisite for safe and comfortable augmentation tasks. Unlike previous approaches that developed SLs specifically for stability support, we propose a general framework for preserving balance with SLs designed for generic use. Our hierarchical three-layer architecture consists of: (i) a prediction layer that estimates human trunk and center of mass (CoM) dynamics, (ii) a planning layer that generates optimal CoM trajectories to counteract trunk movements and computes the corresponding SL control inputs, and (iii) a control layer that executes these inputs on the SL hardware. We evaluated the framework with ten participants performing forward and lateral bending tasks. The results show a clear reduction in stance instability, demonstrating the framework's effectiveness in enhancing balance. This work paves the path towards safe and versatile human-SLs interactions. [This paper has been submitted for publication to IEEE.]", "AI": {"tldr": "A hierarchical framework for maintaining balance in human-supernumerary robotic limb systems, enabling safe augmentation without requiring specialized stability-focused SL designs.", "motivation": "Supernumerary robotic limbs (SLs) have transformative potential but face usability limitations due to safety concerns and control challenges, particularly in maintaining system balance during augmentation tasks.", "method": "A three-layer hierarchical architecture: (1) prediction layer estimates human trunk and center of mass dynamics; (2) planning layer generates optimal CoM trajectories to counteract trunk movements and computes SL control inputs; (3) control layer executes inputs on SL hardware.", "result": "Evaluation with ten participants performing forward and lateral bending tasks showed clear reduction in stance instability, demonstrating the framework's effectiveness in enhancing balance.", "conclusion": "The proposed general framework enables balance preservation with generic SLs, paving the way for safe and versatile human-SL interactions beyond specialized stability-focused designs."}}
{"id": "2602.15248", "categories": ["cs.AI", "math.OC", "q-fin.MF"], "pdf": "https://arxiv.org/pdf/2602.15248", "abs": "https://arxiv.org/abs/2602.15248", "authors": ["Pavel Koptev", "Vishnu Kumar", "Konstantin Malkov", "George Shapiro", "Yury Vikhanov"], "title": "Predicting Invoice Dilution in Supply Chain Finance with Leakage Free Two Stage XGBoost, KAN (Kolmogorov Arnold Networks), and Ensemble Models", "comment": null, "summary": "Invoice or payment dilution is the gap between the approved invoice amount and the actual collection is a significant source of non credit risk and margin loss in supply chain finance. Traditionally, this risk is managed through the buyer's irrevocable payment undertaking (IPU), which commits to full payment without deductions. However, IPUs can hinder supply chain finance adoption, particularly among sub-invested grade buyers. A newer, data-driven methods use real-time dynamic credit limits, projecting dilution for each buyer-supplier pair in real-time. This paper introduces an AI, machine learning framework and evaluates how that can supplement a deterministic algorithm to predict invoice dilution using extensive production dataset across nine key transaction fields.", "AI": {"tldr": "AI/ML framework supplements deterministic algorithm to predict invoice dilution in supply chain finance, addressing non-credit risk from payment gaps.", "motivation": "Invoice/payment dilution (gap between approved invoice amount and actual collection) is a significant source of non-credit risk and margin loss in supply chain finance. Traditional IPU-based risk management hinders adoption, especially among sub-investment grade buyers.", "method": "Introduces an AI/machine learning framework that supplements deterministic algorithms to predict invoice dilution. Uses extensive production dataset across nine key transaction fields. Employs real-time dynamic credit limits and projects dilution for each buyer-supplier pair in real-time.", "result": "Not specified in abstract (requires full paper for evaluation results).", "conclusion": "Data-driven methods using AI/ML can supplement traditional deterministic approaches to better predict and manage invoice dilution risk, potentially improving supply chain finance adoption."}}
{"id": "2602.15162", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15162", "abs": "https://arxiv.org/abs/2602.15162", "authors": ["Fernando Ca\u00f1adas-Ar\u00e1nega", "Francisco J. Ma\u00f1as-\u00c1lvarez", "Jos\u00e9 L- Guzm\u00e1n", "Jos\u00e9 C. Moreno", "Jos\u00e9 L. Blanco-Claraco"], "title": "A ROS2 Benchmarking Framework for Hierarchical Control Strategies in Mobile Robots for Mediterranean Greenhouses", "comment": "53 pages", "summary": "Mobile robots operating in agroindustrial environments, such as Mediterranean greenhouses, are subject to challenging conditions, including uneven terrain, variable friction, payload changes, and terrain slopes, all of which significantly affect control performance and stability. Despite the increasing adoption of robotic platforms in agriculture, the lack of standardized, reproducible benchmarks impedes fair comparisons and systematic evaluations of control strategies under realistic operating conditions. This paper presents a comprehensive benchmarking framework for evaluating mobile robot controllers in greenhouse environments. The proposed framework integrates an accurate three dimensional model of the environment, a physics based simulator, and a hierarchical control architecture comprising low, mid, and high level control layers. Three benchmark categories are defined to enable modular assessment, ranging from actuator level control to full autonomous navigation. Additionally, three disturbance scenarios payload variation, terrain type, and slope are explicitly modeled to replicate real world agricultural conditions. To ensure objective and reproducible evaluation, standardized performance metrics are introduced, including the Squared Absolute Error (SAE), the Squared Control Input (SCI), and composite performance indices. Statistical analysis based on repeated trials is employed to mitigate the influence of sensor noise and environmental variability. The framework is further enhanced by a plugin based architecture that facilitates seamless integration of user defined controllers and planners. The proposed benchmark provides a robust and extensible tool for the quantitative comparison of classical, predictive, and planning based control strategies in realistic conditions, bridging the gap between simulation based analysis and real world agroindustrial applications.", "AI": {"tldr": "A benchmarking framework for evaluating mobile robot controllers in greenhouse environments with standardized metrics, disturbance modeling, and modular assessment categories.", "motivation": "Mobile robots in agroindustrial environments face challenging conditions (uneven terrain, variable friction, payload changes, slopes) that affect control performance. The lack of standardized, reproducible benchmarks impedes fair comparisons and systematic evaluation of control strategies under realistic operating conditions.", "method": "Proposes a comprehensive benchmarking framework integrating: 1) accurate 3D environment model, 2) physics-based simulator, 3) hierarchical control architecture (low, mid, high level layers). Defines three benchmark categories for modular assessment (actuator level to full autonomous navigation). Models three disturbance scenarios (payload variation, terrain type, slope). Uses standardized metrics (Squared Absolute Error, Squared Control Input, composite indices) and statistical analysis with repeated trials. Features plugin-based architecture for user-defined controllers.", "result": "The framework provides a robust, extensible tool for quantitative comparison of classical, predictive, and planning-based control strategies in realistic conditions. It enables reproducible evaluation while mitigating sensor noise and environmental variability through statistical analysis.", "conclusion": "The proposed benchmark bridges the gap between simulation-based analysis and real-world agroindustrial applications, facilitating systematic evaluation and fair comparison of mobile robot controllers in challenging greenhouse environments."}}
{"id": "2602.15270", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15270", "abs": "https://arxiv.org/abs/2602.15270", "authors": ["Farbod Abbasi", "Zachary Patterson", "Bilal Farooq"], "title": "Enhancing Diversity and Feasibility: Joint Population Synthesis from Multi-source Data Using Generative Models", "comment": "12 pages, 8 figures, 5 tables", "summary": "Generating realistic synthetic populations is essential for agent-based models (ABM) in transportation and urban planning. Current methods face two major limitations. First, many rely on a single dataset or follow a sequential data fusion and generation process, which means they fail to capture the complex interplay between features. Second, these approaches struggle with sampling zeros (valid but unobserved attribute combinations) and structural zeros (infeasible combinations due to logical constraints), which reduce the diversity and feasibility of the generated data. This study proposes a novel method to simultaneously integrate and synthesize multi-source datasets using a Wasserstein Generative Adversarial Network (WGAN) with gradient penalty. This joint learning method improves both the diversity and feasibility of synthetic data by defining a regularization term (inverse gradient penalty) for the generator loss function. For the evaluation, we implement a unified evaluation metric for similarity, and place special emphasis on measuring diversity and feasibility through recall, precision, and the F1 score. Results show that the proposed joint approach outperforms the sequential baseline, with recall increasing by 7\\% and precision by 15\\%. Additionally, the regularization term further improves diversity and feasibility, reflected in a 10\\% increase in recall and 1\\% in precision. We assess similarity distributions using a five-metric score. The joint approach performs better overall, and reaches a score of 88.1 compared to 84.6 for the sequential method. Since synthetic populations serve as a key input for ABM, this multi-source generative approach has the potential to significantly enhance the accuracy and reliability of ABM.", "AI": {"tldr": "A novel joint learning method using WGAN with gradient penalty to generate synthetic populations by simultaneously integrating multi-source datasets, addressing limitations of sequential approaches and improving diversity/feasibility through regularization.", "motivation": "Current methods for generating synthetic populations for ABM have two major limitations: 1) reliance on single datasets or sequential data fusion fails to capture complex feature interplay, and 2) struggles with sampling zeros (unobserved but valid combinations) and structural zeros (infeasible combinations), reducing diversity and feasibility.", "method": "Proposes a joint learning approach using Wasserstein Generative Adversarial Network (WGAN) with gradient penalty to simultaneously integrate and synthesize multi-source datasets. Introduces a regularization term (inverse gradient penalty) for the generator loss function to improve diversity and feasibility.", "result": "The joint approach outperforms sequential baseline with recall increasing by 7% and precision by 15%. Regularization term further improves diversity/feasibility with 10% increase in recall and 1% in precision. Similarity evaluation using five-metric score shows joint approach reaches 88.1 vs 84.6 for sequential method.", "conclusion": "The multi-source generative approach using joint learning with WGAN and regularization significantly enhances synthetic population generation, which is crucial for improving accuracy and reliability of agent-based models in transportation and urban planning."}}
{"id": "2602.15201", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15201", "abs": "https://arxiv.org/abs/2602.15201", "authors": ["Ren\u00e9 Zurbr\u00fcgg", "Andrei Cramariuc", "Marco Hutter"], "title": "DexEvolve: Evolutionary Optimization for Robust and Diverse Dexterous Grasp Synthesis", "comment": null, "summary": "Dexterous grasping is fundamental to robotics, yet data-driven grasp prediction heavily relies on large, diverse datasets that are costly to generate and typically limited to a narrow set of gripper morphologies. Analytical grasp synthesis can be used to scale data collection, but necessary simplifying assumptions often yield physically infeasible grasps that need to be filtered in high-fidelity simulators, significantly reducing the total number of grasps and their diversity.\n  We propose a scalable generate-and-refine pipeline for synthesizing large-scale, diverse, and physically feasible grasps. Instead of using high-fidelity simulators solely for verification and filtering, we leverage them as an optimization stage that continuously improves grasp quality without discarding precomputed candidates. More specifically, we initialize an evolutionary search with a seed set of analytically generated, potentially suboptimal grasps. We then refine these proposals directly in a high-fidelity simulator (Isaac Sim) using an asynchronous, gradient-free evolutionary algorithm, improving stability while maintaining diversity. In addition, this refinement stage can be guided toward human preferences and/or domain-specific quality metrics without requiring a differentiable objective. We further distill the refined grasp distribution into a diffusion model for robust real-world deployment, and highlight the role of diversity for both effective training and during deployment. Experiments on a newly introduced Handles dataset and a DexGraspNet subset demonstrate that our approach achieves over 120 distinct stable grasps per object (a 1.7-6x improvement over unrefined analytical methods) while outperforming diffusion-based alternatives by 46-60\\% in unique grasp coverage.", "AI": {"tldr": "A generate-and-refine pipeline for scalable synthesis of diverse, physically feasible robotic grasps using evolutionary refinement in high-fidelity simulation and distillation into diffusion models.", "motivation": "Data-driven grasp prediction requires large, diverse datasets that are costly to generate and limited to specific gripper morphologies. Analytical grasp synthesis can scale data collection but often produces physically infeasible grasps that need filtering, reducing diversity and quantity.", "method": "Proposes a scalable generate-and-refine pipeline: 1) Initialize with analytically generated seed grasps, 2) Refine in high-fidelity simulator (Isaac Sim) using asynchronous gradient-free evolutionary algorithm to improve stability while maintaining diversity, 3) Distill refined grasp distribution into diffusion model for real-world deployment. The refinement can incorporate human preferences and domain-specific metrics without requiring differentiable objectives.", "result": "Achieves over 120 distinct stable grasps per object (1.7-6x improvement over unrefined analytical methods) and outperforms diffusion-based alternatives by 46-60% in unique grasp coverage on Handles dataset and DexGraspNet subset.", "conclusion": "The approach enables scalable synthesis of large-scale, diverse, physically feasible grasps by leveraging high-fidelity simulation as an optimization stage rather than just a verification filter, with evolutionary refinement maintaining diversity and distillation enabling robust real-world deployment."}}
{"id": "2602.15274", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15274", "abs": "https://arxiv.org/abs/2602.15274", "authors": ["Omid Madani", "J. Brian Burns", "Reza Eghbali", "Thomas L. Dean"], "title": "When Remembering and Planning are Worth it: Navigating under Change", "comment": null, "summary": "We explore how different types and uses of memory can aid spatial navigation in changing uncertain environments. In the simple foraging task we study, every day, our agent has to find its way from its home, through barriers, to food. Moreover, the world is non-stationary: from day to day, the location of the barriers and food may change, and the agent's sensing such as its location information is uncertain and very limited. Any model construction, such as a map, and use, such as planning, needs to be robust against these challenges, and if any learning is to be useful, it needs to be adequately fast. We look at a range of strategies, from simple to sophisticated, with various uses of memory and learning. We find that an architecture that can incorporate multiple strategies is required to handle (sub)tasks of a different nature, in particular for exploration and search, when food location is not known, and for planning a good path to a remembered (likely) food location. An agent that utilizes non-stationary probability learning techniques to keep updating its (episodic) memories and that uses those memories to build maps and plan on the fly (imperfect maps, i.e. noisy and limited to the agent's experience) can be increasingly and substantially more efficient than the simpler (minimal-memory) agents, as the task difficulties such as distance to goal are raised, as long as the uncertainty, from localization and change, is not too large.", "AI": {"tldr": "The paper explores memory-based strategies for spatial navigation in changing, uncertain environments, finding that agents using episodic memory and probability learning outperform simpler approaches as task difficulty increases.", "motivation": "To understand how different memory types and learning strategies can enable effective spatial navigation in non-stationary environments with uncertainty, limited sensing, and changing obstacles/goals.", "method": "Study agents in a foraging task with daily changing barrier/food locations and uncertain sensing. Compare strategies from simple to sophisticated using various memory and learning approaches, including non-stationary probability learning for episodic memory updates and on-the-fly map building/planning.", "result": "Agents using episodic memory with probability learning and imperfect map building substantially outperform minimal-memory agents as task difficulty (e.g., distance to goal) increases, provided uncertainty from localization and environmental changes remains manageable.", "conclusion": "Effective navigation in changing uncertain environments requires architectures supporting multiple strategies for different subtasks (exploration vs. planning), with episodic memory updated via probability learning and imperfect map-based planning providing significant efficiency gains over simpler approaches."}}
{"id": "2602.15258", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15258", "abs": "https://arxiv.org/abs/2602.15258", "authors": ["Sebastian Donnelly", "Ruth Anderson", "George Economides", "James Broughton", "Peter Ball", "Alexander Rast", "Andrew Bradley"], "title": "SEG-JPEG: Simple Visual Semantic Communications for Remote Operation of Automated Vehicles over Unreliable Wireless Networks", "comment": null, "summary": "Remote Operation is touted as being key to the rapid deployment of automated vehicles. Streaming imagery to control connected vehicles remotely currently requires a reliable, high throughput network connection, which can be limited in real-world remote operation deployments relying on public network infrastructure. This paper investigates how the application of computer vision assisted semantic communication can be used to circumvent data loss and corruption associated with traditional image compression techniques. By encoding the segmentations of detected road users into colour coded highlights within low resolution greyscale imagery, the required data rate can be reduced by 50 \\% compared with conventional techniques, while maintaining visual clarity. This enables a median glass-to-glass latency of below 200ms even when the network data rate is below 500kbit/s, while clearly outlining salient road users to enhance situational awareness of the remote operator. The approach is demonstrated in an area of variable 4G mobile connectivity using an automated last-mile delivery vehicle. With this technique, the results indicate that large-scale deployment of remotely operated automated vehicles could be possible even on the often constrained public 4G/5G mobile network, providing the potential to expedite the nationwide roll-out of automated vehicles.", "AI": {"tldr": "Semantic communication using computer vision reduces data requirements by 50% for remote vehicle operation, enabling sub-200ms latency on constrained 4G/5G networks.", "motivation": "Remote operation of automated vehicles requires reliable high-throughput networks, which are often limited in real-world deployments using public mobile infrastructure. Traditional image compression suffers from data loss and corruption issues.", "method": "Computer vision assisted semantic communication encodes road user segmentations as color-coded highlights within low-resolution greyscale imagery, reducing data requirements while maintaining visual clarity for remote operators.", "result": "Achieves 50% data rate reduction compared to conventional techniques, median glass-to-glass latency below 200ms even with network data rates below 500kbit/s, and clear outlining of salient road users to enhance situational awareness.", "conclusion": "The approach enables large-scale deployment of remotely operated automated vehicles on constrained public 4G/5G mobile networks, potentially expediting nationwide roll-out of automated vehicles."}}
{"id": "2602.15294", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15294", "abs": "https://arxiv.org/abs/2602.15294", "authors": ["Ming Du", "Yanqi Luo", "Srutarshi Banerjee", "Michael Wojcik", "Jelena Popovic", "Mathew J. Cherukara"], "title": "EAA: Automating materials characterization with vision language model agents", "comment": null, "summary": "We present Experiment Automation Agents (EAA), a vision-language-model-driven agentic system designed to automate complex experimental microscopy workflows. EAA integrates multimodal reasoning, tool-augmented action, and optional long-term memory to support both autonomous procedures and interactive user-guided measurements. Built on a flexible task-manager architecture, the system enables workflows ranging from fully agent-driven automation to logic-defined routines that embed localized LLM queries. EAA further provides a modern tool ecosystem with two-way compatibility for Model Context Protocol (MCP), allowing instrument-control tools to be consumed or served across applications. We demonstrate EAA at an imaging beamline at the Advanced Photon Source, including automated zone plate focusing, natural language-described feature search, and interactive data acquisition. These results illustrate how vision-capable agents can enhance beamline efficiency, reduce operational burden, and lower the expertise barrier for users.", "AI": {"tldr": "EAA is a vision-language-model-driven agentic system that automates complex experimental microscopy workflows through multimodal reasoning, tool-augmented actions, and flexible task management, demonstrated at an imaging beamline with applications like automated focusing and natural language feature search.", "motivation": "To address the complexity and expertise requirements of experimental microscopy workflows by creating an automated system that can reduce operational burden, enhance beamline efficiency, and lower the expertise barrier for users through intelligent automation.", "method": "Built on a flexible task-manager architecture integrating multimodal reasoning, tool-augmented action, and optional long-term memory. The system supports both autonomous procedures and interactive user-guided measurements, with a modern tool ecosystem featuring two-way compatibility for Model Context Protocol (MCP) for instrument-control tools.", "result": "Successfully demonstrated at an imaging beamline at the Advanced Photon Source, including automated zone plate focusing, natural language-described feature search, and interactive data acquisition, showing enhanced beamline efficiency and reduced operational burden.", "conclusion": "Vision-capable agents like EAA can significantly enhance beamline efficiency, reduce operational burden, and lower expertise barriers for users in experimental microscopy workflows, representing an important advancement in scientific automation."}}
{"id": "2602.15309", "categories": ["cs.RO", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2602.15309", "abs": "https://arxiv.org/abs/2602.15309", "authors": ["Mostafa A. Atalla", "Anand S. Sekar", "Remi van Starkenburg", "David J. Jager", "Aim\u00e9e Sakes", "Micha\u00ebl Wiertlewski", "Paul Breedveld"], "title": "OSCAR: An Ovipositor-Inspired Self-Propelling Capsule Robot for Colonoscopy", "comment": null, "summary": "Self-propelling robotic capsules eliminate shaft looping of conventional colonoscopy, reducing patient discomfort. However, reliably moving within the slippery, viscoelastic environment of the colon remains a significant challenge. We present OSCAR, an ovipositor-inspired self-propelling capsule robot that translates the transport strategy of parasitic wasps into a propulsion mechanism for colonoscopy. OSCAR mechanically encodes the ovipositor-inspired motion pattern through a spring-loaded cam system that drives twelve circumferential sliders in a coordinated, phase-shifted sequence. By tuning the motion profile to maximize the retract phase relative to the advance phase, the capsule creates a controlled friction anisotropy at the interface that generates net forward thrust. We developed an analytical model incorporating a Kelvin-Voigt formulation to capture the viscoelastic stick--slip interactions between the sliders and the tissue, linking the asymmetry between advance and retract phase durations to mean thrust, and slider-reversal synchronization to thrust stability. Comprehensive force characterization experiments in ex-vivo porcine colon revealed a mean steady-state traction force of 0.85 N, closely matching the model. Furthermore, experiments confirmed that thrust generation is speed-independent and scales linearly with the phase asymmetry, in agreement with theoretical predictions, underscoring the capsule's predictable performance and scalability. In locomotion validation experiments, OSCAR demonstrated robust performance, achieving an average speed of 3.08 mm/s, a velocity sufficient to match the cecal intubation times of conventional colonoscopy. By coupling phase-encoded friction anisotropy with a predictive model, OSCAR delivers controllable thrust generation at low normal loads, enabling safer and more robust self-propelling locomotion for robotic capsule colonoscopy.", "AI": {"tldr": "OSCAR is an ovipositor-inspired self-propelling capsule robot for colonoscopy that uses phase-shifted slider motion to create friction anisotropy for net forward thrust, achieving 0.85 N traction force and 3.08 mm/s speed.", "motivation": "Self-propelling robotic capsules eliminate shaft looping issues of conventional colonoscopy but face challenges moving reliably in the slippery, viscoelastic colon environment. There's a need for a propulsion mechanism that can generate controllable thrust at low normal loads for safer robotic capsule colonoscopy.", "method": "Developed OSCAR, an ovipositor-inspired capsule robot that translates parasitic wasp transport strategy into propulsion. Uses a spring-loaded cam system to drive twelve circumferential sliders in coordinated, phase-shifted sequence. Created analytical model with Kelvin-Voigt formulation to capture viscoelastic stick-slip interactions between sliders and tissue. Tuned motion profile to maximize retract phase relative to advance phase to create controlled friction anisotropy.", "result": "Mean steady-state traction force of 0.85 N measured in ex-vivo porcine colon, closely matching model predictions. Thrust generation is speed-independent and scales linearly with phase asymmetry. Achieved average speed of 3.08 mm/s, sufficient to match cecal intubation times of conventional colonoscopy. Demonstrated robust performance in locomotion validation experiments.", "conclusion": "OSCAR successfully couples phase-encoded friction anisotropy with predictive modeling to deliver controllable thrust generation at low normal loads, enabling safer and more robust self-propelling locomotion for robotic capsule colonoscopy. The bio-inspired approach provides predictable performance and scalability for clinical applications."}}
{"id": "2602.15351", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15351", "abs": "https://arxiv.org/abs/2602.15351", "authors": ["Kei Takahashi", "Hikaru Sasaki", "Takamitsu Matsubara"], "title": "Feasibility-aware Imitation Learning from Observation with Multimodal Feedback", "comment": null, "summary": "Imitation learning frameworks that learn robot control policies from demonstrators' motions via hand-mounted demonstration interfaces have attracted increasing attention. However, due to differences in physical characteristics between demonstrators and robots, this approach faces two limitations: i) the demonstration data do not include robot actions, and ii) the demonstrated motions may be infeasible for robots. These limitations make policy learning difficult. To address them, we propose Feasibility-Aware Behavior Cloning from Observation (FABCO). FABCO integrates behavior cloning from observation, which complements robot actions using robot dynamics models, with feasibility estimation. In feasibility estimation, the demonstrated motions are evaluated using a robot-dynamics model, learned from the robot's execution data, to assess reproducibility under the robot's dynamics. The estimated feasibility is used for multimodal feedback and feasibility-aware policy learning to improve the demonstrator's motions and learn robust policies. Multimodal feedback provides feasibility through the demonstrator's visual and haptic senses to promote feasible demonstrated motions. Feasibility-aware policy learning reduces the influence of demonstrated motions that are infeasible for robots, enabling the learning of policies that robots can execute stably. We conducted experiments with 15 participants on two tasks and confirmed that FABCO improves imitation learning performance by more than 3.2 times compared to the case without feasibility feedback.", "AI": {"tldr": "FABCO improves imitation learning by integrating feasibility estimation with behavior cloning from observation, using robot dynamics to assess motion reproducibility and providing multimodal feedback to demonstrators.", "motivation": "Imitation learning from human demonstrations faces limitations due to physical differences between humans and robots: demonstration data lacks robot actions, and demonstrated motions may be infeasible for robots, making policy learning difficult.", "method": "FABCO combines behavior cloning from observation (which uses robot dynamics models to complement missing robot actions) with feasibility estimation. Feasibility is assessed using a robot-dynamics model learned from execution data. The approach includes multimodal feedback (visual/haptic) to guide demonstrators toward feasible motions, and feasibility-aware policy learning that reduces influence of infeasible demonstrations.", "result": "Experiments with 15 participants on two tasks showed that FABCO improves imitation learning performance by more than 3.2 times compared to the case without feasibility feedback.", "conclusion": "FABCO effectively addresses the limitations of imitation learning from human demonstrations by integrating feasibility estimation with behavior cloning, enabling learning of robust policies that robots can execute stably through improved demonstration quality and feasibility-aware policy learning."}}
{"id": "2602.15325", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15325", "abs": "https://arxiv.org/abs/2602.15325", "authors": ["Zhixing Zhang", "Jesen Zhang", "Hao Liu", "Qinhan Lv", "Jing Yang", "Kaitong Cai", "Keze Wang"], "title": "AgriWorld:A World Tools Protocol Framework for Verifiable Agricultural Reasoning with Code-Executing LLM Agents", "comment": null, "summary": "Foundation models for agriculture are increasingly trained on massive spatiotemporal data (e.g., multi-spectral remote sensing, soil grids, and field-level management logs) and achieve strong performance on forecasting and monitoring. However, these models lack language-based reasoning and interactive capabilities, limiting their usefulness in real-world agronomic workflows. Meanwhile, large language models (LLMs) excel at interpreting and generating text, but cannot directly reason over high-dimensional, heterogeneous agricultural datasets. We bridge this gap with an agentic framework for agricultural science. It provides a Python execution environment, AgriWorld, exposing unified tools for geospatial queries over field parcels, remote-sensing time-series analytics, crop growth simulation, and task-specific predictors (e.g., yield, stress, and disease risk). On top of this environment, we design a multi-turn LLM agent, Agro-Reflective, that iteratively writes code, observes execution results, and refines its analysis via an execute-observe-refine loop. We introduce AgroBench, with scalable data generation for diverse agricultural QA spanning lookups, forecasting, anomaly detection, and counterfactual \"what-if\" analysis. Experiments outperform text-only and direct tool-use baselines, validating execution-driven reflection for reliable agricultural reasoning.", "AI": {"tldr": "An agentic framework combining LLMs with agricultural data tools enables code-driven reasoning over spatiotemporal agricultural datasets through iterative execution and refinement.", "motivation": "Existing agricultural foundation models lack language-based reasoning and interactive capabilities, while LLMs cannot directly reason over high-dimensional, heterogeneous agricultural datasets, creating a gap in practical agronomic workflows.", "method": "Developed AgriWorld Python environment with unified tools for geospatial queries, remote-sensing analytics, crop growth simulation, and task-specific predictors, plus Agro-Reflective LLM agent that iteratively writes code, observes execution results, and refines analysis through execute-observe-refine loop.", "result": "Experiments on AgroBench (scalable agricultural QA dataset) show the framework outperforms text-only and direct tool-use baselines, validating execution-driven reflection for reliable agricultural reasoning.", "conclusion": "The agentic framework successfully bridges the gap between agricultural data processing and language-based reasoning, enabling practical agricultural science applications through code-driven interactive analysis."}}
{"id": "2602.15354", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.15354", "abs": "https://arxiv.org/abs/2602.15354", "authors": ["Jose Luis Peralta-Cabezas", "Miguel Torres-Torriti", "Marcelo Guarini-Hermann"], "title": "A Comparison of Bayesian Prediction Techniques for Mobile Robot Trajectory Tracking", "comment": "Accepted in Robotica (Dec. 2007), vol. 26, n. 5, pp. 571-585 (c) 2008 Cambridge University Press. https://doi.org/10.1017/S0263574708004153", "summary": "This paper presents a performance comparison of different estimation and prediction techniques applied to the problem of tracking multiple robots. The main performance criteria are the magnitude of the estimation or prediction error, the computational effort and the robustness of each method to non-Gaussian noise. Among the different techniques compared are the well known Kalman filters and their different variants (e.g. extended and unscented), and the more recent techniques relying on Sequential Monte Carlo Sampling methods, such as particle filters and Gaussian Mixture Sigma Point Particle Filter.", "AI": {"tldr": "Performance comparison of estimation/prediction techniques for multi-robot tracking, evaluating error magnitude, computational effort, and robustness to non-Gaussian noise across Kalman filter variants and Sequential Monte Carlo methods.", "motivation": "To systematically evaluate and compare different estimation and prediction techniques for tracking multiple robots, with focus on practical performance metrics including accuracy, computational efficiency, and robustness to challenging noise conditions.", "method": "Comparative analysis of Kalman filter variants (standard, extended, unscented) and Sequential Monte Carlo methods (particle filters, Gaussian Mixture Sigma Point Particle Filter) applied to multi-robot tracking problem.", "result": "The paper presents quantitative performance comparisons across three key metrics: estimation/prediction error magnitude, computational effort requirements, and robustness to non-Gaussian noise conditions.", "conclusion": "Provides empirical evidence for selecting appropriate filtering techniques for multi-robot tracking based on specific application requirements regarding accuracy, computational constraints, and noise characteristics."}}
{"id": "2602.15384", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15384", "abs": "https://arxiv.org/abs/2602.15384", "authors": ["Zhouzhou Shen", "Xueyu Hu", "Xiyun Li", "Tianqing Fang", "Juncheng Li", "Shengyu Zhang"], "title": "World-Model-Augmented Web Agents with Action Correction", "comment": null, "summary": "Web agents based on large language models have demonstrated promising capability in automating web tasks. However, current web agents struggle to reason out sensible actions due to the limitations of predicting environment changes, and might not possess comprehensive awareness of execution risks, prematurely performing risky actions that cause losses and lead to task failure. To address these challenges, we propose WAC, a web agent that integrates model collaboration, consequence simulation, and feedback-driven action refinement. To overcome the cognitive isolation of individual models, we introduce a multi-agent collaboration process that enables an action model to consult a world model as a web-environment expert for strategic guidance; the action model then grounds these suggestions into executable actions, leveraging prior knowledge of environmental state transition dynamics to enhance candidate action proposal. To achieve risk-aware resilient task execution, we introduce a two-stage deduction chain. A world model, specialized in environmental state transitions, simulates action outcomes, which a judge model then scrutinizes to trigger action corrective feedback when necessary. Experiments show that WAC achieves absolute gains of 1.8% on VisualWebArena and 1.3% on Online-Mind2Web.", "AI": {"tldr": "WAC is a web agent framework that improves task execution through multi-agent collaboration, consequence simulation, and feedback-driven action refinement to address reasoning limitations and risk awareness in current web agents.", "motivation": "Current web agents based on LLMs struggle with reasoning about environment changes and lack comprehensive risk awareness, often performing risky actions prematurely that lead to task failures and losses.", "method": "WAC integrates model collaboration, consequence simulation, and feedback-driven action refinement. It uses a multi-agent collaboration process where an action model consults a world model as a web-environment expert for strategic guidance, then grounds suggestions into executable actions. A two-stage deduction chain involves a world model simulating action outcomes and a judge model scrutinizing them to trigger corrective feedback when needed.", "result": "WAC achieves absolute gains of 1.8% on VisualWebArena and 1.3% on Online-Mind2Web benchmarks, demonstrating improved performance over existing web agents.", "conclusion": "The proposed WAC framework effectively addresses reasoning limitations and risk awareness in web agents through collaborative modeling and feedback-driven refinement, leading to more robust and successful task execution."}}
{"id": "2602.15357", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15357", "abs": "https://arxiv.org/abs/2602.15357", "authors": ["Xinhao Chen", "Hongkun Yao", "Anuruddha Bhattacharjee", "Suraj Raval", "Lamar O. Mair", "Yancy Diaz-Mercado", "Axel Krieger"], "title": "Fluoroscopy-Constrained Magnetic Robot Control via Zernike-Based Field Modeling and Nonlinear MPC", "comment": null, "summary": "Magnetic actuation enables surgical robots to navigate complex anatomical pathways while reducing tissue trauma and improving surgical precision. However, clinical deployment is limited by the challenges of controlling such systems under fluoroscopic imaging, which provides low frame rate and noisy pose feedback. This paper presents a control framework that remains accurate and stable under such conditions by combining a nonlinear model predictive control (NMPC) framework that directly outputs coil currents, an analytically differentiable magnetic field model based on Zernike polynomials, and a Kalman filter to estimate the robot state. Experimental validation is conducted with two magnetic robots in a 3D-printed fluid workspace and a spine phantom replicating drug delivery in the epidural space. Results show the proposed control method remains highly accurate when feedback is downsampled to 3 Hz with added Gaussian noise (sigma = 2 mm), mimicking clinical fluoroscopy. In the spine phantom experiments, the proposed method successfully executed a drug delivery trajectory with a root mean square (RMS) position error of 1.18 mm while maintaining safe clearance from critical anatomical boundaries.", "AI": {"tldr": "NMPC-based control framework for magnetic surgical robots maintains accuracy under low-frame-rate, noisy fluoroscopic imaging using differentiable magnetic field model and Kalman filtering.", "motivation": "Clinical deployment of magnetic surgical robots is limited by challenges in controlling them under fluoroscopic imaging, which provides low frame rate and noisy pose feedback, necessitating robust control methods.", "method": "Combines nonlinear model predictive control (NMPC) that directly outputs coil currents, an analytically differentiable magnetic field model based on Zernike polynomials, and a Kalman filter for robot state estimation.", "result": "The control method remains highly accurate with feedback downsampled to 3 Hz with added Gaussian noise (sigma = 2 mm). In spine phantom experiments, achieved RMS position error of 1.18 mm while maintaining safe clearance from anatomical boundaries.", "conclusion": "The proposed control framework enables accurate and stable operation of magnetic surgical robots under clinical fluoroscopic imaging conditions, facilitating their clinical deployment for minimally invasive procedures."}}
{"id": "2602.15391", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15391", "abs": "https://arxiv.org/abs/2602.15391", "authors": ["Ankit Sharma", "Nachiket Tapas", "Jyotiprakash Patra"], "title": "Improving LLM Reliability through Hybrid Abstention and Adaptive Detection", "comment": null, "summary": "Large Language Models (LLMs) deployed in production environments face a fundamental safety-utility trade-off either a strict filtering mechanisms prevent harmful outputs but often block benign queries or a relaxed controls risk unsafe content generation. Conventional guardrails based on static rules or fixed confidence thresholds are typically context-insensitive and computationally expensive, resulting in high latency and degraded user experience. To address these limitations, we introduce an adaptive abstention system that dynamically adjusts safety thresholds based on real-time contextual signals such as domain and user history. The proposed framework integrates a multi-dimensional detection architecture composed of five parallel detectors, combined through a hierarchical cascade mechanism to optimize both speed and precision. The cascade design reduces unnecessary computation by progressively filtering queries, achieving substantial latency improvements compared to non-cascaded models and external guardrail systems. Extensive evaluation on mixed and domain-specific workloads demonstrates significant reductions in false positives, particularly in sensitive domains such as medical advice and creative writing. The system maintains high safety precision and near-perfect recall under strict operating modes. Overall, our context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment.", "AI": {"tldr": "An adaptive abstention system for LLMs that dynamically adjusts safety thresholds using real-time contextual signals, combining five parallel detectors in a hierarchical cascade to optimize safety-utility trade-off while reducing latency and false positives.", "motivation": "Current LLM deployment faces a fundamental safety-utility trade-off: strict filtering blocks benign queries while relaxed controls risk unsafe content. Conventional guardrails are context-insensitive, computationally expensive, and cause high latency, degrading user experience.", "method": "Introduces an adaptive abstention system with dynamic safety thresholds based on real-time contextual signals (domain, user history). Uses a multi-dimensional detection architecture with five parallel detectors combined through a hierarchical cascade mechanism to optimize speed and precision. The cascade progressively filters queries to reduce unnecessary computation.", "result": "Achieves substantial latency improvements compared to non-cascaded models and external guardrail systems. Shows significant reductions in false positives, especially in sensitive domains like medical advice and creative writing. Maintains high safety precision and near-perfect recall under strict operating modes.", "conclusion": "The context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment that addresses the limitations of conventional static guardrails."}}
{"id": "2602.15397", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15397", "abs": "https://arxiv.org/abs/2602.15397", "authors": ["Zibin Dong", "Yicheng Liu", "Shiduo Zhang", "Baijun Ye", "Yifu Yuan", "Fei Ni", "Jingjing Gong", "Xipeng Qiu", "Hang Zhao", "Yinchuan Li", "Jianye Hao"], "title": "ActionCodec: What Makes for Good Action Tokenizers", "comment": null, "summary": "Vision-Language-Action (VLA) models leveraging the native autoregressive paradigm of Vision-Language Models (VLMs) have demonstrated superior instruction-following and training efficiency. Central to this paradigm is action tokenization, yet its design has primarily focused on reconstruction fidelity, failing to address its direct impact on VLA optimization. Consequently, the fundamental question of \\textit{what makes for good action tokenizers} remains unanswered. In this paper, we bridge this gap by establishing design principles specifically from the perspective of VLA optimization. We identify a set of best practices based on information-theoretic insights, including maximized temporal token overlap, minimized vocabulary redundancy, enhanced multimodal mutual information, and token independence. Guided by these principles, we introduce \\textbf{ActionCodec}, a high-performance action tokenizer that significantly enhances both training efficiency and VLA performance across diverse simulation and real-world benchmarks. Notably, on LIBERO, a SmolVLM2-2.2B fine-tuned with ActionCodec achieves a 95.5\\% success rate without any robotics pre-training. With advanced architectural enhancements, this reaches 97.4\\%, representing a new SOTA for VLA models without robotics pre-training. We believe our established design principles, alongside the released model, will provide a clear roadmap for the community to develop more effective action tokenizers.", "AI": {"tldr": "ActionCodec introduces information-theoretic design principles for action tokenizers in Vision-Language-Action models, optimizing for VLA performance rather than just reconstruction fidelity, achieving state-of-the-art results without robotics pre-training.", "motivation": "Existing action tokenization in VLA models focuses primarily on reconstruction fidelity but fails to address the direct impact on VLA optimization. The fundamental question of what makes good action tokenizers remains unanswered, creating a gap in understanding how tokenizer design affects overall VLA performance.", "method": "Establishes design principles from VLA optimization perspective based on information-theoretic insights: maximized temporal token overlap, minimized vocabulary redundancy, enhanced multimodal mutual information, and token independence. Introduces ActionCodec, a high-performance action tokenizer implementing these principles.", "result": "ActionCodec significantly enhances training efficiency and VLA performance across diverse simulation and real-world benchmarks. On LIBERO, SmolVLM2-2.2B fine-tuned with ActionCodec achieves 95.5% success rate without robotics pre-training, reaching 97.4% with architectural enhancements - new SOTA for VLA models without robotics pre-training.", "conclusion": "The established design principles and released ActionCodec model provide a clear roadmap for developing more effective action tokenizers, bridging the gap between tokenizer design and VLA optimization performance."}}
{"id": "2602.15403", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15403", "abs": "https://arxiv.org/abs/2602.15403", "authors": ["Thomas \u00c5gotnes"], "title": "Common Belief Revisited", "comment": null, "summary": "Contrary to common belief, common belief is not KD4.\n  If individual belief is KD45, common belief does indeed lose the 5 property and keep the D and 4 properties -- and it has none of the other commonly considered properties of knowledge and belief. But it has another property: $C(C\u03c6\\rightarrow \u03c6)$ -- corresponding to so-called shift-reflexivity (reflexivity one step ahead). This observation begs the question:\n  is KD4 extended with this axiom a complete characterisation of common belief in the KD45 case? If not, what \\emph{is} the logic of common belief? In this paper we show that the answer to the first question is ``no'': there is one additional axiom, and, furthermore, it relies on the number of agents. We show that the result is a complete characterisation of common belief, settling the open problem.", "AI": {"tldr": "The paper shows that KD4 extended with shift-reflexivity is incomplete for characterizing common belief in KD45 systems, identifies an additional axiom dependent on agent count, and provides a complete characterization.", "motivation": "To resolve the open problem of characterizing the logic of common belief when individual belief is KD45, specifically whether KD4 plus shift-reflexivity ($C(C\u03c6\u2192\u03c6)$) is complete.", "method": "Logical analysis and axiomatic characterization of common belief operators in multi-agent epistemic/doxastic logic, examining properties preserved under common belief construction from individual KD45 belief.", "result": "KD4 plus shift-reflexivity is incomplete; one additional axiom is required that depends on the number of agents, yielding a complete characterization of common belief.", "conclusion": "The paper settles the open problem by providing a complete axiomatization of common belief in KD45 systems, showing it requires both shift-reflexivity and an agent-count-dependent axiom beyond KD4."}}
{"id": "2602.15398", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15398", "abs": "https://arxiv.org/abs/2602.15398", "authors": ["Abdelrahman Metwally", "Monijesu James", "Aleksey Fedoseev", "Miguel Altamirano Cabrera", "Dzmitry Tsetserukou", "Andrey Somov"], "title": "Hybrid F' and ROS2 Architecture for Vision-Based Autonomous Flight: Design and Experimental Validation", "comment": "Paper accepted to ICIT 2026", "summary": "Autonomous aerospace systems require architectures that balance deterministic real-time control with advanced perception capabilities. This paper presents an integrated system combining NASA's F' flight software framework with ROS2 middleware via Protocol Buffers bridging. We evaluate the architecture through a 32.25-minute indoor quadrotor flight test using vision-based navigation. The vision system achieved 87.19 Hz position estimation with 99.90\\% data continuity and 11.47 ms mean latency, validating real-time performance requirements. All 15 ground commands executed successfully with 100 % success rate, demonstrating robust F'--PX4 integration. System resource utilization remained low (15.19 % CPU, 1,244 MB RAM) with zero stale telemetry messages, confirming efficient operation on embedded platforms. Results validate the feasibility of hybrid flight-software architectures combining certification-grade determinism with flexible autonomy for autonomous aerial vehicles.", "AI": {"tldr": "Integrated NASA F' flight software with ROS2 via Protocol Buffers for autonomous quadrotor systems, achieving real-time vision-based navigation with high reliability and low resource usage.", "motivation": "Autonomous aerospace systems need architectures that combine deterministic real-time control (for certification) with advanced perception capabilities (for autonomy), requiring integration of flight software frameworks with modern robotics middleware.", "method": "Developed an integrated system combining NASA's F' flight software framework with ROS2 middleware using Protocol Buffers bridging, then evaluated through a 32.25-minute indoor quadrotor flight test with vision-based navigation.", "result": "Vision system achieved 87.19 Hz position estimation with 99.90% data continuity and 11.47 ms mean latency; all 15 ground commands executed successfully (100% success rate); system used only 15.19% CPU and 1,244 MB RAM with zero stale telemetry messages.", "conclusion": "The hybrid architecture successfully combines certification-grade deterministic control (F') with flexible autonomy (ROS2), validating feasibility for autonomous aerial vehicles while maintaining real-time performance and low resource utilization on embedded platforms."}}
{"id": "2602.15531", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.15531", "abs": "https://arxiv.org/abs/2602.15531", "authors": ["Javier Irigoyen", "Roberto Daza", "Aythami Morales", "Julian Fierrez", "Francisco Jurado", "Alvaro Ortigosa", "Ruben Tolosana"], "title": "GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway", "comment": "10 pages, 3 figures. Published in Intl. Conf. on Learning Analytics & Knowledge Workshops (LAK Workshops 2026, GenAI-LA 26)", "summary": "This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.", "AI": {"tldr": "EduEVAL-DB is a dataset for evaluating AI tutors, containing 854 explanations from human teachers and LLM-simulated teacher roles across 139 ScienceQA questions, annotated with pedagogical risk labels using a five-dimensional rubric.", "motivation": "To support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations, addressing the need for standardized assessment of educational AI systems.", "method": "Created dataset with 854 explanations (1 human + 6 LLM-simulated per question) from 139 ScienceQA questions across K-12 subjects. Used prompt engineering to instantiate teacher roles based on real instructional styles and shortcomings. Developed pedagogical risk rubric with five dimensions: factual correctness, explanatory depth/completeness, focus/relevance, student-level appropriateness, and ideological bias. Applied semi-automatic annotation with expert teacher review.", "result": "Dataset comprises 854 annotated explanations. Preliminary validation experiments benchmarked Gemini 2.5 Pro against Llama 3.1 8B, examining supervised fine-tuning on EduEVAL-DB for pedagogical risk detection on consumer hardware.", "conclusion": "EduEVAL-DB provides a valuable resource for evaluating and training AI tutors, with potential for improving pedagogical risk detection in deployable models, though further validation is needed."}}
{"id": "2602.15400", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15400", "abs": "https://arxiv.org/abs/2602.15400", "authors": ["Zerui Li", "Hongpei Zheng", "Fangguo Zhao", "Aidan Chan", "Jian Zhou", "Sihao Lin", "Shijie Li", "Qi Wu"], "title": "One Agent to Guide Them All: Empowering MLLMs for Vision-and-Language Navigation via Explicit World Representation", "comment": null, "summary": "A navigable agent needs to understand both high-level semantic instructions and precise spatial perceptions. Building navigation agents centered on Multimodal Large Language Models (MLLMs) demonstrates a promising solution due to their powerful generalization ability. However, the current tightly coupled design dramatically limits system performance. In this work, we propose a decoupled design that separates low-level spatial state estimation from high-level semantic planning. Unlike previous methods that rely on predefined, oversimplified textual maps, we introduce an interactive metric world representation that maintains rich and consistent information, allowing MLLMs to interact with and reason on it for decision-making. Furthermore, counterfactual reasoning is introduced to further elicit MLLMs' capacity, while the metric world representation ensures the physical validity of the produced actions. We conduct comprehensive experiments in both simulated and real-world environments. Our method establishes a new zero-shot state-of-the-art, achieving 48.8\\% Success Rate (SR) in R2R-CE and 42.2\\% in RxR-CE benchmarks. Furthermore, to validate the versatility of our metric representation, we demonstrate zero-shot sim-to-real transfer across diverse embodiments, including a wheeled TurtleBot 4 and a custom-built aerial drone. These real-world deployments verify that our decoupled framework serves as a robust, domain-invariant interface for embodied Vision-and-Language navigation.", "AI": {"tldr": "Decoupled navigation framework separates spatial state estimation from semantic planning using interactive metric world representation, achieving SOTA zero-shot performance and enabling sim-to-real transfer across diverse robots.", "motivation": "Current MLLM-based navigation agents suffer from tightly coupled designs that limit performance, and rely on oversimplified textual maps that lack rich spatial information needed for effective decision-making.", "method": "Proposes a decoupled design separating low-level spatial state estimation from high-level semantic planning. Introduces interactive metric world representation that maintains rich, consistent spatial information for MLLM reasoning. Incorporates counterfactual reasoning to elicit MLLM capabilities while ensuring physical validity of actions through metric representation.", "result": "Achieves new zero-shot state-of-the-art: 48.8% Success Rate (SR) in R2R-CE and 42.2% in RxR-CE benchmarks. Demonstrates successful zero-shot sim-to-real transfer across diverse embodiments including TurtleBot 4 wheeled robot and custom-built aerial drone.", "conclusion": "The decoupled framework with interactive metric world representation serves as a robust, domain-invariant interface for embodied Vision-and-Language navigation, enabling effective generalization and real-world deployment across different robotic platforms."}}
{"id": "2602.15532", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15532", "abs": "https://arxiv.org/abs/2602.15532", "authors": ["Ryan Othniel Kearns"], "title": "Quantifying construct validity in large language model evaluations", "comment": null, "summary": "The LLM community often reports benchmark results as if they are synonymous with general model capabilities. However, benchmarks can have problems that distort performance, like test set contamination and annotator error. How can we know that a benchmark is a reliable indicator of some capability that we want to measure? This question concerns the construct validity of LLM benchmarks, and it requires separating benchmark results from capabilities when we model and predict LLM performance.\n  Both social scientists and computer scientists propose formal models - latent factor models and scaling laws - for identifying the capabilities underlying benchmark scores. However, neither technique is satisfactory for construct validity. Latent factor models ignore scaling laws, and as a result, the capabilities they extract often proxy model size. Scaling laws ignore measurement error, and as a result, the capabilities they extract are both uninterpretable and overfit to the observed benchmarks.\n  This thesis presents the structured capabilities model, the first model to extract interpretable and generalisable capabilities from a large collection of LLM benchmark results. I fit this model and its two alternatives on a large sample of results from the OpenLLM Leaderboard. Structured capabilities outperform latent factor models on parsimonious fit indices, and exhibit better out-of-distribution benchmark prediction than scaling laws. These improvements are possible because neither existing approach separates model scale from capabilities in the appropriate way. Model scale should inform capabilities, as in scaling laws, and these capabilities should inform observed results up to measurement error, as in latent factor models. In combining these two insights, structured capabilities demonstrate better explanatory and predictive power for quantifying construct validity in LLM evaluations.", "AI": {"tldr": "The paper introduces a structured capabilities model that combines insights from latent factor models and scaling laws to extract interpretable, generalizable capabilities from LLM benchmark results, addressing construct validity issues in LLM evaluation.", "motivation": "Current LLM benchmarks have problems like test set contamination and annotator error, making it difficult to determine if benchmark results reliably indicate actual model capabilities. Existing approaches (latent factor models and scaling laws) are insufficient for establishing construct validity - latent factor models ignore scaling laws and proxy model size, while scaling laws ignore measurement error and produce uninterpretable, overfit capabilities.", "method": "The structured capabilities model combines insights from both approaches: it incorporates model scale information from scaling laws while accounting for measurement error like latent factor models. The model is fitted on a large sample of results from the OpenLLM Leaderboard and compared against both latent factor models and scaling laws.", "result": "Structured capabilities outperform latent factor models on parsimonious fit indices and demonstrate better out-of-distribution benchmark prediction than scaling laws. The model successfully separates model scale from capabilities in an appropriate way, showing that model scale should inform capabilities (as in scaling laws) and these capabilities should inform observed results up to measurement error (as in latent factor models).", "conclusion": "By combining insights from both existing approaches, the structured capabilities model provides better explanatory and predictive power for quantifying construct validity in LLM evaluations, offering the first method to extract interpretable and generalizable capabilities from large collections of LLM benchmark results."}}
{"id": "2602.15424", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15424", "abs": "https://arxiv.org/abs/2602.15424", "authors": ["Branimir \u0106aran", "Vladimir Mili\u0107", "Bojan Jerbi\u0107"], "title": "Lyapunov-Based $\\mathcal{L}_2$-Stable PI-Like Control of a Four-Wheel Independently Driven and Steered Robot", "comment": "SUBMITTED FOR POTENTIAL PUBLICATION IN IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION", "summary": "In this letter, Lyapunov-based synthesis of a PI-like controller is proposed for $\\mathcal{L}_2$-stable motion control of an independently driven and steered four-wheel mobile robot. An explicit, structurally verified model is used to enable systematic controller design with stability and performance guarantees suitable for real-time operation. A Lyapunov function is constructed to yield explicit bounds and $\\mathcal{L}_2$ stability results, supporting feedback synthesis that reduces configuration dependent effects. The resulting control law maintains a PI-like form suitable for standard embedded implementation while preserving rigorous stability properties. Effectiveness and robustness are demonstrated experimentally on a real four-wheel mobile robot platform.", "AI": {"tldr": "Lyapunov-based synthesis of a PI-like controller for L\u2082-stable motion control of a four-wheel mobile robot with independent drive and steering.", "motivation": "To develop a systematic controller design for four-wheel mobile robots that provides stability and performance guarantees suitable for real-time operation, addressing configuration-dependent effects while maintaining practical implementation feasibility.", "method": "Uses an explicit, structurally verified model for systematic controller design. Constructs a Lyapunov function to yield explicit bounds and L\u2082 stability results, enabling feedback synthesis that reduces configuration-dependent effects. The resulting control law maintains a PI-like form suitable for standard embedded implementation.", "result": "The proposed controller achieves L\u2082-stable motion control with rigorous stability properties. Effectiveness and robustness are demonstrated experimentally on a real four-wheel mobile robot platform.", "conclusion": "The Lyapunov-based synthesis successfully produces a PI-like controller for four-wheel mobile robots that combines theoretical guarantees (L\u2082 stability, explicit bounds) with practical implementation advantages (standard embedded form, real-time operation), validated through experimental demonstration."}}
{"id": "2602.15553", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15553", "abs": "https://arxiv.org/abs/2602.15553", "authors": ["Gabriele Conte", "Alessio Mattiace", "Gianni Carmosino", "Potito Aghilar", "Giovanni Servedio", "Francesco Musicco", "Vito Walter Anelli", "Tommaso Di Noia", "Francesco Maria Donini"], "title": "RUVA: Personalized Transparent On-Device Graph Reasoning", "comment": null, "summary": "The Personal AI landscape is currently dominated by \"Black Box\" Retrieval-Augmented Generation. While standard vector databases offer statistical matching, they suffer from a fundamental lack of accountability: when an AI hallucinates or retrieves sensitive data, the user cannot inspect the cause nor correct the error. Worse, \"deleting\" a concept from a vector space is mathematically imprecise, leaving behind probabilistic \"ghosts\" that violate true privacy. We propose Ruva, the first \"Glass Box\" architecture designed for Human-in-the-Loop Memory Curation. Ruva grounds Personal AI in a Personal Knowledge Graph, enabling users to inspect what the AI knows and to perform precise redaction of specific facts. By shifting the paradigm from Vector Matching to Graph Reasoning, Ruva ensures the \"Right to be Forgotten.\" Users are the editors of their own lives; Ruva hands them the pen. The project and the demo video are available at http://sisinf00.poliba.it/ruva/.", "AI": {"tldr": "Ruva introduces a \"Glass Box\" Personal AI architecture using Personal Knowledge Graphs instead of black-box vector databases, enabling user inspection and precise fact redaction to ensure privacy and the \"Right to be Forgotten.\"", "motivation": "Current Personal AI systems rely on \"Black Box\" Retrieval-Augmented Generation with vector databases that lack accountability. Users cannot inspect causes of AI hallucinations or sensitive data retrieval, and \"deleting\" concepts from vector spaces is mathematically imprecise, leaving probabilistic \"ghosts\" that violate true privacy.", "method": "Ruva proposes a \"Glass Box\" architecture for Human-in-the-Loop Memory Curation. It grounds Personal AI in a Personal Knowledge Graph rather than vector matching, shifting the paradigm from statistical vector matching to graph reasoning. This enables users to inspect what the AI knows and perform precise redaction of specific facts.", "result": "Ruva ensures the \"Right to be Forgotten\" by allowing users to be editors of their own lives through precise fact redaction. The architecture provides transparency and accountability where traditional vector databases fail, eliminating probabilistic \"ghosts\" that compromise privacy.", "conclusion": "Ruva represents a paradigm shift from opaque vector matching to transparent graph reasoning in Personal AI, empowering users with inspection capabilities and precise data control to address fundamental privacy and accountability limitations of current systems."}}
{"id": "2602.15513", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15513", "abs": "https://arxiv.org/abs/2602.15513", "authors": ["Ji Li", "Jing Xia", "Mingyi Li", "Shiyan Hu"], "title": "Improving MLLMs in Embodied Exploration and Question Answering with Human-Inspired Memory Modeling", "comment": null, "summary": "Deploying Multimodal Large Language Models as the brain of embodied agents remains challenging, particularly under long-horizon observations and limited context budgets. Existing memory assisted methods often rely on textual summaries, which discard rich visual and spatial details and remain brittle in non-stationary environments. In this work, we propose a non-parametric memory framework that explicitly disentangles episodic and semantic memory for embodied exploration and question answering. Our retrieval-first, reasoning-assisted paradigm recalls episodic experiences via semantic similarity and verifies them through visual reasoning, enabling robust reuse of past observations without rigid geometric alignment. In parallel, we introduce a program-style rule extraction mechanism that converts experiences into structured, reusable semantic memory, facilitating cross-environment generalization. Extensive experiments demonstrate state-of-the-art performance on embodied question answering and exploration benchmarks, yielding a 7.3% gain in LLM-Match and an 11.4% gain in LLM MatchXSPL on A-EQA, as well as +7.7% success rate and +6.8% SPL on GOAT-Bench. Analyses reveal that our episodic memory primarily improves exploration efficiency, while semantic memory strengthens complex reasoning of embodied agents.", "AI": {"tldr": "Proposes a non-parametric memory framework with disentangled episodic and semantic memory for embodied agents, using retrieval-first reasoning-assisted paradigm and program-style rule extraction to improve exploration and question answering.", "motivation": "Existing memory methods for multimodal LLMs in embodied agents rely on textual summaries that discard visual/spatial details and are brittle in non-stationary environments, especially under long-horizon observations and limited context budgets.", "method": "Non-parametric memory framework with explicit episodic-semantic memory disentanglement. Retrieval-first, reasoning-assisted paradigm recalls episodic experiences via semantic similarity and verifies through visual reasoning. Program-style rule extraction converts experiences into structured, reusable semantic memory for cross-environment generalization.", "result": "State-of-the-art performance on embodied QA and exploration benchmarks: 7.3% gain in LLM-Match and 11.4% gain in LLM MatchXSPL on A-EQA; +7.7% success rate and +6.8% SPL on GOAT-Bench. Episodic memory improves exploration efficiency, semantic memory strengthens complex reasoning.", "conclusion": "The proposed memory framework effectively addresses limitations of existing methods by preserving rich visual/spatial details through disentangled episodic-semantic memory and enabling robust reuse of past observations without rigid geometric alignment."}}
{"id": "2602.15580", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15580", "abs": "https://arxiv.org/abs/2602.15580", "authors": ["Hongxuan Wu", "Yukun Zhang", "Xueqing Zhou"], "title": "How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning", "comment": null, "summary": "When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information Decomposition (PID) that decomposes the predictive information at each Transformer layer into redundant, vision-unique, language-unique, and synergistic components. To make PID tractable for high-dimensional neural representations, we introduce \\emph{PID Flow}, a pipeline combining dimensionality reduction, normalizing-flow Gaussianization, and closed-form Gaussian PID estimation. Applying this framework to LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks, we uncover a consistent \\emph{modal transduction} pattern: visual-unique information peaks early and decays with depth, language-unique information surges in late layers to account for roughly 82\\% of the final prediction, and cross-modal synergy remains below 2\\%. This trajectory is highly stable across model variants (layer-wise correlations $>$0.96) yet strongly task-dependent, with semantic redundancy governing the detailed information fingerprint. To establish causality, we perform targeted Image$\\rightarrow$Question attention knockouts and show that disrupting the primary transduction pathway induces predictable increases in trapped visual-unique information, compensatory synergy, and total information cost -- effects that are strongest in vision-dependent tasks and weakest in high-redundancy tasks. Together, these results provide an information-theoretic, causal account of how vision becomes language in multimodal Transformers, and offer quantitative guidance for identifying architectural bottlenecks where modality-specific information is lost.", "AI": {"tldr": "The paper introduces PID Flow, a framework using Partial Information Decomposition to analyze how multimodal Transformers process visual questions, revealing a consistent pattern where visual information peaks early, language dominates late layers, and cross-modal synergy remains minimal.", "motivation": "To understand whether multimodal Transformer predictions are driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation, and how this information structure evolves across layers in visual question answering models.", "method": "Developed PID Flow pipeline combining dimensionality reduction, normalizing-flow Gaussianization, and closed-form Gaussian PID estimation to make Partial Information Decomposition tractable for high-dimensional neural representations. Applied to LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks.", "result": "Uncovered consistent modal transduction pattern: visual-unique information peaks early and decays with depth, language-unique information surges in late layers (accounting for ~82% of final prediction), and cross-modal synergy remains below 2%. This pattern is stable across model variants (layer-wise correlations >0.96) but task-dependent. Targeted attention knockouts causally demonstrate that disrupting primary transduction pathways increases trapped visual-unique information, compensatory synergy, and total information cost.", "conclusion": "The study provides an information-theoretic, causal account of how vision becomes language in multimodal Transformers, revealing that predictions are primarily driven by linguistic reasoning in late layers rather than genuine cross-modal fusion. Offers quantitative guidance for identifying architectural bottlenecks where modality-specific information is lost."}}
{"id": "2602.15533", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15533", "abs": "https://arxiv.org/abs/2602.15533", "authors": ["Welf Rehberg", "Mihir Kulkarni", "Philipp Weiss", "Kostas Alexis"], "title": "Efficient Knowledge Transfer for Jump-Starting Control Policy Learning of Multirotors through Physics-Aware Neural Architectures", "comment": "8 pages. Accepted to IEEE Robotics and Automation Letters", "summary": "Efficiently training control policies for robots is a major challenge that can greatly benefit from utilizing knowledge gained from training similar systems through cross-embodiment knowledge transfer. In this work, we focus on accelerating policy training using a library-based initialization scheme that enables effective knowledge transfer across multirotor configurations. By leveraging a physics-aware neural control architecture that combines a reinforcement learning-based controller and a supervised control allocation network, we enable the reuse of previously trained policies. To this end, we utilize a policy evaluation-based similarity measure that identifies suitable policies for initialization from a library. We demonstrate that this measure correlates with the reduction in environment interactions needed to reach target performance and is therefore suited for initialization. Extensive simulation and real-world experiments confirm that our control architecture achieves state-of-the-art control performance, and that our initialization scheme saves on average up to $73.5\\%$ of environment interactions (compared to training a policy from scratch) across diverse quadrotor and hexarotor designs, paving the way for efficient cross-embodiment transfer in reinforcement learning.", "AI": {"tldr": "A library-based initialization scheme using policy evaluation similarity enables efficient cross-embodiment knowledge transfer for multirotor control policies, reducing training interactions by up to 73.5%.", "motivation": "Training control policies for robots is challenging and can benefit from transferring knowledge between similar systems. The paper aims to accelerate policy training for multirotor configurations by reusing previously trained policies through cross-embodiment knowledge transfer.", "method": "Uses a physics-aware neural control architecture combining reinforcement learning-based controller and supervised control allocation network. Implements a library-based initialization scheme with policy evaluation-based similarity measure to identify suitable policies for initialization from a library of previously trained policies.", "result": "The policy evaluation similarity measure correlates with reduction in environment interactions needed to reach target performance. The approach saves on average up to 73.5% of environment interactions compared to training from scratch across diverse quadrotor and hexarotor designs. The control architecture achieves state-of-the-art performance in both simulation and real-world experiments.", "conclusion": "The proposed library-based initialization scheme with policy evaluation similarity enables effective cross-embodiment knowledge transfer for multirotor control policies, significantly reducing training time and interactions while maintaining high control performance."}}
{"id": "2602.15635", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15635", "abs": "https://arxiv.org/abs/2602.15635", "authors": ["Konstantin Sidorov"], "title": "On inferring cumulative constraints", "comment": "17 pages, 6 figures, 4 tables; submitted to the 32nd International Conference on Principles and Practice of Constraint Programming (CP 2026)", "summary": "Cumulative constraints are central in scheduling with constraint programming, yet propagation is typically performed per constraint, missing multi-resource interactions and causing severe slowdowns on some benchmarks. I present a preprocessing method for inferring additional cumulative constraints that capture such interactions without search-time probing. This approach interprets cumulative constraints as linear inequalities over occupancy vectors and generates valid inequalities by (i) discovering covers, the sets of tasks that cannot run in parallel, (ii) strengthening the cover inequalities for the discovered sets with lifting, and (iii) injecting the resulting constraints back into the scheduling problem instance. Experiments on standard RCPSP and RCPSP/max test suites show that these inferred constraints improve search performance and tighten objective bounds on favorable instances, while incurring little degradation on unfavorable ones. Additionally, these experiments discover 25 new lower bounds and five new best solutions; eight of the lower bounds are obtained directly from the inferred constraints.", "AI": {"tldr": "A preprocessing method that infers additional cumulative constraints from existing ones by discovering task covers that cannot run in parallel, strengthening them via lifting, and injecting them back to improve search performance and tighten bounds in scheduling problems.", "motivation": "Cumulative constraints are fundamental in scheduling with constraint programming, but current propagation methods operate per constraint, missing multi-resource interactions and causing performance slowdowns on certain benchmarks. There's a need to capture these interactions without search-time probing to improve efficiency.", "method": "The method interprets cumulative constraints as linear inequalities over occupancy vectors and generates valid inequalities through three steps: (1) discovering covers (sets of tasks that cannot run in parallel), (2) strengthening these cover inequalities via lifting techniques, and (3) injecting the resulting constraints back into the original scheduling problem instance as preprocessing.", "result": "Experiments on standard RCPSP and RCPSP/max test suites show improved search performance and tighter objective bounds on favorable instances with minimal degradation on unfavorable ones. The method discovered 25 new lower bounds and five new best solutions, with eight lower bounds obtained directly from the inferred constraints.", "conclusion": "The preprocessing approach successfully captures multi-resource interactions in cumulative scheduling constraints, leading to improved search performance, tighter bounds, and new optimality proofs without requiring search-time probing, demonstrating practical value in constraint programming for scheduling."}}
{"id": "2602.15543", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15543", "abs": "https://arxiv.org/abs/2602.15543", "authors": ["Young-Chae Son", "Jung-Woo Lee", "Yoon-Ji Choi", "Dae-Kwan Ko", "Soo-Chul Lim"], "title": "Selective Perception for Robot: Task-Aware Attention in Multimodal VLA", "comment": null, "summary": "In robotics, Vision-Language-Action (VLA) models that integrate diverse multimodal signals from multi-view inputs have emerged as an effective approach. However, most prior work adopts static fusion that processes all visual inputs uniformly, which incurs unnecessary computational overhead and allows task-irrelevant background information to act as noise. Inspired by the principles of human active perception, we propose a dynamic information fusion framework designed to maximize the efficiency and robustness of VLA models. Our approach introduces a lightweight adaptive routing architecture that analyzes the current text prompt and observations from a wrist-mounted camera in real-time to predict the task-relevance of multiple camera views. By conditionally attenuating computations for views with low informational utility and selectively providing only essential visual features to the policy network, Our framework achieves computation efficiency proportional to task relevance. Furthermore, to efficiently secure large-scale annotation data for router training, we established an automated labeling pipeline utilizing Vision-Language Models (VLMs) to minimize data collection and annotation costs. Experimental results in real-world robotic manipulation scenarios demonstrate that the proposed approach achieves significant improvements in both inference efficiency and control performance compared to existing VLA models, validating the effectiveness and practicality of dynamic information fusion in resource-constrained, real-time robot control environments.", "AI": {"tldr": "Dynamic information fusion framework for VLA models using adaptive routing to selectively process camera views based on task relevance, improving efficiency and performance in robotic manipulation.", "motivation": "Current VLA models use static fusion that processes all visual inputs uniformly, leading to unnecessary computational overhead and allowing task-irrelevant background information to act as noise. This is inefficient for resource-constrained, real-time robot control environments.", "method": "Proposes a lightweight adaptive routing architecture that analyzes text prompts and wrist-camera observations in real-time to predict task-relevance of multiple camera views. Conditionally attenuates computations for low-utility views and selectively provides essential visual features to the policy network. Uses an automated labeling pipeline with VLMs to generate training data for the router.", "result": "Achieves significant improvements in both inference efficiency and control performance compared to existing VLA models in real-world robotic manipulation scenarios. Computation efficiency becomes proportional to task relevance.", "conclusion": "Dynamic information fusion is effective and practical for resource-constrained, real-time robot control environments, validating the benefits of selective visual processing inspired by human active perception principles."}}
{"id": "2602.15645", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.15645", "abs": "https://arxiv.org/abs/2602.15645", "authors": ["Lucas Elbert Suryana", "Farah Bierenga", "Sanne van Buuren", "Pepijn Kooij", "Elsefien Tulleners", "Federico Scari", "Simeon Calvert", "Bart van Arem", "Arkady Zgonnikov"], "title": "CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving", "comment": "21 pages, on submission to Transportation Research Part C", "summary": "Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive decision making or merely post hoc rationalizations. This limitation is especially significant in safety critical domains because it can create false confidence. To address this gap, we propose CARE Drive, Context Aware Reasons Evaluation for Driving, a model agnostic framework for evaluating reason responsiveness in vision language models applied to automated driving. CARE Drive compares baseline and reason augmented model decisions under controlled contextual variation to assess whether human reasons causally influence decision behavior. The framework employs a two stage evaluation process. Prompt calibration ensures stable outputs. Systematic contextual perturbation then measures decision sensitivity to human reasons such as safety margins, social pressure, and efficiency constraints. We demonstrate CARE Drive in a cyclist overtaking scenario involving competing normative considerations. Results show that explicit human reasons significantly influence model decisions, improving alignment with expert recommended behavior. However, responsiveness varies across contextual factors, indicating uneven sensitivity to different types of reasons. These findings provide empirical evidence that reason responsiveness in foundation models can be systematically evaluated without modifying model parameters.", "AI": {"tldr": "CARE Drive is a model-agnostic framework for evaluating whether vision language models in automated driving make decisions based on human-relevant reasons rather than just post-hoc rationalizations.", "motivation": "Existing evaluation methods for foundation models in automated driving focus on outcome-based performance (safety, trajectory accuracy) but fail to assess whether model decisions reflect genuine human-relevant reasoning. This creates false confidence in safety-critical domains where understanding the reasoning behind decisions is crucial.", "method": "CARE Drive uses a two-stage evaluation process: 1) Prompt calibration to ensure stable model outputs, and 2) Systematic contextual perturbation to measure decision sensitivity to human reasons (safety margins, social pressure, efficiency constraints). The framework compares baseline and reason-augmented model decisions under controlled contextual variation to assess causal influence of human reasons.", "result": "Explicit human reasons significantly influence model decisions, improving alignment with expert-recommended behavior. However, responsiveness varies across contextual factors, indicating uneven sensitivity to different types of reasons. The framework successfully demonstrates reason responsiveness evaluation in a cyclist overtaking scenario with competing normative considerations.", "conclusion": "CARE Drive provides empirical evidence that reason responsiveness in foundation models can be systematically evaluated without modifying model parameters, addressing a critical gap in assessing whether automated driving models make decisions based on genuine human-relevant reasoning rather than post-hoc rationalizations."}}
{"id": "2602.15549", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15549", "abs": "https://arxiv.org/abs/2602.15549", "authors": ["Guoqin Tang", "Qingxuan Jia", "Gang Chen", "Tong Li", "Zeyuan Huang", "Zihang Lv", "Ning Ji"], "title": "VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing", "comment": null, "summary": "Vision-language model (VLM) shows promise for high-level planning in smart manufacturing, yet their deployment in dynamic workcells faces two critical challenges: (1) stateless operation, they cannot persistently track out-of-view states, causing world-state drift; and (2) opaque reasoning, failures are difficult to diagnose, leading to costly blind retries. This paper presents VLM-DEWM, a cognitive architecture that decouples VLM reasoning from world-state management through a persistent, queryable Dynamic External World Model (DEWM). Each VLM decision is structured into an Externalizable Reasoning Trace (ERT), comprising action proposal, world belief, and causal assumption, which is validated against DEWM before execution. When failures occur, discrepancy analysis between predicted and observed states enables targeted recovery instead of global replanning. We evaluate VLM-DEWM on multi-station assembly, large-scale facility exploration, and real-robot recovery under induced failures. Compared to baseline memory-augmented VLM systems, VLM DEWM improves state-tracking accuracy from 56% to 93%, increases recovery success rate from below 5% to 95%, and significantly reduces computational overhead through structured memory. These results establish VLM-DEWM as a verifiable and resilient solution for long-horizon robotic operations in dynamic manufacturing environments.", "AI": {"tldr": "VLM-DEWM is a cognitive architecture that decouples VLM reasoning from world-state management using a persistent Dynamic External World Model to address stateless operation and opaque reasoning challenges in smart manufacturing.", "motivation": "Vision-language models show promise for high-level planning in smart manufacturing but face two critical deployment challenges: (1) stateless operation causing world-state drift due to inability to persistently track out-of-view states, and (2) opaque reasoning making failures difficult to diagnose and leading to costly blind retries.", "method": "VLM-DEWM decouples VLM reasoning from world-state management through a persistent, queryable Dynamic External World Model (DEWM). Each VLM decision is structured into an Externalizable Reasoning Trace (ERT) comprising action proposal, world belief, and causal assumption, which is validated against DEWM before execution. When failures occur, discrepancy analysis between predicted and observed states enables targeted recovery instead of global replanning.", "result": "Compared to baseline memory-augmented VLM systems, VLM-DEWM improves state-tracking accuracy from 56% to 93%, increases recovery success rate from below 5% to 95%, and significantly reduces computational overhead through structured memory. Evaluations were conducted on multi-station assembly, large-scale facility exploration, and real-robot recovery under induced failures.", "conclusion": "VLM-DEWM establishes a verifiable and resilient solution for long-horizon robotic operations in dynamic manufacturing environments by addressing stateless operation and opaque reasoning challenges through structured memory and discrepancy analysis."}}
{"id": "2602.15669", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15669", "abs": "https://arxiv.org/abs/2602.15669", "authors": ["Xiachong Feng", "Liang Zhao", "Weihong Zhong", "Yichong Huang", "Yuxuan Gu", "Lingpeng Kong", "Xiaocheng Feng", "Bing Qin"], "title": "PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra", "comment": "ICLR 2026", "summary": "Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.", "AI": {"tldr": "PERSONA is a training-free framework that achieves fine-tuning level personality control in LLMs by manipulating orthogonal personality vectors in activation space through vector arithmetic operations.", "motivation": "Current methods for personality control in LLMs rely on static prompting or expensive fine-tuning, which fail to capture the dynamic and compositional nature of human personality traits.", "method": "Three-stage framework: 1) Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; 2) Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); 3) Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference.", "result": "On PersonalityBench, achieves mean score of 9.60, nearly matching supervised fine-tuning upper bound of 9.61 without gradient updates. On Persona-Evolve benchmark for dynamic adaptation, achieves up to 91% win rates across diverse model families.", "conclusion": "Personality traits in LLMs are mathematically tractable as extractable, approximately orthogonal directions in representation space, opening new directions for interpretable and efficient behavioral control without training."}}
{"id": "2602.15567", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15567", "abs": "https://arxiv.org/abs/2602.15567", "authors": ["Jieting Long", "Dechuan Liu", "Weidong Cai", "Ian Manchester", "Weiming Zhi"], "title": "Constraining Streaming Flow Models for Adapting Learned Robot Trajectory Distributions", "comment": "8 pages, 8 figure", "summary": "Robot motion distributions often exhibit multi-modality and require flexible generative models for accurate representation. Streaming Flow Policies (SFPs) have recently emerged as a powerful paradigm for generating robot trajectories by integrating learned velocity fields directly in action space, enabling smooth and reactive control. However, existing formulations lack mechanisms for adapting trajectories post-training to enforce safety and task-specific constraints. We propose Constraint-Aware Streaming Flow (CASF), a framework that augments streaming flow policies with constraint-dependent metrics that reshape the learned velocity field during execution. CASF models each constraint, defined in either the robot's workspace or configuration space, as a differentiable distance function that is converted into a local metric and pulled back into the robot's control space. Far from restricted regions, the resulting metric reduces to the identity; near constraint boundaries, it smoothly attenuates or redirects motion, effectively deforming the underlying flow to maintain safety. This allows trajectories to be adapted in real time, ensuring that robot actions respect joint limits, avoid collisions, and remain within feasible workspaces, while preserving the multi-modal and reactive properties of streaming flow policies. We demonstrate CASF in simulated and real-world manipulation tasks, showing that it produces constraint-satisfying trajectories that remain smooth, feasible, and dynamically consistent, outperforming standard post-hoc projection baselines.", "AI": {"tldr": "CASF extends streaming flow policies with constraint-dependent metrics that reshape velocity fields in real-time to enforce safety and task constraints while preserving multi-modality and reactivity.", "motivation": "Streaming Flow Policies (SFPs) effectively generate multi-modal robot trajectories but lack mechanisms for adapting trajectories post-training to enforce safety and task-specific constraints during execution.", "method": "CASF augments SFPs with constraint-dependent metrics that reshape learned velocity fields. Each constraint is modeled as a differentiable distance function, converted to a local metric, and pulled back into control space. The metric smoothly attenuates or redirects motion near constraint boundaries while reducing to identity far from restricted regions.", "result": "CASF produces constraint-satisfying trajectories that remain smooth, feasible, and dynamically consistent in simulated and real-world manipulation tasks, outperforming standard post-hoc projection baselines.", "conclusion": "CASF enables real-time adaptation of streaming flow policies to enforce safety constraints while preserving their multi-modal and reactive properties, providing a practical framework for constraint-aware robot motion generation."}}
{"id": "2602.15725", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15725", "abs": "https://arxiv.org/abs/2602.15725", "authors": ["Sarim Chaudhry"], "title": "Recursive Concept Evolution for Compositional Reasoning in Large Language Models", "comment": null, "summary": "Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.", "AI": {"tldr": "RCE enables LLMs to dynamically modify their internal representation geometry during inference by generating low-rank concept subspaces, improving performance on compositional reasoning tasks.", "motivation": "Current LLMs perform poorly on compositional reasoning benchmarks despite strong overall performance, as existing methods only expand token-level search without modifying the model's fixed latent representation space, causing performance collapse when required abstractions aren't encoded.", "method": "Recursive Concept Evolution (RCE) framework that allows pretrained LLMs to modify internal representation geometry during inference through dynamically generated low-rank concept subspaces. These subspaces are spawned when representational inadequacy is detected, selected via minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability.", "result": "RCE integrated with Mistral-7B yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE benchmarks.", "conclusion": "RCE enables LLMs to construct new abstractions rather than just recombining existing ones, addressing the fundamental limitation of fixed representation spaces in compositional reasoning tasks."}}
{"id": "2602.15608", "categories": ["cs.RO", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.15608", "abs": "https://arxiv.org/abs/2602.15608", "authors": ["Mostafa A. Atalla", "Daan van Bemmel", "Jack Cummings", "Paul Breedveld", "Micha\u00ebl Wiertlewski", "Aim\u00e9e Sakes"], "title": "Grip as Needed, Glide on Demand: Ultrasonic Lubrication for Robotic Locomotion", "comment": "Accepted for publication in the 2026 IEEE International Conference on Robotics and Automation (ICRA) in Vienna", "summary": "Friction is the essential mediator of terrestrial locomotion, yet in robotic systems it is almost always treated as a passive property fixed by surface materials and conditions. Here, we introduce ultrasonic lubrication as a method to actively control friction in robotic locomotion. By exciting resonant structures at ultrasonic frequencies, contact interfaces can dynamically switch between \"grip\" and \"slip\" states, enabling locomotion. We developed two friction control modules, a cylindrical design for lumen-like environments and a flat-plate design for external surfaces, and integrated them into bio-inspired systems modeled after inchworm and wasp ovipositor locomotion. Both systems achieved bidirectional locomotion with nearly perfect locomotion efficiencies that exceeded 90%. Friction characterization experiments further demonstrated substantial friction reduction across various surfaces, including rigid, soft, granular, and biological tissue interfaces, under dry and wet conditions, and on surfaces with different levels of roughness, confirming the broad applicability of ultrasonic lubrication to locomotion tasks. These findings establish ultrasonic lubrication as a viable active friction control mechanism for robotic locomotion, with the potential to reduce design complexity and improve efficiency of robotic locomotion systems.", "AI": {"tldr": "Ultrasonic lubrication enables active friction control for robotic locomotion by dynamically switching between grip and slip states, achieving >90% efficiency in bio-inspired systems across diverse surfaces.", "motivation": "Friction is essential for terrestrial locomotion but is typically treated as a passive, fixed property in robotics. The authors aim to develop active friction control methods to improve robotic locomotion efficiency and reduce design complexity.", "method": "Developed ultrasonic lubrication by exciting resonant structures at ultrasonic frequencies to dynamically control friction. Created two friction control modules (cylindrical for lumen-like environments, flat-plate for external surfaces) and integrated them into bio-inspired inchworm and wasp ovipositor locomotion systems.", "result": "Both systems achieved bidirectional locomotion with nearly perfect efficiencies exceeding 90%. Friction characterization showed substantial reduction across various surfaces (rigid, soft, granular, biological tissue) under dry/wet conditions and different roughness levels, confirming broad applicability.", "conclusion": "Ultrasonic lubrication is established as a viable active friction control mechanism for robotic locomotion, with potential to reduce design complexity and improve efficiency of robotic locomotion systems."}}
{"id": "2602.15776", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15776", "abs": "https://arxiv.org/abs/2602.15776", "authors": ["Yiqin Yang", "Xu Yang", "Yuhua Jiang", "Ni Mu", "Hao Hu", "Runpeng Xie", "Ziyou Zhang", "Siyuan Li", "Yuan-Hua Ni", "Qianchuan Zhao", "Bo Xu"], "title": "GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems", "comment": null, "summary": "In the realm of multi-agent systems, the challenge of \\emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.", "AI": {"tldr": "GlobeDiff: A multi-modal diffusion algorithm for global state inference in partially observable multi-agent systems", "motivation": "Existing approaches for partial observability in multi-agent systems have limitations - belief-based methods don't fully leverage global information, and communication methods lack robust models to effectively use auxiliary information.", "method": "Global State Diffusion Algorithm (GlobeDiff) formulates state inference as a multi-modal diffusion process to infer global state from local observations, overcoming ambiguities in state estimation.", "result": "Theoretical proof that GlobeDiff's estimation error under both unimodal and multi-modal distributions can be bounded, with extensive experiments showing superior performance and accurate global state inference.", "conclusion": "GlobeDiff effectively addresses partial observability challenges in multi-agent systems through multi-modal diffusion-based global state inference with provable error bounds and superior empirical performance."}}
{"id": "2602.15633", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15633", "abs": "https://arxiv.org/abs/2602.15633", "authors": ["Haichao Liu", "Yufeng Hu", "Shuang Wang", "Kangjun Guo", "Jun Ma", "Jinni Zhou"], "title": "SpecFuse: A Spectral-Temporal Fusion Predictive Control Framework for UAV Landing on Oscillating Marine Platforms", "comment": "8 pages, 5 figures, 4 tables", "summary": "Autonomous landing of Uncrewed Aerial Vehicles (UAVs) on oscillating marine platforms is severely constrained by wave-induced multi-frequency oscillations, wind disturbances, and prediction phase lags in motion prediction. Existing methods either treat platform motion as a general random process or lack explicit modeling of wave spectral characteristics, leading to suboptimal performance under dynamic sea conditions. To address these limitations, we propose SpecFuse: a novel spectral-temporal fusion predictive control framework that integrates frequency-domain wave decomposition with time-domain recursive state estimation for high-precision 6-DoF motion forecasting of Uncrewed Surface Vehicles (USVs). The framework explicitly models dominant wave harmonics to mitigate phase lags, refining predictions in real time via IMU data without relying on complex calibration. Additionally, we design a hierarchical control architecture featuring a sampling-based HPO-RRT* algorithm for dynamic trajectory planning under non-convex constraints and a learning-augmented predictive controller that fuses data-driven disturbance compensation with optimization-based execution. Extensive validations (2,000 simulations + 8 lake experiments) show our approach achieves a 3.2 cm prediction error, 4.46 cm landing deviation, 98.7% / 87.5% success rates (simulation / real-world), and 82 ms latency on embedded hardware, outperforming state-of-the-art methods by 44%-48% in accuracy. Its robustness to wave-wind coupling disturbances supports critical maritime missions such as search and rescue and environmental monitoring. All code, experimental configurations, and datasets will be released as open-source to facilitate reproducibility.", "AI": {"tldr": "SpecFuse: A spectral-temporal fusion predictive control framework for autonomous UAV landing on oscillating marine platforms, achieving high-precision 6-DoF motion forecasting and robust landing performance through wave decomposition and hierarchical control.", "motivation": "Autonomous UAV landing on marine platforms is challenging due to wave-induced multi-frequency oscillations, wind disturbances, and prediction phase lags. Existing methods either treat platform motion as random processes or lack explicit wave spectral modeling, resulting in suboptimal performance under dynamic sea conditions.", "method": "SpecFuse integrates frequency-domain wave decomposition with time-domain recursive state estimation for 6-DoF motion forecasting. It explicitly models dominant wave harmonics to mitigate phase lags and refines predictions via IMU data. The hierarchical control architecture includes: 1) sampling-based HPO-RRT* algorithm for dynamic trajectory planning under non-convex constraints, and 2) learning-augmented predictive controller that fuses data-driven disturbance compensation with optimization-based execution.", "result": "Extensive validation (2,000 simulations + 8 lake experiments) shows: 3.2 cm prediction error, 4.46 cm landing deviation, 98.7% simulation success rate, 87.5% real-world success rate, 82 ms latency on embedded hardware. Outperforms state-of-the-art methods by 44%-48% in accuracy. Demonstrates robustness to wave-wind coupling disturbances.", "conclusion": "SpecFuse provides a robust solution for autonomous UAV landing on oscillating marine platforms by explicitly modeling wave spectral characteristics and integrating spectral-temporal fusion with hierarchical control. The approach enables critical maritime missions like search and rescue and environmental monitoring. All code, configurations, and datasets will be open-sourced for reproducibility."}}
{"id": "2602.15785", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15785", "abs": "https://arxiv.org/abs/2602.15785", "authors": ["Jessica Hullman", "David Broska", "Huaman Sun", "Aaron Shaw"], "title": "This human study did not involve human subjects: Validating LLM simulations as behavioral evidence", "comment": null, "summary": "A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.", "AI": {"tldr": "LLMs as synthetic participants in social science research require different validation strategies: heuristic approaches for exploratory work and statistical calibration for confirmatory research, with both dependent on how well LLMs approximate target populations.", "motivation": "To provide guidance on when LLM-based simulations support valid inference about human behavior in social science experiments, addressing the gap between growing use of LLMs as synthetic participants and limited methodological guidance for valid causal inference.", "method": "Contrasts two validation strategies: heuristic approaches (prompt engineering, model fine-tuning) for exploratory research, and statistical calibration (combining auxiliary human data with statistical adjustments) for confirmatory research with formal statistical guarantees.", "result": "Heuristic approaches lack formal statistical guarantees but are useful for exploratory tasks; statistical calibration preserves validity under explicit assumptions and provides more precise causal effect estimates at lower cost than human-only experiments.", "conclusion": "Both approaches depend on LLMs' ability to approximate relevant populations, and researchers should avoid myopic focus on simply substituting LLMs for humans, considering broader opportunities in study design."}}
{"id": "2602.15642", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15642", "abs": "https://arxiv.org/abs/2602.15642", "authors": ["Alexander Wachter", "Alexander Willert", "Marc-Philip Ecker", "Christian Hartl-Nesic"], "title": "Spatially-Aware Adaptive Trajectory Optimization with Controller-Guided Feedback for Autonomous Racing", "comment": "Accepted at ICRA 2026", "summary": "We present a closed-loop framework for autonomous raceline optimization that combines NURBS-based trajectory representation, CMA-ES global trajectory optimization, and controller-guided spatial feedback. Instead of treating tracking errors as transient disturbances, our method exploits them as informative signals of local track characteristics via a Kalman-inspired spatial update. This enables the construction of an adaptive, acceleration-based constraint map that iteratively refines trajectories toward near-optimal performance under spatially varying track and vehicle behavior. In simulation, our approach achieves a 17.38% lap time reduction compared to a controller parametrized with maximum static acceleration. On real hardware, tested with different tire compounds ranging from high to low friction, we obtain a 7.60% lap time improvement without explicitly parametrizing friction. This demonstrates robustness to changing grip conditions in real-world scenarios.", "AI": {"tldr": "Closed-loop autonomous raceline optimization framework using NURBS trajectory representation, CMA-ES global optimization, and controller-guided spatial feedback to exploit tracking errors as informative signals for adaptive constraint mapping.", "motivation": "Traditional raceline optimization treats tracking errors as disturbances rather than informative signals. The authors aim to develop a framework that exploits tracking errors to understand local track characteristics and adapt trajectories to varying conditions like changing friction.", "method": "Combines NURBS-based trajectory representation for smoothness, CMA-ES for global trajectory optimization, and a controller-guided spatial feedback system with Kalman-inspired spatial update. The method uses tracking errors to construct adaptive acceleration-based constraint maps that iteratively refine trajectories.", "result": "17.38% lap time reduction in simulation compared to controller with maximum static acceleration. On real hardware with varying tire compounds (high to low friction), achieved 7.60% lap time improvement without explicit friction parametrization.", "conclusion": "The framework demonstrates robustness to changing grip conditions in real-world scenarios by exploiting tracking errors as informative signals rather than disturbances, enabling adaptive optimization without explicit friction modeling."}}
{"id": "2602.15791", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15791", "abs": "https://arxiv.org/abs/2602.15791", "authors": ["Suhyung Jang", "Ghang Lee", "Jaekun Lee", "Hyunjun Lee"], "title": "Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings", "comment": "42nd International Symposium on Automation and Robotics in Construction (ISARC 2025)", "summary": "Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.", "AI": {"tldr": "This paper proposes using LLM embeddings instead of one-hot encoding to better represent building object subtypes in BIM models, achieving improved classification performance with GraphSAGE models.", "motivation": "Conventional encoding methods like one-hot encoding fail to capture nuanced relationships among closely related building object subtypes, limiting AI's semantic comprehension in the AECO industry. There's a need for better semantic representation that preserves finer distinctions between related building elements.", "method": "The study uses LLM embeddings (OpenAI GPT and Meta LLaMA) as encodings for building object subtypes, then trains GraphSAGE models to classify 42 subtypes across five high-rise residential BIMs. Various embedding dimensions are tested, including original high-dimensional embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via Matryoshka representation model.", "result": "LLM encodings outperformed conventional one-hot encoding baseline. The llama-3 (compacted) embedding achieved a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. This demonstrates improved classification performance for building object subtypes.", "conclusion": "LLM-based encodings show promise for enhancing AI's ability to interpret complex, domain-specific building semantics. As LLMs and dimensionality reduction techniques evolve, this approach has considerable potential for broad application in semantic elaboration tasks throughout the AECO industry."}}
{"id": "2602.15684", "categories": ["cs.RO", "cs.AI", "cs.HC", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.15684", "abs": "https://arxiv.org/abs/2602.15684", "authors": ["Feras Kiki", "Pouya P. Niaz", "Alireza Madani", "Cagatay Basdogan"], "title": "Estimating Human Muscular Fatigue in Dynamic Collaborative Robotic Tasks with Learning-Based Models", "comment": "ICRA 2026 Original Contribution, Vienne, Austria", "summary": "Assessing human muscle fatigue is critical for optimizing performance and safety in physical human-robot interaction(pHRI). This work presents a data-driven framework to estimate fatigue in dynamic, cyclic pHRI using arm-mounted surface electromyography(sEMG). Subject-specific machine-learning regression models(Random Forest, XGBoost, and Linear Regression predict the fraction of cycles to fatigue(FCF) from three frequency-domain and one time-domain EMG features, and are benchmarked against a convolutional neural network(CNN) that ingests spectrograms of filtered EMG. Framing fatigue estimation as regression (rather than classification) captures continuous progression toward fatigue, supporting earlier detection, timely intervention, and adaptive robot control. In experiments with ten participants, a collaborative robot under admittance control guided repetitive lateral (left-right) end-effector motions until muscular fatigue. Average FCF RMSE across participants was 20.8+/-4.3% for the CNN, 23.3+/-3.8% for Random Forest, 24.8+/-4.5% for XGBoost, and 26.9+/-6.1% for Linear Regression. To probe cross-task generalization, one participant additionally performed unseen vertical (up-down) and circular repetitions; models trained only on lateral data were tested directly and largely retained accuracy, indicating robustness to changes in movement direction, arm kinematics, and muscle recruitment, while Linear Regression deteriorated. Overall, the study shows that both feature-based ML and spectrogram-based DL can estimate remaining work capacity during repetitive pHRI, with the CNN delivering the lowest error and the tree-based models close behind. The reported transfer to new motion patterns suggests potential for practical fatigue monitoring without retraining for every task, improving operator protection and enabling fatigue-aware shared autonomy, for safer fatigue-adaptive pHRI control.", "AI": {"tldr": "A data-driven framework using sEMG and machine learning models (Random Forest, XGBoost, Linear Regression, CNN) estimates muscle fatigue during physical human-robot interaction, with CNN achieving lowest error (20.8% RMSE) and showing cross-task generalization to unseen movement patterns.", "motivation": "Assessing human muscle fatigue is critical for optimizing performance and safety in physical human-robot interaction (pHRI). Current approaches often treat fatigue as a binary classification problem rather than capturing its continuous progression, limiting early detection and timely intervention capabilities.", "method": "The study uses arm-mounted surface electromyography (sEMG) to monitor muscle activity during dynamic, cyclic pHRI. Subject-specific machine learning models (Random Forest, XGBoost, Linear Regression) predict Fraction of Cycles to Fatigue (FCF) from three frequency-domain and one time-domain EMG features. A convolutional neural network (CNN) processes spectrograms of filtered EMG. Experiments involved ten participants performing repetitive lateral motions with a collaborative robot under admittance control until muscular fatigue, with additional cross-task testing on vertical and circular motions.", "result": "Average FCF RMSE across participants was 20.8\u00b14.3% for CNN, 23.3\u00b13.8% for Random Forest, 24.8\u00b14.5% for XGBoost, and 26.9\u00b16.1% for Linear Regression. Cross-task generalization tests showed models trained on lateral data largely retained accuracy when tested on unseen vertical and circular motions, indicating robustness to changes in movement direction, arm kinematics, and muscle recruitment, though Linear Regression deteriorated.", "conclusion": "Both feature-based machine learning and spectrogram-based deep learning can effectively estimate remaining work capacity during repetitive pHRI, with CNN delivering the lowest error. The demonstrated transfer learning to new motion patterns suggests practical fatigue monitoring without task-specific retraining, enabling fatigue-aware shared autonomy and safer fatigue-adaptive pHRI control."}}
{"id": "2602.15816", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.15816", "abs": "https://arxiv.org/abs/2602.15816", "authors": ["Xiaoran Liu", "Istvan David"], "title": "Developing AI Agents with Simulated Data: Why, what, and how?", "comment": null, "summary": "As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.", "AI": {"tldr": "Introduction to simulation-based synthetic data generation for AI training, covering concepts, benefits, challenges, and a digital twin framework.", "motivation": "Insufficient data volume and quality are major barriers to adopting modern subsymbolic AI, creating high demand for synthetic data generation techniques.", "method": "Simulation-based approach for synthetic data generation, with a reference framework for describing, designing, and analyzing digital twin-based AI simulation solutions.", "result": "The chapter provides a comprehensive introduction to key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training.", "conclusion": "Simulation offers a systematic approach to generating diverse synthetic data, with digital twin-based frameworks providing structured solutions for AI training data needs."}}
{"id": "2602.15733", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15733", "abs": "https://arxiv.org/abs/2602.15733", "authors": ["Qiang Zhang", "Jiahao Ma", "Peiran Liu", "Shuai Shi", "Zeran Su", "Zifan Wang", "Jingkai Sun", "Wei Cui", "Jialin Yu", "Gang Han", "Wen Zhao", "Pihai Sun", "Kangning Yin", "Jiaxu Wang", "Jiahang Cao", "Lingfeng Zhang", "Hao Cheng", "Xiaoshuai Hao", "Yiding Ji", "Junwei Liang", "Jian Tang", "Renjing Xu", "Yijie Guo"], "title": "MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction", "comment": "17 pages, 6 figures", "summary": "Humanoid motion control has witnessed significant breakthroughs in recent years, with deep reinforcement learning (RL) emerging as a primary catalyst for achieving complex, human-like behaviors. However, the high dimensionality and intricate dynamics of humanoid robots make manual motion design impractical, leading to a heavy reliance on expensive motion capture (MoCap) data. These datasets are not only costly to acquire but also frequently lack the necessary geometric context of the surrounding physical environment. Consequently, existing motion synthesis frameworks often suffer from a decoupling of motion and scene, resulting in physical inconsistencies such as contact slippage or mesh penetration during terrain-aware tasks. In this work, we present MeshMimic, an innovative framework that bridges 3D scene reconstruction and embodied intelligence to enable humanoid robots to learn coupled \"motion-terrain\" interactions directly from video. By leveraging state-of-the-art 3D vision models, our framework precisely segments and reconstructs both human trajectories and the underlying 3D geometry of terrains and objects. We introduce an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions, alongside a contact-invariant retargeting method that transfers human-environment interaction features to the humanoid agent. Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across diverse and challenging terrains. Our approach proves that a low-cost pipeline utilizing only consumer-grade monocular sensors can facilitate the training of complex physical interactions, offering a scalable path toward the autonomous evolution of humanoid robots in unstructured environments.", "AI": {"tldr": "MeshMimic is a framework that enables humanoid robots to learn coupled motion-terrain interactions directly from video, using 3D scene reconstruction and optimization to overcome limitations of expensive motion capture data.", "motivation": "Current humanoid motion control relies heavily on expensive motion capture data that lacks geometric context of physical environments, leading to physical inconsistencies like contact slippage and mesh penetration in terrain-aware tasks. There's a need for a low-cost approach that can learn coupled motion-terrain interactions from readily available video data.", "method": "MeshMimic bridges 3D scene reconstruction and embodied intelligence by: 1) Using state-of-the-art 3D vision models to segment and reconstruct both human trajectories and underlying 3D geometry of terrains/objects; 2) Introducing an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions; 3) Developing a contact-invariant retargeting method that transfers human-environment interaction features to humanoid agents.", "result": "Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across diverse and challenging terrains. The framework proves that a low-cost pipeline using only consumer-grade monocular sensors can facilitate training of complex physical interactions.", "conclusion": "MeshMimic offers a scalable path toward autonomous evolution of humanoid robots in unstructured environments by enabling learning of coupled motion-terrain interactions directly from video, overcoming limitations of expensive motion capture data and decoupled motion-scene synthesis approaches."}}
{"id": "2602.15767", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.15767", "abs": "https://arxiv.org/abs/2602.15767", "authors": ["Atharva S Kashyap", "Ugne Aleksandra Morkute", "Patricia Alves-Oliveira"], "title": "Robot-Assisted Social Dining as a White Glove Service", "comment": "20 pages, 9 figures. Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI '26)", "summary": "Robot-assisted feeding enables people with disabilities who require assistance eating to enjoy a meal independently and with dignity. However, existing systems have only been tested in-lab or in-home, leaving in-the-wild social dining contexts (e.g., restaurants) largely unexplored. Designing a robot for such contexts presents unique challenges, such as dynamic and unsupervised dining environments that a robot needs to account for and respond to. Through speculative participatory design with people with disabilities, supported by semi-structured interviews and a custom AI-based visual storyboarding tool, we uncovered ideal scenarios for in-the-wild social dining. Our key insight suggests that such systems should: embody the principles of a white glove service where the robot (1) supports multimodal inputs and unobtrusive outputs; (2) has contextually sensitive social behavior and prioritizes the user; (3) has expanded roles beyond feeding; (4) adapts to other relationships at the dining table. Our work has implications for in-the-wild and group contexts of robot-assisted feeding.", "AI": {"tldr": "Robot-assisted feeding systems for social dining contexts should embody white glove service principles with multimodal inputs, context-sensitive social behavior, expanded roles beyond feeding, and adaptation to group dynamics.", "motivation": "Existing robot-assisted feeding systems have only been tested in controlled environments (in-lab or in-home), leaving real-world social dining contexts like restaurants unexplored. These dynamic, unsupervised environments present unique challenges that current systems don't address.", "method": "Used speculative participatory design with people with disabilities, supported by semi-structured interviews and a custom AI-based visual storyboarding tool to uncover ideal scenarios for in-the-wild social dining.", "result": "Key insight suggests robot-assisted feeding systems for social dining should embody white glove service principles: (1) support multimodal inputs and unobtrusive outputs; (2) have contextually sensitive social behavior and prioritize the user; (3) have expanded roles beyond feeding; (4) adapt to other relationships at the dining table.", "conclusion": "The work has implications for designing robot-assisted feeding systems for in-the-wild and group contexts, emphasizing the need for socially aware, adaptable systems that maintain user dignity in public dining settings."}}
{"id": "2602.15813", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15813", "abs": "https://arxiv.org/abs/2602.15813", "authors": ["Haochen Zhang", "Nirav Savaliya", "Faizan Siddiqui", "Enna Sachdeva"], "title": "FAST-EQA: Efficient Embodied Question Answering with Global and Local Region Relevancy", "comment": "WACV 2026", "summary": "Embodied Question Answering (EQA) combines visual scene understanding, goal-directed exploration, spatial and temporal reasoning under partial observability. A central challenge is to confine physical search to question-relevant subspaces while maintaining a compact, actionable memory of observations. Furthermore, for real-world deployment, fast inference time during exploration is crucial. We introduce FAST-EQA, a question-conditioned framework that (i) identifies likely visual targets, (ii) scores global regions of interest to guide navigation, and (iii) employs Chain-of-Thought (CoT) reasoning over visual memory to answer confidently. FAST-EQA maintains a bounded scene memory that stores a fixed-capacity set of region-target hypotheses and updates them online, enabling robust handling of both single and multi-target questions without unbounded growth. To expand coverage efficiently, a global exploration policy treats narrow openings and doors as high-value frontiers, complementing local target seeking with minimal computation. Together, these components focus the agent's attention, improve scene coverage, and improve answer reliability while running substantially faster than prior approaches. On HMEQA and EXPRESS-Bench, FAST-EQA achieves state-of-the-art performance, while performing competitively on OpenEQA and MT-HM3D.", "AI": {"tldr": "FAST-EQA introduces a question-conditioned framework for Embodied Question Answering that combines target identification, region scoring for navigation, and Chain-of-Thought reasoning over bounded visual memory to achieve fast inference and state-of-the-art performance.", "motivation": "EQA requires efficient physical search in question-relevant subspaces while maintaining compact, actionable memory. Real-world deployment demands fast inference time during exploration, which existing approaches struggle to achieve.", "method": "FAST-EQA uses: (1) question-conditioned target identification, (2) global region scoring for navigation guidance, (3) Chain-of-Thought reasoning over visual memory, (4) bounded scene memory storing fixed-capacity region-target hypotheses updated online, and (5) global exploration policy treating narrow openings/doors as high-value frontiers.", "result": "Achieves state-of-the-art performance on HMEQA and EXPRESS-Bench, competitive performance on OpenEQA and MT-HM3D, while running substantially faster than prior approaches.", "conclusion": "FAST-EQA effectively focuses agent attention, improves scene coverage, and enhances answer reliability through its question-conditioned framework with bounded memory and efficient exploration strategies."}}
{"id": "2602.15827", "categories": ["cs.RO", "cs.AI", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.15827", "abs": "https://arxiv.org/abs/2602.15827", "authors": ["Zhen Wu", "Xiaoyu Huang", "Lujie Yang", "Yuanhang Zhang", "Koushil Sreenath", "Xi Chen", "Pieter Abbeel", "Rocky Duan", "Angjoo Kanazawa", "Carmelo Sferrazza", "Guanya Shi", "C. Karen Liu"], "title": "Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching", "comment": null, "summary": "While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.", "AI": {"tldr": "PHP enables humanoid robots to perform agile parkour using motion matching for skill composition and RL for policy learning, achieving autonomous obstacle traversal with onboard perception.", "motivation": "Current humanoid locomotion lacks the agility, adaptivity, and expressiveness needed for dynamic parkour in complex environments, which requires both low-level robustness and high-level decision-making capabilities.", "method": "Two-stage approach: 1) Motion matching with nearest-neighbor search composes human skills into long-horizon kinematic trajectories, 2) RL expert policies trained for composed motions are distilled into a single depth-based multi-skill student policy using DAgger+RL combination.", "result": "Successfully demonstrated on Unitree G1 robot: climbing obstacles up to 1.25m (96% robot height) and long-horizon multi-obstacle traversal with closed-loop adaptation to real-time perturbations using only onboard depth sensing.", "conclusion": "PHP framework enables humanoid robots to achieve human-like parkour agility through modular skill composition and perception-driven autonomous decision-making, advancing dynamic locomotion in complex environments."}}
{"id": "2602.15828", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15828", "abs": "https://arxiv.org/abs/2602.15828", "authors": ["Yuxuan Kuang", "Sungjae Park", "Katerina Fragkiadaki", "Shubham Tulsiani"], "title": "Dex4D: Task-Agnostic Point Track Policy for Sim-to-Real Dexterous Manipulation", "comment": "Project page: https://dex4d.github.io/", "summary": "Learning generalist policies capable of accomplishing a plethora of everyday tasks remains an open challenge in dexterous manipulation. In particular, collecting large-scale manipulation data via real-world teleoperation is expensive and difficult to scale. While learning in simulation provides a feasible alternative, designing multiple task-specific environments and rewards for training is similarly challenging. We propose Dex4D, a framework that instead leverages simulation for learning task-agnostic dexterous skills that can be flexibly recomposed to perform diverse real-world manipulation tasks. Specifically, Dex4D learns a domain-agnostic 3D point track conditioned policy capable of manipulating any object to any desired pose. We train this 'Anypose-to-Anypose' policy in simulation across thousands of objects with diverse pose configurations, covering a broad space of robot-object interactions that can be composed at test time. At deployment, this policy can be zero-shot transferred to real-world tasks without finetuning, simply by prompting it with desired object-centric point tracks extracted from generated videos. During execution, Dex4D uses online point tracking for closed-loop perception and control. Extensive experiments in simulation and on real robots show that our method enables zero-shot deployment for diverse dexterous manipulation tasks and yields consistent improvements over prior baselines. Furthermore, we demonstrate strong generalization to novel objects, scene layouts, backgrounds, and trajectories, highlighting the robustness and scalability of the proposed framework.", "AI": {"tldr": "Dex4D learns domain-agnostic dexterous manipulation policies in simulation that can be zero-shot transferred to real-world tasks by prompting with object-centric point tracks from generated videos.", "motivation": "Learning generalist dexterous manipulation policies is challenging due to expensive real-world teleoperation data collection and difficulty scaling simulation-based training with task-specific environments and rewards.", "method": "Trains a domain-agnostic 3D point track conditioned policy (\"Anypose-to-Anypose\") in simulation across thousands of objects with diverse pose configurations, enabling flexible recomposition at test time. Uses online point tracking for closed-loop perception and control during deployment.", "result": "Enables zero-shot transfer to real-world tasks without finetuning, shows consistent improvements over prior baselines, and demonstrates strong generalization to novel objects, scene layouts, backgrounds, and trajectories.", "conclusion": "Dex4D provides a robust and scalable framework for learning task-agnostic dexterous skills that can be flexibly recomposed for diverse real-world manipulation tasks with strong generalization capabilities."}}
